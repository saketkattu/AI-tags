{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the nesseasary libraries\n",
    "from collections import Counter,OrderedDict\n",
    "import ipywidgets as widgets\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"id\": 324,\n  \"title\": \"AdverTorch\",\n  \"description\": \"A Toolbox for Adversarial Robustness Research\",\n  \"tags\": [\n    \"code\",\n    \"library\",\n    \"security\",\n    \"adversarial-learning\",\n    \"adversarial-attacks\",\n    \"adversarial-perturbations\"\n  ]\n}\n"
     ]
    }
   ],
   "source": [
    "#Load projects\n",
    "url=\"https://raw.githubusercontent.com/GokuMohandas/applied-ml/main/datasets/projects.json\"\n",
    "projects=json.loads(urlopen(url).read())\n",
    "print(json.dumps(projects[-305],indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2032 projects\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     id                                              title  \\\n",
       "0  2438  How to Deal with Files in Google Colab: What Y...   \n",
       "1  2437                                             Rasoee   \n",
       "2  2436    Machine Learning Methods Explained (+ Examples)   \n",
       "3  2435  Top “Applied Data Science” Papers from ECML-PK...   \n",
       "4  2434                          OpenMMLab Computer Vision   \n",
       "\n",
       "                                         description  \\\n",
       "0  How to supercharge your Google Colab experienc...   \n",
       "1  A powerful web and mobile application that ide...   \n",
       "2  Most common techniques used in data science pr...   \n",
       "3  Explore the innovative world of Machine Learni...   \n",
       "4  MMCV is a python library for CV research and s...   \n",
       "\n",
       "                                                tags  \n",
       "0        [article, google-colab, colab, file-system]  \n",
       "1  [api, article, code, dataset, paper, research,...  \n",
       "2  [article, deep-learning, machine-learning, dim...  \n",
       "3  [article, deep-learning, machine-learning, adv...  \n",
       "4  [article, code, pytorch, library, 3d, computer...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>description</th>\n      <th>tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2438</td>\n      <td>How to Deal with Files in Google Colab: What Y...</td>\n      <td>How to supercharge your Google Colab experienc...</td>\n      <td>[article, google-colab, colab, file-system]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2437</td>\n      <td>Rasoee</td>\n      <td>A powerful web and mobile application that ide...</td>\n      <td>[api, article, code, dataset, paper, research,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2436</td>\n      <td>Machine Learning Methods Explained (+ Examples)</td>\n      <td>Most common techniques used in data science pr...</td>\n      <td>[article, deep-learning, machine-learning, dim...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2435</td>\n      <td>Top “Applied Data Science” Papers from ECML-PK...</td>\n      <td>Explore the innovative world of Machine Learni...</td>\n      <td>[article, deep-learning, machine-learning, adv...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2434</td>\n      <td>OpenMMLab Computer Vision</td>\n      <td>MMCV is a python library for CV research and s...</td>\n      <td>[article, code, pytorch, library, 3d, computer...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Creating a pandas dataframe \n",
    "df=pd.DataFrame(projects)\n",
    "print(f\"{len(df)} projects\")\n",
    "df.head(5)"
   ]
  },
  {
   "source": [
    "#Loading the tags\n",
    "url1=\"https://raw.githubusercontent.com/GokuMohandas/applied-ml/main/datasets/tags.json\"\n",
    "tags_dict=OrderedDict(json.loads(urlopen(url1).read()))\n",
    "print(f\"{len(tags_dict)} tags\")\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "400 tags\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='tag', index=283, options=('3d', 'action-localization', 'action-rec…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9c07ac241604e57bccc4ce7133e730d"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(tag=list(tags_dict.keys()))\n",
    "def display_tag_details(tag='question-answering'):\n",
    "    print(json.dumps(tags_dict[tag],indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     id                                              title  \\\n",
       "0  2438  How to Deal with Files in Google Colab: What Y...   \n",
       "1  2437                                             Rasoee   \n",
       "2  2436    Machine Learning Methods Explained (+ Examples)   \n",
       "3  2435  Top “Applied Data Science” Papers from ECML-PK...   \n",
       "4  2434                          OpenMMLab Computer Vision   \n",
       "\n",
       "                                         description  \\\n",
       "0  How to supercharge your Google Colab experienc...   \n",
       "1  A powerful web and mobile application that ide...   \n",
       "2  Most common techniques used in data science pr...   \n",
       "3  Explore the innovative world of Machine Learni...   \n",
       "4  MMCV is a python library for CV research and s...   \n",
       "\n",
       "                                                tags  \\\n",
       "0        [article, google-colab, colab, file-system]   \n",
       "1  [api, article, code, dataset, paper, research,...   \n",
       "2  [article, deep-learning, machine-learning, dim...   \n",
       "3  [article, deep-learning, machine-learning, adv...   \n",
       "4  [article, code, pytorch, library, 3d, computer...   \n",
       "\n",
       "                                                text  \n",
       "0  How to Deal with Files in Google Colab: What Y...  \n",
       "1  Rasoee A powerful web and mobile application t...  \n",
       "2  Machine Learning Methods Explained (+ Examples...  \n",
       "3  Top “Applied Data Science” Papers from ECML-PK...  \n",
       "4  OpenMMLab Computer Vision MMCV is a python lib...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>description</th>\n      <th>tags</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2438</td>\n      <td>How to Deal with Files in Google Colab: What Y...</td>\n      <td>How to supercharge your Google Colab experienc...</td>\n      <td>[article, google-colab, colab, file-system]</td>\n      <td>How to Deal with Files in Google Colab: What Y...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2437</td>\n      <td>Rasoee</td>\n      <td>A powerful web and mobile application that ide...</td>\n      <td>[api, article, code, dataset, paper, research,...</td>\n      <td>Rasoee A powerful web and mobile application t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2436</td>\n      <td>Machine Learning Methods Explained (+ Examples)</td>\n      <td>Most common techniques used in data science pr...</td>\n      <td>[article, deep-learning, machine-learning, dim...</td>\n      <td>Machine Learning Methods Explained (+ Examples...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2435</td>\n      <td>Top “Applied Data Science” Papers from ECML-PK...</td>\n      <td>Explore the innovative world of Machine Learni...</td>\n      <td>[article, deep-learning, machine-learning, adv...</td>\n      <td>Top “Applied Data Science” Papers from ECML-PK...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2434</td>\n      <td>OpenMMLab Computer Vision</td>\n      <td>MMCV is a python library for CV research and s...</td>\n      <td>[article, code, pytorch, library, 3d, computer...</td>\n      <td>OpenMMLab Computer Vision MMCV is a python lib...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#Combining the projects title and descrption separately as feature but we'll combine them to create on input feature\n",
    "\n",
    "df['text']= df.title+\" \"+ df.description\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constrains\n",
    "def filter(l,include=[],exclude=[]):\n",
    "    \"\"\"\n",
    "    Filtering a list using inclution and exclution list of items \n",
    "    \"\"\"\n",
    "    filtered=[item for item in l if item in include and item not in exclude]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inclusion/exclusion criteria for tags\n",
    "include = list(tags_dict.keys())\n",
    "exclude = ['machine-learning', 'deep-learning',  'data-science',\n",
    "           'neural-networks', 'python', 'r', 'visualization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter tags for each project\n",
    "df.tags=df.tags.apply(filter,include=include,exclude=exclude)\n",
    "tags=Counter(itertools.chain.from_iterable(df.tags.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=30, description='min_tag_freq', max=429), Output()), _dom_classes=('widg…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6827bdc704964c1ca956fb291801909d"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(min_tag_freq=(0,tags.most_common()[0][1]))\n",
    "def separate_tag_by_freq(min_tag_freq=30):\n",
    "    tags_above_freq = Counter(tag for tag in tags.elements()\n",
    "                                    if tags[tag] >= min_tag_freq)\n",
    "    tags_below_freq = Counter(tag for tag in tags.elements()\n",
    "                                    if tags[tag] < min_tag_freq)\n",
    "    print (\"Most popular tags:\\n\", tags_above_freq.most_common(5))\n",
    "    print (\"\\nTags that just made the cut:\\n\", tags_above_freq.most_common()[-5:])\n",
    "    print (\"\\nTags that just missed the cut:\\n\", tags_below_freq.most_common(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tag_freq = 30\n",
    "tags_above_freq = Counter(tag for tag in tags.elements()\n",
    "                          if tags[tag] >= min_tag_freq)\n",
    "df.tags = df.tags.apply(filter, include=list(tags_above_freq.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1444 projects\n"
     ]
    }
   ],
   "source": [
    "df = df[df.tags.map(len) > 0]\n",
    "print (f\"{len(df)} projects\")"
   ]
  },
  {
   "source": [
    "# Exploratory data analysis\n",
    "NOTE: Revisit and re-do EDA when your data grows"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n  File \"c:\\python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\python39\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Python39\\Scripts\\pip.exe\\__main__.py\", line 4, in <module>\nModuleNotFoundError: No module named 'pip'\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "source": [
    "## Tags Per Project\n",
    "Q. How many tags do the projects have ?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 720x216 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"240.646875pt\" version=\"1.1\" viewBox=\"0 0 617.9725 240.646875\" width=\"617.9725pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 240.646875 \r\nL 617.9725 240.646875 \r\nL 617.9725 -0 \r\nL 0 -0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 52.7725 191.476875 \r\nL 610.7725 191.476875 \r\nL 610.7725 28.396875 \r\nL 52.7725 28.396875 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 59.7475 191.476875 \r\nL 115.5475 191.476875 \r\nL 115.5475 36.162589 \r\nL 59.7475 36.162589 \r\nz\r\n\" style=\"fill:#3274a1;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 129.4975 191.476875 \r\nL 185.2975 191.476875 \r\nL 185.2975 81.04325 \r\nL 129.4975 81.04325 \r\nz\r\n\" style=\"fill:#e1812c;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 199.2475 191.476875 \r\nL 255.0475 191.476875 \r\nL 255.0475 130.547978 \r\nL 199.2475 130.547978 \r\nz\r\n\" style=\"fill:#3a923a;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 268.9975 191.476875 \r\nL 324.7975 191.476875 \r\nL 324.7975 148.228239 \r\nL 268.9975 148.228239 \r\nz\r\n\" style=\"fill:#c03d3e;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 338.7475 191.476875 \r\nL 394.5475 191.476875 \r\nL 394.5475 174.884631 \r\nL 338.7475 174.884631 \r\nz\r\n\" style=\"fill:#9372b2;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 408.4975 191.476875 \r\nL 464.2975 191.476875 \r\nL 464.2975 186.852807 \r\nL 408.4975 186.852807 \r\nz\r\n\" style=\"fill:#845b53;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 478.2475 191.476875 \r\nL 534.0475 191.476875 \r\nL 534.0475 190.660863 \r\nL 478.2475 190.660863 \r\nz\r\n\" style=\"fill:#d684bd;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 547.9975 191.476875 \r\nL 603.7975 191.476875 \r\nL 603.7975 190.660863 \r\nL 547.9975 190.660863 \r\nz\r\n\" style=\"fill:#7f7f7f;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m7de4fa4817\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"87.6475\" xlink:href=\"#m7de4fa4817\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 1 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(82.5575 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"157.3975\" xlink:href=\"#m7de4fa4817\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(152.3075 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"227.1475\" xlink:href=\"#m7de4fa4817\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 3 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(222.0575 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"296.8975\" xlink:href=\"#m7de4fa4817\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(291.8075 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"366.6475\" xlink:href=\"#m7de4fa4817\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(361.5575 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"436.3975\" xlink:href=\"#m7de4fa4817\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(431.3075 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"506.1475\" xlink:href=\"#m7de4fa4817\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 7 -->\r\n      <defs>\r\n       <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n      </defs>\r\n      <g transform=\"translate(501.0575 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"575.8975\" xlink:href=\"#m7de4fa4817\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(570.8075 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_9\">\r\n     <!-- Number of tags -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 23.09375 72.90625 \r\nL 55.421875 11.921875 \r\nL 55.421875 72.90625 \r\nL 64.984375 72.90625 \r\nL 64.984375 0 \r\nL 51.703125 0 \r\nL 19.390625 60.984375 \r\nL 19.390625 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-78\"/>\r\n      <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n      <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(269.55375 230.119375)scale(0.16 -0.16)\">\r\n      <use xlink:href=\"#DejaVuSans-78\"/>\r\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"138.183594\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"235.595703\" xlink:href=\"#DejaVuSans-98\"/>\r\n      <use x=\"299.072266\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"360.595703\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"401.708984\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"433.496094\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"494.677734\" xlink:href=\"#DejaVuSans-102\"/>\r\n      <use x=\"529.882812\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"561.669922\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"600.878906\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"662.158203\" xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"725.634766\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_9\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m9b3cf57fe9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m9b3cf57fe9\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(39.41 195.276094)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m9b3cf57fe9\" y=\"164.276475\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(26.685 168.075693)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m9b3cf57fe9\" y=\"137.076074\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(26.685 140.875293)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m9b3cf57fe9\" y=\"109.875674\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(26.685 113.674893)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m9b3cf57fe9\" y=\"82.675274\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(26.685 86.474493)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m9b3cf57fe9\" y=\"55.474873\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 500 -->\r\n      <g transform=\"translate(26.685 59.274092)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- Number of projects -->\r\n     <defs>\r\n      <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 -0.984375 \r\nQ 18.40625 -11.421875 14.421875 -16.109375 \r\nQ 10.453125 -20.796875 1.609375 -20.796875 \r\nL -1.8125 -20.796875 \r\nL -1.8125 -13.1875 \r\nL 0.59375 -13.1875 \r\nQ 5.71875 -13.1875 7.5625 -10.8125 \r\nQ 9.421875 -8.453125 9.421875 -0.984375 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-106\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n     </defs>\r\n     <g transform=\"translate(19.3575 186.978125)rotate(-90)scale(0.16 -0.16)\">\r\n      <use xlink:href=\"#DejaVuSans-78\"/>\r\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"138.183594\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"235.595703\" xlink:href=\"#DejaVuSans-98\"/>\r\n      <use x=\"299.072266\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"360.595703\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"401.708984\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"433.496094\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"494.677734\" xlink:href=\"#DejaVuSans-102\"/>\r\n      <use x=\"529.882812\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"561.669922\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"625.146484\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"666.228516\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"727.410156\" xlink:href=\"#DejaVuSans-106\"/>\r\n      <use x=\"755.193359\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"816.716797\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"871.697266\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"910.90625\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_19\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_20\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_21\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_22\">\r\n    <path clip-path=\"url(#p7a7f095cb5)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path d=\"M 52.7725 191.476875 \r\nL 52.7725 28.396875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path d=\"M 610.7725 191.476875 \r\nL 610.7725 28.396875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path d=\"M 52.7725 191.476875 \r\nL 610.7725 191.476875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path d=\"M 52.7725 28.396875 \r\nL 610.7725 28.396875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_17\">\r\n    <!-- Tags per project -->\r\n    <defs>\r\n     <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n    </defs>\r\n    <g transform=\"translate(250.11 22.396875)scale(0.2 -0.2)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n     <use x=\"60.833984\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"122.113281\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"185.589844\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"237.689453\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"269.476562\" xlink:href=\"#DejaVuSans-112\"/>\r\n     <use x=\"332.953125\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"394.476562\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"435.589844\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"467.376953\" xlink:href=\"#DejaVuSans-112\"/>\r\n     <use x=\"530.853516\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"571.935547\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"633.117188\" xlink:href=\"#DejaVuSans-106\"/>\r\n     <use x=\"660.900391\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"722.423828\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"777.404297\" xlink:href=\"#DejaVuSans-116\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p7a7f095cb5\">\r\n   <rect height=\"163.08\" width=\"558\" x=\"52.7725\" y=\"28.396875\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "num_tags_per_project=[len(tags) for tags in df.tags]\n",
    "num_tags,num_projects=zip(*Counter(num_tags_per_project).items())\n",
    "plt.figure(figsize=(10,3))\n",
    "ax=sns.barplot(list(num_tags),list(num_projects))\n",
    "plt.title(\"Tags per project\",fontsize=20)\n",
    "plt.xlabel(\"Number of tags\", fontsize=16)\n",
    "ax.set_xticklabels(range(1, len(num_tags)+1), rotation=0, fontsize=16)\n",
    "plt.ylabel(\"Number of projects\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Tag distrbution\n",
    "What are the most popular tags?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1800x360 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"561.84125pt\" version=\"1.1\" viewBox=\"0 0 1454.9725 561.84125\" width=\"1454.9725pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 561.84125 \r\nL 1454.9725 561.84125 \r\nL 1454.9725 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 52.7725 300.196875 \r\nL 1447.7725 300.196875 \r\nL 1447.7725 28.396875 \r\nL 52.7725 28.396875 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 56.758214 300.196875 \r\nL 88.643929 300.196875 \r\nL 88.643929 41.339732 \r\nL 56.758214 41.339732 \r\nz\r\n\" style=\"fill:#ea96a3;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 96.615357 300.196875 \r\nL 128.501071 300.196875 \r\nL 128.501071 66.078993 \r\nL 96.615357 66.078993 \r\nz\r\n\" style=\"fill:#ea9794;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 136.4725 300.196875 \r\nL 168.358214 300.196875 \r\nL 168.358214 144.520551 \r\nL 136.4725 144.520551 \r\nz\r\n\" style=\"fill:#e6957c;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 176.329643 300.196875 \r\nL 208.215357 300.196875 \r\nL 208.215357 171.673398 \r\nL 176.329643 171.673398 \r\nz\r\n\" style=\"fill:#e0914f;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 216.186786 300.196875 \r\nL 248.0725 300.196875 \r\nL 248.0725 181.931141 \r\nL 216.186786 181.931141 \r\nz\r\n\" style=\"fill:#cf964d;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 256.043929 300.196875 \r\nL 287.929643 300.196875 \r\nL 287.929643 227.789283 \r\nL 256.043929 227.789283 \r\nz\r\n\" style=\"fill:#c29a4b;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 295.901071 300.196875 \r\nL 327.786786 300.196875 \r\nL 327.786786 236.236835 \r\nL 295.901071 236.236835 \r\nz\r\n\" style=\"fill:#b79c49;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 335.758214 300.196875 \r\nL 367.643929 300.196875 \r\nL 367.643929 244.080991 \r\nL 335.758214 244.080991 \r\nz\r\n\" style=\"fill:#ab9e47;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 375.615357 300.196875 \r\nL 407.501071 300.196875 \r\nL 407.501071 253.13194 \r\nL 375.615357 253.13194 \r\nz\r\n\" style=\"fill:#a09f45;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 415.4725 300.196875 \r\nL 447.358214 300.196875 \r\nL 447.358214 254.94213 \r\nL 415.4725 254.94213 \r\nz\r\n\" style=\"fill:#95a346;\"/>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 455.329643 300.196875 \r\nL 487.215357 300.196875 \r\nL 487.215357 256.148923 \r\nL 455.329643 256.148923 \r\nz\r\n\" style=\"fill:#88a746;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 495.186786 300.196875 \r\nL 527.0725 300.196875 \r\nL 527.0725 258.562509 \r\nL 495.186786 258.562509 \r\nz\r\n\" style=\"fill:#75ab47;\"/>\r\n   </g>\r\n   <g id=\"patch_15\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 535.043929 300.196875 \r\nL 566.929643 300.196875 \r\nL 566.929643 261.579492 \r\nL 535.043929 261.579492 \r\nz\r\n\" style=\"fill:#4fb047;\"/>\r\n   </g>\r\n   <g id=\"patch_16\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 574.901071 300.196875 \r\nL 606.786786 300.196875 \r\nL 606.786786 263.993079 \r\nL 574.901071 263.993079 \r\nz\r\n\" style=\"fill:#48af6f;\"/>\r\n   </g>\r\n   <g id=\"patch_17\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 614.758214 300.196875 \r\nL 646.643929 300.196875 \r\nL 646.643929 264.596475 \r\nL 614.758214 264.596475 \r\nz\r\n\" style=\"fill:#49ae83;\"/>\r\n   </g>\r\n   <g id=\"patch_18\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 654.615357 300.196875 \r\nL 686.501071 300.196875 \r\nL 686.501071 265.803269 \r\nL 654.615357 265.803269 \r\nz\r\n\" style=\"fill:#4aad8f;\"/>\r\n   </g>\r\n   <g id=\"patch_19\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 694.4725 300.196875 \r\nL 726.358214 300.196875 \r\nL 726.358214 267.010062 \r\nL 694.4725 267.010062 \r\nz\r\n\" style=\"fill:#4aac99;\"/>\r\n   </g>\r\n   <g id=\"patch_20\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 734.329643 300.196875 \r\nL 766.215357 300.196875 \r\nL 766.215357 269.423648 \r\nL 734.329643 269.423648 \r\nz\r\n\" style=\"fill:#4baca1;\"/>\r\n   </g>\r\n   <g id=\"patch_21\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 774.186786 300.196875 \r\nL 806.0725 300.196875 \r\nL 806.0725 269.423648 \r\nL 774.186786 269.423648 \r\nz\r\n\" style=\"fill:#4baba8;\"/>\r\n   </g>\r\n   <g id=\"patch_22\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 814.043929 300.196875 \r\nL 845.929643 300.196875 \r\nL 845.929643 269.423648 \r\nL 814.043929 269.423648 \r\nz\r\n\" style=\"fill:#4cabb0;\"/>\r\n   </g>\r\n   <g id=\"patch_23\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 853.901071 300.196875 \r\nL 885.786786 300.196875 \r\nL 885.786786 269.423648 \r\nL 853.901071 269.423648 \r\nz\r\n\" style=\"fill:#4eabb8;\"/>\r\n   </g>\r\n   <g id=\"patch_24\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 893.758214 300.196875 \r\nL 925.643929 300.196875 \r\nL 925.643929 270.630441 \r\nL 893.758214 270.630441 \r\nz\r\n\" style=\"fill:#50acc3;\"/>\r\n   </g>\r\n   <g id=\"patch_25\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 933.615357 300.196875 \r\nL 965.501071 300.196875 \r\nL 965.501071 271.233838 \r\nL 933.615357 271.233838 \r\nz\r\n\" style=\"fill:#53accf;\"/>\r\n   </g>\r\n   <g id=\"patch_26\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 973.4725 300.196875 \r\nL 1005.358214 300.196875 \r\nL 1005.358214 272.440631 \r\nL 973.4725 272.440631 \r\nz\r\n\" style=\"fill:#5aade0;\"/>\r\n   </g>\r\n   <g id=\"patch_27\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 1013.329643 300.196875 \r\nL 1045.215357 300.196875 \r\nL 1045.215357 275.457614 \r\nL 1013.329643 275.457614 \r\nz\r\n\" style=\"fill:#86aee6;\"/>\r\n   </g>\r\n   <g id=\"patch_28\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 1053.186786 300.196875 \r\nL 1085.0725 300.196875 \r\nL 1085.0725 275.457614 \r\nL 1053.186786 275.457614 \r\nz\r\n\" style=\"fill:#a0adea;\"/>\r\n   </g>\r\n   <g id=\"patch_29\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 1093.043929 300.196875 \r\nL 1124.929643 300.196875 \r\nL 1124.929643 276.061011 \r\nL 1093.043929 276.061011 \r\nz\r\n\" style=\"fill:#b2a9eb;\"/>\r\n   </g>\r\n   <g id=\"patch_30\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 1132.901071 300.196875 \r\nL 1164.786786 300.196875 \r\nL 1164.786786 276.061011 \r\nL 1132.901071 276.061011 \r\nz\r\n\" style=\"fill:#c1a3ea;\"/>\r\n   </g>\r\n   <g id=\"patch_31\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 1172.758214 300.196875 \r\nL 1204.643929 300.196875 \r\nL 1204.643929 276.664407 \r\nL 1172.758214 276.664407 \r\nz\r\n\" style=\"fill:#ce9be9;\"/>\r\n   </g>\r\n   <g id=\"patch_32\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 1212.615357 300.196875 \r\nL 1244.501071 300.196875 \r\nL 1244.501071 276.664407 \r\nL 1212.615357 276.664407 \r\nz\r\n\" style=\"fill:#dc91e7;\"/>\r\n   </g>\r\n   <g id=\"patch_33\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 1252.4725 300.196875 \r\nL 1284.358214 300.196875 \r\nL 1284.358214 279.68139 \r\nL 1252.4725 279.68139 \r\nz\r\n\" style=\"fill:#e78ae0;\"/>\r\n   </g>\r\n   <g id=\"patch_34\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 1292.329643 300.196875 \r\nL 1324.215357 300.196875 \r\nL 1324.215357 279.68139 \r\nL 1292.329643 279.68139 \r\nz\r\n\" style=\"fill:#e78dd2;\"/>\r\n   </g>\r\n   <g id=\"patch_35\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 1332.186786 300.196875 \r\nL 1364.0725 300.196875 \r\nL 1364.0725 280.284787 \r\nL 1332.186786 280.284787 \r\nz\r\n\" style=\"fill:#e890c7;\"/>\r\n   </g>\r\n   <g id=\"patch_36\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 1372.043929 300.196875 \r\nL 1403.929643 300.196875 \r\nL 1403.929643 280.888184 \r\nL 1372.043929 280.888184 \r\nz\r\n\" style=\"fill:#e992bc;\"/>\r\n   </g>\r\n   <g id=\"patch_37\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 1411.901071 300.196875 \r\nL 1443.786786 300.196875 \r\nL 1443.786786 282.094977 \r\nL 1411.901071 282.094977 \r\nz\r\n\" style=\"fill:#e994b1;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m0c0ea4377c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"72.701071\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- natural-language-processing -->\r\n      <defs>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 4.890625 31.390625 \r\nL 31.203125 31.390625 \r\nL 31.203125 23.390625 \r\nL 4.890625 23.390625 \r\nz\r\n\" id=\"DejaVuSans-45\"/>\r\n       <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      </defs>\r\n      <g transform=\"translate(76.564196 508.271875)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"63.378906\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"124.658203\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"163.867188\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"227.246094\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"268.359375\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"329.638672\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"357.421875\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"393.505859\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"421.289062\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"482.568359\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"545.947266\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"609.423828\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"672.802734\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"734.082031\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"797.558594\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"859.082031\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"895.166016\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"958.642578\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"999.724609\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"1060.90625\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"1115.886719\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"1177.410156\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"1229.509766\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"1281.609375\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"1309.392578\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1372.771484\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.558214\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- computer-vision -->\r\n      <defs>\r\n       <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      </defs>\r\n      <g transform=\"translate(116.421339 420.546562)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"116.162109\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"213.574219\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"277.050781\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"340.429688\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"379.638672\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"441.162109\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"482.181641\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"518.234375\" xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"577.414062\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"605.197266\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"657.296875\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"685.080078\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"746.261719\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"152.415357\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- pytorch -->\r\n      <defs>\r\n       <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      </defs>\r\n      <g transform=\"translate(156.278482 360.746875)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-121\"/>\r\n       <use x=\"122.65625\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"161.865234\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"223.046875\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"264.128906\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"319.109375\" xlink:href=\"#DejaVuSans-104\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"192.2725\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- tensorflow -->\r\n      <defs>\r\n       <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n       <path d=\"M 4.203125 54.6875 \r\nL 13.1875 54.6875 \r\nL 24.421875 12.015625 \r\nL 35.59375 54.6875 \r\nL 46.1875 54.6875 \r\nL 57.421875 12.015625 \r\nL 68.609375 54.6875 \r\nL 77.59375 54.6875 \r\nL 63.28125 0 \r\nL 52.6875 0 \r\nL 40.921875 44.828125 \r\nL 29.109375 0 \r\nL 18.5 0 \r\nz\r\n\" id=\"DejaVuSans-119\"/>\r\n      </defs>\r\n      <g transform=\"translate(196.135625 380.620312)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"100.732422\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"164.111328\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"216.210938\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"277.392578\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"318.505859\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"353.710938\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"381.494141\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"442.675781\" xlink:href=\"#DejaVuSans-119\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.129643\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- transformers -->\r\n      <g transform=\"translate(235.992768 397.733125)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"141.601562\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"204.980469\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"257.080078\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"292.285156\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"353.466797\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"394.564453\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"491.976562\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"553.5\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"594.613281\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"271.986786\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- attention -->\r\n      <g transform=\"translate(275.849911 371.056562)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"61.279297\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"100.488281\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"139.697266\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"201.220703\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"264.599609\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"303.808594\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"331.591797\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"392.773438\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"311.843929\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- convolutional-neural-networks -->\r\n      <defs>\r\n       <path d=\"M 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 31.109375 \r\nL 44.921875 54.6875 \r\nL 56.390625 54.6875 \r\nL 27.390625 29.109375 \r\nL 57.625 0 \r\nL 45.90625 0 \r\nL 18.109375 26.703125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nz\r\n\" id=\"DejaVuSans-107\"/>\r\n      </defs>\r\n      <g transform=\"translate(315.707054 520.064687)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"116.162109\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"179.541016\" xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"238.720703\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"299.902344\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"327.685547\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"391.064453\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"430.273438\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"458.056641\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"519.238281\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"582.617188\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"643.896484\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"671.679688\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"707.763672\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"771.142578\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"832.666016\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"896.044922\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"937.158203\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"998.4375\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"1026.220703\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"1062.304688\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1125.683594\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"1187.207031\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"1226.416016\" xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"1308.203125\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"1369.384766\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"1410.498047\" xlink:href=\"#DejaVuSans-107\"/>\r\n       <use x=\"1468.408203\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"351.701071\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- keras -->\r\n      <g transform=\"translate(355.564196 345.539375)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-107\"/>\r\n       <use x=\"57.863281\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"119.386719\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"160.5\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"221.779297\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"391.558214\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- graphs -->\r\n      <g transform=\"translate(395.421339 355.472812)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"104.589844\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"165.869141\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"229.345703\" xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"292.724609\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_10\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"431.415357\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- embeddings -->\r\n      <defs>\r\n       <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      </defs>\r\n      <g transform=\"translate(435.278482 393.66875)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"61.523438\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"158.935547\" xlink:href=\"#DejaVuSans-98\"/>\r\n       <use x=\"222.412109\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"283.935547\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"347.412109\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"410.888672\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"438.671875\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"502.050781\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"565.527344\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_11\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"471.2725\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- generative-adversarial-networks -->\r\n      <g transform=\"translate(475.135625 535.15625)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"125\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"188.378906\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"249.902344\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"291.015625\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"352.294922\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"391.503906\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"419.287109\" xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"478.466797\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"539.990234\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"576.074219\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"637.353516\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"700.830078\" xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"760.009766\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"821.533203\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"862.646484\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"914.746094\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"976.025391\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"1017.138672\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"1044.921875\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"1106.201172\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"1133.984375\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"1170.068359\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1233.447266\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"1294.970703\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"1334.179688\" xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"1415.966797\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"1477.148438\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"1518.261719\" xlink:href=\"#DejaVuSans-107\"/>\r\n       <use x=\"1576.171875\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_12\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"511.129643\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- object-detection -->\r\n      <defs>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 -0.984375 \r\nQ 18.40625 -11.421875 14.421875 -16.109375 \r\nQ 10.453125 -20.796875 1.609375 -20.796875 \r\nL -1.8125 -20.796875 \r\nL -1.8125 -13.1875 \r\nL 0.59375 -13.1875 \r\nQ 5.71875 -13.1875 7.5625 -10.8125 \r\nQ 9.421875 -8.453125 9.421875 -0.984375 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-106\"/>\r\n      </defs>\r\n      <g transform=\"translate(514.992768 421.51125)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"61.181641\" xlink:href=\"#DejaVuSans-98\"/>\r\n       <use x=\"124.658203\" xlink:href=\"#DejaVuSans-106\"/>\r\n       <use x=\"152.441406\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"213.964844\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"268.945312\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"308.154297\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"344.238281\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"407.714844\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"469.238281\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"508.447266\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"569.970703\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"624.951172\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"664.160156\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"691.943359\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"753.125\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_13\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"550.986786\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- huggingface -->\r\n      <g transform=\"translate(554.849911 394.187187)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"63.378906\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"126.757812\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"190.234375\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"253.710938\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"281.494141\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"344.873047\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"408.349609\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"443.554688\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"504.833984\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"559.814453\" xlink:href=\"#DejaVuSans-101\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_14\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"590.843929\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- scikit-learn -->\r\n      <g transform=\"translate(594.707054 384.321562)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"107.080078\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"134.863281\" xlink:href=\"#DejaVuSans-107\"/>\r\n       <use x=\"192.773438\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"220.556641\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"259.765625\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"295.849609\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"323.632812\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"385.15625\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"446.435547\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"487.533203\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_15\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"630.701071\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- reinforcement-learning -->\r\n      <g transform=\"translate(634.564196 468.903437)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"41.082031\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"102.605469\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"130.388672\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"193.767578\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"228.972656\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"290.154297\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"331.236328\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"386.216797\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"447.740234\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"545.152344\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"606.675781\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"670.054688\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"709.263672\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"745.347656\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"773.130859\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"834.654297\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"895.933594\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"937.03125\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1000.410156\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"1028.193359\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1091.572266\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_16\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"670.558214\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- representation-learning -->\r\n      <g transform=\"translate(674.421339 472.889063)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"41.082031\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"102.605469\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"166.082031\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"207.164062\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"268.6875\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"320.787109\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"382.310547\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"445.689453\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"484.898438\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"546.177734\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"585.386719\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"613.169922\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"674.351562\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"737.730469\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"773.814453\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"801.597656\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"863.121094\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"924.400391\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"965.498047\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1028.876953\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"1056.660156\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1120.039062\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_17\">\r\n     <g id=\"line2d_17\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"710.415357\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- interpretability -->\r\n      <g transform=\"translate(714.278482 411.468437)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"91.162109\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"130.371094\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"191.894531\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"233.007812\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"296.484375\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"337.566406\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"399.089844\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"438.298828\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"499.578125\" xlink:href=\"#DejaVuSans-98\"/>\r\n       <use x=\"563.054688\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"590.837891\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"618.621094\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"646.404297\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"685.613281\" xlink:href=\"#DejaVuSans-121\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_18\">\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"750.2725\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_18\">\r\n      <!-- image-classification -->\r\n      <g transform=\"translate(754.135625 446.409375)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"125.195312\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"186.474609\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"249.951172\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"311.474609\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"347.558594\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"402.539062\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"430.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"491.601562\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"543.701172\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"595.800781\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"623.583984\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"658.789062\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"686.572266\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"741.552734\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"802.832031\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"842.041016\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"869.824219\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"931.005859\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_19\">\r\n     <g id=\"line2d_19\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"790.129643\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_19\">\r\n      <!-- production -->\r\n      <g transform=\"translate(793.992768 382.676562)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"104.558594\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"165.740234\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"229.216797\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"292.595703\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"347.576172\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"386.785156\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"414.568359\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"475.75\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_20\">\r\n     <g id=\"line2d_20\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"829.986786\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_20\">\r\n      <!-- language-modeling -->\r\n      <g transform=\"translate(833.849911 442.675313)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"152.441406\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"215.917969\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"279.296875\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"340.576172\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"404.052734\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"465.576172\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"501.660156\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"599.072266\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"660.253906\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"723.730469\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"785.253906\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"813.037109\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"840.820312\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"904.199219\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_21\">\r\n     <g id=\"line2d_21\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"869.843929\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_21\">\r\n      <!-- graph-neural-networks -->\r\n      <g transform=\"translate(873.707054 467.01125)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"104.589844\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"165.869141\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"229.345703\" xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"292.724609\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"328.808594\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"392.1875\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"453.710938\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"517.089844\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"558.203125\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"619.482422\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"647.265625\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"683.349609\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"746.728516\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"808.251953\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"847.460938\" xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"929.248047\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"990.429688\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"1031.542969\" xlink:href=\"#DejaVuSans-107\"/>\r\n       <use x=\"1089.453125\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_22\">\r\n     <g id=\"line2d_22\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"909.701071\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_22\">\r\n      <!-- regression -->\r\n      <g transform=\"translate(913.564196 380.729687)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"41.082031\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"102.605469\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"166.082031\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"207.164062\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"268.6875\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"320.787109\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"372.886719\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"400.669922\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"461.851562\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_23\">\r\n     <g id=\"line2d_23\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"949.558214\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_23\">\r\n      <!-- segmentation -->\r\n      <g transform=\"translate(953.421339 404.000313)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"113.623047\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"177.099609\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"274.511719\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"336.035156\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"399.414062\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"438.623047\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"499.902344\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"539.111328\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"566.894531\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"628.076172\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_24\">\r\n     <g id=\"line2d_24\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"989.415357\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_24\">\r\n      <!-- transfer-learning -->\r\n      <g transform=\"translate(993.278482 424.88)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"141.601562\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"204.980469\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"257.080078\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"292.285156\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"353.808594\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"394.828125\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"430.912109\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"458.695312\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"520.21875\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"581.498047\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"622.595703\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"685.974609\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"713.757812\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"777.136719\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_25\">\r\n     <g id=\"line2d_25\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1029.2725\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_25\">\r\n      <!-- data-augmentation -->\r\n      <g transform=\"translate(1033.135625 442.130625)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"124.755859\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"163.964844\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"225.244141\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"261.328125\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"322.607422\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"385.986328\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"449.462891\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"546.875\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"608.398438\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"671.777344\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"710.986328\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"772.265625\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"811.474609\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"839.257812\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"900.439453\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_26\">\r\n     <g id=\"line2d_26\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1069.129643\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_26\">\r\n      <!-- autoencoders -->\r\n      <g transform=\"translate(1072.992768 403.005)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"61.279297\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"124.658203\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"163.867188\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"225.048828\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"286.572266\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"349.951172\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"404.931641\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"466.113281\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"529.589844\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"591.113281\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"632.226562\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_27\">\r\n     <g id=\"line2d_27\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1108.986786\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_27\">\r\n      <!-- self-supervised-learning -->\r\n      <g transform=\"translate(1112.849911 475.765625)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"113.623047\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"141.40625\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"176.533203\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"212.617188\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"264.716797\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"328.095703\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"391.572266\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"453.095703\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"494.208984\" xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"553.388672\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"581.171875\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"633.271484\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"694.794922\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"758.271484\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"794.355469\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"822.138672\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"883.662109\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"944.941406\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"986.039062\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1049.417969\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"1077.201172\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1140.580078\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_28\">\r\n     <g id=\"line2d_28\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1148.843929\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_28\">\r\n      <!-- tensorflow-js -->\r\n      <g transform=\"translate(1152.707054 396.85375)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"100.732422\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"164.111328\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"216.210938\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"277.392578\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"318.505859\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"353.710938\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"381.494141\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"442.675781\" xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"524.462891\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"560.546875\" xlink:href=\"#DejaVuSans-106\"/>\r\n       <use x=\"588.330078\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_29\">\r\n     <g id=\"line2d_29\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1188.701071\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_29\">\r\n      <!-- unsupervised-learning -->\r\n      <g transform=\"translate(1192.564196 463.745312)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"63.378906\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"126.757812\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"178.857422\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"242.236328\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"305.712891\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"367.236328\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"408.349609\" xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"467.529297\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"495.3125\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"547.412109\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"608.935547\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"672.412109\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"708.496094\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"736.279297\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"797.802734\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"859.082031\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"900.179688\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"963.558594\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"991.341797\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1054.720703\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_30\">\r\n     <g id=\"line2d_30\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1228.558214\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_30\">\r\n      <!-- wandb -->\r\n      <g transform=\"translate(1232.421339 353.87375)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"81.787109\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"143.066406\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"206.445312\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"269.921875\" xlink:href=\"#DejaVuSans-98\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_31\">\r\n     <g id=\"line2d_31\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1268.415357\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_31\">\r\n      <!-- time-series -->\r\n      <g transform=\"translate(1272.278482 385.336563)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"39.208984\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"66.992188\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"164.404297\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"225.927734\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"262.011719\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"314.111328\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"375.634766\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"416.748047\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"444.53125\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"506.054688\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_32\">\r\n     <g id=\"line2d_32\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1308.2725\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_32\">\r\n      <!-- flask -->\r\n      <g transform=\"translate(1312.135625 339.994062)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"35.205078\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"62.988281\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"124.267578\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"176.367188\" xlink:href=\"#DejaVuSans-107\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_33\">\r\n     <g id=\"line2d_33\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1348.129643\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_33\">\r\n      <!-- node-classification -->\r\n      <g transform=\"translate(1351.992768 437.7425)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"63.378906\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"124.560547\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"188.037109\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"249.560547\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"285.644531\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"340.625\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"368.408203\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"429.6875\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"481.787109\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"533.886719\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"561.669922\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"596.875\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"624.658203\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"679.638672\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"740.917969\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"780.126953\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"807.910156\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"869.091797\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_34\">\r\n     <g id=\"line2d_34\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1387.986786\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_34\">\r\n      <!-- question-answering -->\r\n      <defs>\r\n       <path d=\"M 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\nM 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nL 54.390625 -20.796875 \r\nL 45.40625 -20.796875 \r\nz\r\n\" id=\"DejaVuSans-113\"/>\r\n      </defs>\r\n      <g transform=\"translate(1391.849911 444.945938)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-113\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"126.855469\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"188.378906\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"240.478516\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"279.6875\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"307.470703\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"368.652344\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"432.03125\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"468.115234\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"529.394531\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"592.773438\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"644.873047\" xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"726.660156\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"788.183594\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"829.296875\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"857.080078\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"920.458984\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_35\">\r\n     <g id=\"line2d_35\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1427.843929\" xlink:href=\"#m0c0ea4377c\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_35\">\r\n      <!-- pretraining -->\r\n      <g transform=\"translate(1431.707054 384.684687)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"104.558594\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"166.082031\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"205.291016\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"246.404297\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"307.683594\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"335.466797\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"398.845703\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"426.628906\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"490.007812\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_36\">\r\n     <!-- Tag -->\r\n     <defs>\r\n      <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n     </defs>\r\n     <g transform=\"translate(735.425 551.31375)scale(0.16 -0.16)\">\r\n      <use xlink:href=\"#DejaVuSans-84\"/>\r\n      <use x=\"60.833984\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"122.113281\" xlink:href=\"#DejaVuSans-103\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_36\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m5d0c7372ca\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m5d0c7372ca\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_37\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(39.41 303.996094)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_37\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m5d0c7372ca\" y=\"239.857215\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_38\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(26.685 243.656433)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_38\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m5d0c7372ca\" y=\"179.517554\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_39\">\r\n      <!-- 200 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(26.685 183.316773)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_39\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m5d0c7372ca\" y=\"119.177894\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_40\">\r\n      <!-- 300 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(26.685 122.977113)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_40\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m5d0c7372ca\" y=\"58.838234\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_41\">\r\n      <!-- 400 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(26.685 62.637452)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_42\">\r\n     <!-- Number of projects -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 23.09375 72.90625 \r\nL 55.421875 11.921875 \r\nL 55.421875 72.90625 \r\nL 64.984375 72.90625 \r\nL 64.984375 0 \r\nL 51.703125 0 \r\nL 19.390625 60.984375 \r\nL 19.390625 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-78\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n     </defs>\r\n     <g transform=\"translate(19.3575 241.338125)rotate(-90)scale(0.16 -0.16)\">\r\n      <use xlink:href=\"#DejaVuSans-78\"/>\r\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"138.183594\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"235.595703\" xlink:href=\"#DejaVuSans-98\"/>\r\n      <use x=\"299.072266\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"360.595703\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"401.708984\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"433.496094\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"494.677734\" xlink:href=\"#DejaVuSans-102\"/>\r\n      <use x=\"529.882812\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"561.669922\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"625.146484\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"666.228516\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"727.410156\" xlink:href=\"#DejaVuSans-106\"/>\r\n      <use x=\"755.193359\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"816.716797\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"871.697266\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"910.90625\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_41\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_42\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_43\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_44\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_45\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_46\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_47\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_48\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_49\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_50\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_51\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_52\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_53\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_54\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_55\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_56\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_57\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_58\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_59\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_60\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_61\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_62\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_63\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_64\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_65\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_66\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_67\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_68\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_69\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_70\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_71\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_72\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_73\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_74\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_75\">\r\n    <path clip-path=\"url(#pc4b4726056)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"patch_38\">\r\n    <path d=\"M 52.7725 300.196875 \r\nL 52.7725 28.396875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_39\">\r\n    <path d=\"M 1447.7725 300.196875 \r\nL 1447.7725 28.396875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_40\">\r\n    <path d=\"M 52.7725 300.196875 \r\nL 1447.7725 300.196875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_41\">\r\n    <path d=\"M 52.7725 28.396875 \r\nL 1447.7725 28.396875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_43\">\r\n    <!-- Tag distribution -->\r\n    <g transform=\"translate(671.549062 22.396875)scale(0.2 -0.2)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n     <use x=\"60.833984\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"122.113281\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"185.589844\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"217.376953\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"280.853516\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"308.636719\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"360.736328\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"399.945312\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"441.058594\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"468.841797\" xlink:href=\"#DejaVuSans-98\"/>\r\n     <use x=\"532.318359\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"595.697266\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"634.90625\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"662.689453\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"723.871094\" xlink:href=\"#DejaVuSans-110\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pc4b4726056\">\r\n   <rect height=\"271.8\" width=\"1395\" x=\"52.7725\" y=\"28.396875\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABa4AAAIxCAYAAACl9C8TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd5h1VXk3/u+N2I1iwQYoFmLEEs2L7U1sQWMjYiIa88aoCcrPXjB2saBJ7IqJJSQWjEnUIJZYsKNpFuyxgxFFURABKaJR798fe08YhpnhPM+UfR7m87muc51z1tpnr++cmTmzz33WrF3dHQAAAAAAmBc7TR0AAAAAAAAWU7gGAAAAAGCuKFwDAAAAADBXFK4BAAAAAJgrCtcAAAAAAMwVhWsAAAAAAOaKwjUAAGynqrpcVXVVvWtJ+4vG9n0myvXIcfwDlrT/sKr+a4pMizJM+twAALBjULgGAGDTjAXLbbk8aOrMU6iqI8ev/ypTZ9lWKxXNAQBgW+w8dQAAALaUZy/T9tgkV0hyWJLTl/R9bsMTbYznJ/m7JN+aaPw3Jvlgku9ONP5qpn5uAADYAShcAwCwabr7WUvbxlnVV0jysu7+1iZH2hDdfUqSUyYc//Rc8EOAuTD1cwMAwI7BUiEAAMy9qrp1Vf11VX2xqk6vqnOr6mtV9byq+pUVHnOlqnplVX1v3P5LVfWIqrrxuJTFX2/D+JeqqudU1beq6qdVdXxVPSPJxVfYftl1nKtq36p6b1V9d9zPSVX171X1pLH/clXVSe49PuSURcum/Nei/RxbVWdV1aWr6rlVdVxV/Wzha7qw5Tqq6spVdfg4/rnj8/r/LbPdfuN+/myF/fxwaa4kfzXe/ecly75cZbXnZuy7e1V9cNH3+KtVdWhVXW6ZbReeg0tU1bOq6pvjc3rC+L0ySQcAYAfmYA4AgB3BI5P8dpKPJXlfhoLxLZI8KcnvVNX/7e5zFzYei9kfS3KjJJ9K8oYkV07yF0k+vC0DV9VOSd6Z5M5Jvpbk5UkuM2b6P9uwn3snOTLJqeP+vp/kKkn2TvL/ZVhC42cZllO5b5IbJnlhknPGXZy8ZJc7JXlXkhtkeE5OTXLCDFEuneSYDO8F3jh+LfdJ8uqqum53P2nWr2kZhye5V5K7JfnnJF9e1HfOso8YVdXBSV6c5IzxsT9KcqckhyTZr6pu191nLX1YkqOS3CzJ0UnOTvK7SZ6eZJckj1rD1wIAwIQUrgEA2BE8Pcm3u/uXixur6jFJXpbkwCSvWLL9jZK8prsfvGj75yX5zDaO/eAMResPJ7lrd//PuK9Dk3x6G/Zz0Hh96+4+bsnXcZUk6e6fJXlWVd04Q+H6Bd39wxX2d+kkv5LkxuPSILO6bpIPJLnHMl/LE6rqLd29LV/X/+ruw6vqEhkK12/p7iNneVxV/VqSF2QoVu/T3f89tleS1yd5QJJDkxy85KGXSXLFJDfq7jPGxxySoWB+UFUdso3PDQAAc8JSIQAAzL3u/tbSovXolRlmKd9lSfsDx/anL9nP8eNjtsWfjNdPXij0jvv6QZLnbeO+Osm5F2hcuTh9YZ6ynYXZJ63wtVSSB21nlrV4YJKLJXnxQtF6zNUZZtWfm+RPxtnvSz1+oWg9PubHSd6c5BIZZmIDALADUrgGAGDuVdUlq+pxVfWfVXVaVf1iXAv6ZxkKlLst2vYaSa6W5Pju/v4yu/u3bRz+5kl+0t2fWqbvmG3Yzz9kKAx/rqpeUVUHjFnX4pPb8Zgzu/uzy7QfM17ffPvjbLffGK8vsIzL+D38coalP66zpPuXSZb7Wr4zXl9xvQICALC5LBUCAMBcG5eLeGeS30nyjQxrGv8gQ9E6SZ6Y5JKLHnKF8foHK+xypfblxr7UuO9vrbDJcoXxZXX3G6rqrCSPzbCm9cPHMT6eYTb3R2fd1+ic7j5zGx+TrPz1L3wtV1ihfyMtjHnSCv0L7bssaf9Jd/90me1/Pl5fbK3BAACYhsI1AADz7vYZitbvTPJ7i5cMqapLZjh532I/Hq+vtsL+Vmq/gO4+t6p+uspjrj7rvsb9HZXkqPHkkbdOcs8MRez3VNVNuvub27K7bRl7kQv7Ws5Y1LbwXF/gfUNVXSzJ5bYzw1ILY149y59g8hpLtgMA4CLOUiEAAMy764/Xb19mnevbZskxbXd/L8Os4utV1XKF5d/axvE/m+TSVXWLZfrusI37SpJ095nd/YHuflSSl2Y4yeCdF23yi/F6I2YM/0pVLbccyB3G68VLb5w2Xu+xzPY3yflnui/YnuwLY95haUdVXS3J3hmK1ttS2AcAYAemcA0AwLz71nh9h8WNVXXNJIet8Ji/z7D29XOXPOa6GZfo2AavG6+fV1UXX7SvqyV58qw7qao7jzPEl1qYAX3OorZTx+trbUvQbfD8Fb6WTvL6Rdt9McOJEQ+oqisu2v5yGQruy9me7EdkKHg/vqr+t0g+LhPzl0kuleR1K5ygEwCAiyBLhQAAMO8+mmFG7gOqas8kH09yzST3SHJszltGYrHnJNkvyYFVdZMMJ/27cpL7jrfvlfOWwbgwf5fkPknulOQLVfWuJJce9/WfWXRiyAvxqiRXrKqPZijG/yLJrTLMGv96krct2vZDSR6W5A1V9fYkZyc5ubsPn3Gs1Xwzw/O39GvZNckLuvvTCxt291lV9aokj8twUsl3ZCgi3yXJ15Kcvsz+/zXD+uNPqardk5wytr+4u3+yXKDu/kpVPTXJ88dcb8kw23vfJPsk+XySZ6ztywYAYEdixjUAAHOtu/8nyV0zFJCvk+QxGQq+L0/yu1mmAN3dP85QEH51kmtnKLz+VpKnj49LzlsL+8LG/2WGtaj/PMOSHo9Ocrckf53kQdvwpTw7Q0H6pkkOGi9XTPKsJLfp7rMWjfnWJE/LsNzGwRkK8Y/ehrFW85MM64b/W5L7J3lIkpOTPLS7n7TM9k8YM3aSh2ZYb/wNGZ77XyzduLtPylAIPy7Jg8fsz0ly2dVCdfcLMjzPn0lyvwzfs8tneN5vu50nogQAYAdV3dt7ThcAANjxVNXjkrwkyf27+x+mzgMAAFyQwjUAABdJVXXN8USNi9uul+Q/Mszk3b27T132wQAAwKSscQ0AwEXV+6rq7CSfS3JGkutmWPf6UkkerWgNAADzy4xrAAAukqrq4AxrJV8vwwzrMzOczPGw7n73lNkAAIDVKVwDAAAAADBXdpo6AAAAAAAALHaRW+P6Kle5Su+5555TxwAAAAAAYBWf/vSnf9jduy7Xd5ErXO+555459thjp44BAAAAAMAqquqElfosFQIAAAAAwFxRuAYAAAAAYK4oXAMAAAAAMFcUrgEAAAAAmCsK1wAAAAAAzBWFawAAAAAA5orCNQAAAAAAc0XhGgAAAACAuaJwDQAAAADAXFG4BgAAAABgrihcAwAAAAAwV3aeOsBGO+VVb5x0/F0fdv9JxwcAAAAA2NGYcQ0AAAAAwFxRuAYAAAAAYK4oXAMAAAAAMFcUrgEAAAAAmCsK1wAAAAAAzBWFawAAAAAA5orCNQAAAAAAc0XhGgAAAACAuaJwDQAAAADAXFG4BgAAAABgrihcAwAAAAAwVxSuAQAAAACYKwrXAAAAAADMlUkK11V1sar6bFW9a7x/nar6RFV9o6reXFWXGNsvOd4/buzfc4q8AAAAAABsnqlmXD8myVcW3X9+kpd2915JTkty4Nh+YJLTuvv6SV46bgcAAAAAwEXYpheuq2r3JPdI8nfj/Ury20mOHDc5Ism9xtv7j/cz9u87bg8AAAAAwEXUFDOuX5bkiUl+Od6/cpLTu/vn4/0Tk+w23t4tyXeSZOw/Y9z+fKrqoKo6tqqOPeWUUzYyOwAAAAAAG2xTC9dVtV+Sk7v704ubl9m0Z+g7r6H78O7ep7v32XXXXdchKQAAAAAAU9l5k8f7zST3rKq7J7lUkstnmIG9S1XtPM6q3j3J98btT0yyR5ITq2rnJFdI8qNNzgwAAAAAwCba1BnX3f2U7t69u/dMcr8kH+7uP0rykSQHjJs9MMk7xtvvHO9n7P9wd19gxjUAAAAAABcdU6xxvZwnJTm4qo7LsIb1a8b21yS58th+cJInT5QPAAAAAIBNstlLhfyv7j4myTHj7W8mueUy25yb5D6bGgwAAAAAgEnNy4xrAAAAAABIMuGMawanvPoVk46/60MfMen4AAAAAABLmXENAAAAAMBcUbgGAAAAAGCuKFwDAAAAADBXFK4BAAAAAJgrCtcAAAAAAMwVhWsAAAAAAOaKwjUAAAAAAHNF4RoAAAAAgLmicA0AAAAAwFxRuAYAAAAAYK4oXAMAAAAAMFcUrgEAAAAAmCsK1wAAAAAAzBWFawAAAAAA5orCNQAAAAAAc0XhGgAAAACAuaJwDQAAAADAXFG4BgAAAABgrihcAwAAAAAwVxSuAQAAAACYKwrXAAAAAADMFYVrAAAAAADmypoK11V15fUKAgAAAAAAyYyF66p6SFU9YdH9m1TViUlOrqpjq+rqG5YQAAAAAIAtZdYZ149K8pNF91+S5PQkj01yhSSHrnMuAAAAAAC2qJ1n3O5aSb6aJFV1hSS3T3Kv7n5PVZ2a5C83KB8AAAAAAFvMrDOuL5bkl+Pt30rSSY4Z738nyVXXNxYAAAAAAFvVrIXrbyS5x3j7fkn+o7vPGe9fM8mP1jsYAAAAAABb06xLhbwoyd9X1QOTXDHJfRb13THJF9Y7GAAAAAAAW9NMhevu/seqOiHJrZN8qrs/tqj7B0nesRHhAAAAAADYemYqXFfV7ZJ8prv/fZnuFyb5jXVNBQAAAADAljXrGtcfSbL3Cn03GPsBAAAAAGDNZi1c1yp9l0zyi3XIAgAAAAAAKy8VUlV7JrnuoqZ9qupySza7dJI/TfLtdU8GAAAAAMCWtNoa1w9M8swkPV7+Kuefed3j/Z8necRGBQQAAAAAYGtZrXD9+iTHZChOfzhDcfrLS7b5aZKvd/ePNiIcAAAAAABbz4qF6+4+IckJSVJVd0zy6e4+a7OCAQAAAACwNc16csafJrn7ch1VdZ+qutX6RQIAAAAAYCubtXD9l0lutELfDcd+AAAAAABYs1kL17+e5OMr9H0yyU3XJw4AAAAAAFvdrIXrS62y7cWSXHZ94gAAAAAAsNXNWrj+SpJ7rtB3zyRfW584AAAAAABsdTvPuN2rk/xNVf04yd8mOTHJbkkOSnJgkodvTDwAAAAAALaamQrX3f23VXWDJI9LcvDiriQv7e7DNyIcAAAAAABbz6wzrtPdf1ZVr0py5yRXSvLDJB/s7m9uVDgAAAAAALaemQvXSdLdxyc5foOyAAAAAADAzCdnTFVdtqoeXVVHVtWHq2qvsf1+VfVrGxcRAAAAAICtZKYZ11W1R5Jjkuye5KtJbpzkV8buOya5U5IHb0A+AAAAAAC2mFlnXL84yU+T7JXk/ySpRX0fTXK7dc4FAAAAAMAWNWvh+s5Jntnd307SS/q+m2S3WXZSVZeqqk9W1eer6ktV9eyx/TpV9Ymq+kZVvbmqLjG2X3K8f9zYv+eMeQEAAAAA2EHNWri+RJIzV+i7QpL/mXE/P03y293960luluSuVXXrJM9P8tLu3ivJaUkOHLc/MMlp3X39JC8dtwMAAAAA4CJs1sL1F5Lce4W+uyX59Cw76cFZ492Lj5dO8ttJjhzbj0hyr/H2/uP9jP37VtXiZUoAAAAAALiImenkjElemOTIsWb8j2Pb3lW1f4ZZ0fecdcCquliGQvf1k7wiyfFJTu/un4+bnJjzlh7ZLcl3kqS7f15VZyS5cpIfzjoeAAAAAAA7lplmXHf3UUkenuQ+ST44Nr8hyWOTPLK7j551wO7+RXffLMnuSW6Z5IbLbTZeLze7euka26mqg6rq2Ko69pRTTpk1CgAAAAAAc2jWGdfp7ldX1d8nuU2SqyY5Ncl/dPdKa19f2P5Or6pjktw6yS5VtfM463r3JN8bNzsxyR5JTqyqnTOsp/2jZfZ1eJLDk2Sfffa5QGEbAAAAAIAdx6xrXCdJuvvs7v5gd/9jd79vW4vWVbVrVe0y3r50kjsl+UqSjyQ5YNzsgUneMd5+53g/Y/+Hu1thGgAAAADgImzFGddVdbskn+nus8bbq+kMM7CP7+6frrLdNZIcMa5zvVOSt3T3u6rqy0neVFXPTfLZJK8Zt39Nkr+vquMyzLS+30xfFQAAAAAAO6zVlgo5JsMyHp8cb88y0/nUqvqj7v7Acp3d/YUkN1+m/ZsZ1rte2n5uhnW1AQAAAADYIlYrXN8xyZcX3b4wV0jysCQvTnLTNeYCAAAAAGCLWrFw3d0fXe72aqrq7CTvXYdcAAAAAABsUavNuL6AqrpSktskuVKGNa0/3t0/WrTJR5Lsun7xAAAAAADYamYuXI8nTnx8kkskqbH5p1X1ou4+JEm6+5dJzlj3lAAAAAAAbBkzFa6r6rFJnprkNUnemOT7Sa6e5P5JnlpVp3T3yzcsJQAAAAAAW8asM64fmuSw7n7coravJfloVZ2V5OFJFK4BAAAAAFiznWbcbs8k716h791jPwAAAAAArNmshetTk9x4hb4bjf0AAAAAALBmsxau35bkOVX1x1V18SSpqp2r6g+THJrkrRsVEAAAAACArWXWwvVTknwuyRFJzqmqHyT5SZJ/SPL5DCduBAAAAACANZvp5IzdfWZV3S7JPZLcNsmVkvwoyUeTvLe7e+MiAgAAAACwlVxo4bqqLpHkYUk+1N3vSvKuDU8FAAAAAMCWdaFLhXT3z5I8L8MsawAAAAAA2FCzrnH9lSTX3cggAAAAAACQzF64fkaSQ6rqJhsZBgAAAAAAZjo5Y5InJblcks9W1beSnJRk8QkZu7tvv87ZAAAAAADYgmYtXP8iyZc3MggAAAAAACQzFq67+w4bnAMAAAAAAJLMvsY1AAAAAABsilmXCklV7ZLkcUluk2S3JN9N8h9JXtbdp29MPAAAAAAAtpqZCtdV9etJPpjkCkk+nmG966sleWqSh1fVvt39xQ1LyWROeuUhk45/jYc/Z9LxAQAAAIDNN+uM65cnOTXJPt19wkJjVe2Z5Ogkf5XkDuucDQAAAACALWjWNa5vkeSQxUXrJOnubyV5ZpJbrnMuAAAAAAC2qFkL16cm+ekKfeeO/QAAAAAAsGazFq5fleQJVXWpxY1Vdekkf5bkFesdDAAAAACArWnWNa4vk+TaSb5dVe9J8oMMJ2e8e5KfJLlsVR06btvd/cx1TwoAAAAAwJYwa+H6qYtuP2CZ/qctut0Z1r0GAAAAAIBtNlPhurtnXVIEAAAAAADWREEaAAAAAIC5onANAAAAAMBcUbgGAAAAAGCuKFwDAAAAADBXFK4BAAAAAJgrKxauq+qoqrr+ePsBVXXlzYsFAAAAAMBWtdqM6/2TXGm8/bok19v4OAAAAAAAbHWrFa5/kOQ24+1K0hsfBwAAAACArW61wvVbkry0qn6RoWj98ar6xQqXn29OXAAAAAAALup2XqXvcUn+PcneSZ6Z5PVJvrsJmQAAAAAA2MJWLFx3dyf55ySpqgclOay7P79JuQAAAAAA2KJWm3H9v7r7OhsdBAAAAAAAktXXuD6fqrpGVb2oqj5VVcdX1Ser6gVVdfWNDAgAAAAAwNYyU+G6qn41yeeTPDrJWUk+meTsJI9J8rmq2mvDEgIAAAAAsKXMtFRIkucnOSPJLbv7WwuNVXXtJO8f+39/3dMBAAAAALDlzLpUyB2THLK4aJ0k3X1CkmeN/QAAAAAAsGazFq4vkeTMFfrOHPsBAAAAAGDNZi1cfy7Jo6rqfNtXVSV5+NgPAAAAAABrNusa14cmeVeSr1TVm5OclOTqSe6TZK8k99iYeAAAAAAAbDUzFa67++iq2i/Jc5M8LUkl6SSfTrJfd79/4yLCyk54+b0mHf/aj377pOMDAAAAwEXRrDOu091HJzm6qi6T5IpJTuvuczYsGQAAAAAAW9LMhesFY7FawRoAAAAAgA0x68kZAQAAAABgU2xq4bqq9qiqj1TVV6rqS1X1mLH9SlX1gar6xnh9xbG9qurlVXVcVX2hqn5jM/MCAAAAALD5NnvG9c+TPL67b5jk1kkeUVV7J3lykg91915JPjTeT5K7JdlrvByU5FWbnBcAAAAAgE22qYXr7j6puz8z3j4zyVeS7JZk/yRHjJsdkeRe4+39k7yhBx9PsktVXWMzMwMAAAAAsLkutHBdVZeoqs9U1e+s58BVtWeSmyf5RJKrdfdJyVDcTnLVcbPdknxn0cNOHNsAAAAAALiIutDCdXf/LMl1MizzsS6q6nJJ3prksd3949U2XS7SMvs7qKqOrapjTznllPWKCQAAAADABGZdKuQDSdZlxnVVXTxD0fofuvuosfkHC0uAjNcnj+0nJtlj0cN3T/K9pfvs7sO7e5/u3mfXXXddj5gAAAAAAExk5xm3+6skb6yqnZO8PclJWTLzubu/eWE7qapK8pokX+nulyzqemeSByZ53nj9jkXtj6yqNyW5VZIzFpYUgXn3uVfdc9Lxb/awd046PgAAAABsr1kL1x8drw9O8rgVtrnYDPv5zSR/nOSLVfW5se2pGQrWb6mqA5N8O8l9xr73JLl7kuOSnJPkT2bMCwAAAADADmrWwvW6FIy7+9+y/LrVSbLvMtt3kkesx9gAAAAAAOwYZipcd/cRGx0EAAAAAACS2U/OmCSpqp2q6sZVdfuquuxGhQIAAAAAYOuauXBdVY9I8v0kX0jy4SQ3GNvfXlWP3ph4AAAAAABsNTMVrqvqIUkOS/L2JPfN+dep/tck917/aAAAAAAAbEWzzrg+OMmLu/ugJG9b0vfVjLOvAQAAAABgrWYtXF8nyftW6Ds7yS7rEwcAAAAAgK1u1sL1D5PsuULfDZJ8d13SAAAAAACw5c1auP6XJM+oqusuauuqukqSx2VY+xoAAAAAANZs1sL105P8NMl/Jflgkk7y8iRfSfKLJIduSDoAAAAAALacmQrX3X1qkn2S/GWSiyc5PsnOSf46yW26+4wNSwgAAAAAwJay86wbdveZSZ4zXgAAAAAAYEPMXLhOkqq6fJIbJ9ktyYlJvtTdP96IYAAAAAAAbE0zF66r6hlJHp/kcklqbD6zql7Y3c/diHAAAAAAAGw9MxWuq+rZSQ5J8ndJ3pTkB0muluQPkzy7qnbu7mdtVEgAAAAAALaOWWdcPyTJi7v7CYvavpTkw1V1RpKDkjxrnbMBAAAAALAF7TTjdldI8r4V+o4e+wEAAAAAYM1mLVx/IsktVui7xdgPAAAAAABrtuJSIVW1uKj96CRvq6qfJ/nnnLfG9X2T/GmS/TcyJAAAAAAAW8dqa1z/PEkvul9JnjdesqT9CxeyLwAAAAAAmMlqxeZDc/7CNQAAAAAAbLgVC9fd/axNzAEAAAAAAElmPzkjAAAAAABsipnXpa6qGyY5IMkeSS61pLu7+4HrGQwAAAAAgK1ppsJ1VT0gyWszrHl9cpKfLdnEWtgAAAAAAKyLWWdcH5LkHUkO7O7TNzAPAAAAAABb3KyF66sneaiiNQAAAAAAG23WkzP+e5IbbmQQAAAAAABIZp9x/cgkR1XVqUnen+S0pRt09y/XMxgAAAAAAFvTrIXrE5N8NskbV+jvbdgXAAAAAACsaNZi898m+YMkb0/y1SQ/27BEAAAAAABsabMWrvdP8oTuPmwjwwAAAAAAwKwnZzw7yZc3MggAAAAAACSzF65fl+T/bWQQAAAAAABIZl8q5IQkf1hVH0hydJLTlm7Q3a9dz2AAAAAAAGxNsxauXzVeXzvJvsv0dxKFawAAAAAA1mzWwvV1NjQFAAAAAACMZipcd/cJGx0EAAAAAACS2U/OCAAAAAAAm2KmGddV9d8Z1rFeUXdfd10SAQAAAACwpc26xvVHc8HC9ZWT/N8kZyX58HqGAgAAAABg65p1jesHLddeVbskOTrJB9cxEwAAAAAAW9ia1rju7tOTvDDJM9YnDgAAAAAAW916nJzx3CS7r8N+AAAAAABg5jWuL6Cqdk5y4yTPSvKl9QoEbI5//dv9Jh3/tg9516TjAwAAADC/ZipcV9Uvc8GTMy74cZJ7rFsiAAAAAAC2tFlnXB+aCxauz01yQpL3dvcZ65oKAAAAAIAta6bCdXc/a4NzAAAAAABAkvU5OSMAAAAAAKybFWdcV9UztmVH3X3o2uMAAAAAALDVrbZUyLNmePzida8VrgEAAAAAWLPVlgq5+IVcbpHk/UkqyXEbGxMAAAAAgK1ixcJ1d/9iuUuS6yZ5Y5JPJNk7yUHjNQAAAAAArNlqS4WcT1XtkeSZSR6Q5LQkf5bkld39s23Yx2uT7Jfk5O6+8dh2pSRvTrJnkm8luW93n1ZVleSwJHdPck6SB3X3Z2YdC9ixHf2au086/l0PfM+k4wMAAABsZastFZIkqaqrVtVhSb6e5N4Z1rK+bne/bFuK1qPXJ7nrkrYnJ/lQd++V5EPj/SS5W5K9xstBSV61jWMBAAAAALADWrFwXVVXqKq/SHJ8kgMzzH6+bnc/t7vP3p7BuvtjSX60pHn/JEeMt49Icq9F7W/owceT7FJV19iecQEAAAAA2HGstlTIfye5QoYTMD43yUlJrlhVV1xu4+7+5nZmuFp3nzTu46SquurYvluS7yza7sSx7aSlO6iqgzLMys61rnWt7YwBAAAAAMA8WK1wvct4fZckvzPDvi629jjnU8u09XIbdvfhSQ5Pkn322WfZbQAAAAAA2DGsVrj+k03K8IOqusY42/oaSU4e209Mssei7XZP8r1NygQAAAAAwERWLFx39xEr9a2zdyZ5YJLnjdfvWNT+yKp6U5JbJTljYUkRAAAAAAAuulabcb3uquqfktwhyVWq6sQkz8xQsH5LVR2Y5NtJ7jNu/p4kd09yXJJzsnkzwAEu1JGvu+uk4x/wJ0dPOj4AAADARtrUwnV3/+EKXfsus20necTGJgIAAAAAYN7sNHUAAAAAAABYTOEaAAAAAIC5onANAAAAAMBcUbgGAAAAAGCuKFwDAAAAADBXFK4BAAAAAJgrCtcAAAAAAMyVnacOAMD6O+L1vzPp+K5PPWsAACAASURBVA980PsnHR8AAADYsZlxDQAAAADAXDHjGoBN98o33mXS8R9+//dNOj4AAACwOjOuAQAAAACYK2ZcA8ASz3/TtDPCn3Q/M8IBAADY2sy4BgAAAABgrphxDQA7kCceeddJx3/BAUdPOj4AAABbgxnXAAAAAADMFYVrAAAAAADmiqVCAIB1c693TLuUydv3t5QJAADARYEZ1wAAAAAAzBUzrgGALeNub3/spOO/914vm3R8AACAHYXCNQDAnLj725476fjv+b2nTzo+AADAAoVrAABmco+jpp0x/u7fn3bGPAAAsHkUrgEAuEi4x1sPn3T8d9/7oEnHBwCAixInZwQAAAAAYK4oXAMAAAAAMFcsFQIAAJtgv7e+YdLx33XvB6zYt9+Rb97EJBf0rgP+YNX+3z3yHZuUZHn/csD+q/bvf+T7NinJ8t5xwF0mHR8AYCMoXAMAAFyE/d5b/23S8d9279+adHwAYMekcA0AAMBkDnjr5yYd/8h732zS8QGA5SlcAwAAwAr+6KgTJh3/H37/2pOODwBTcXJGAAAAAADmisI1AAAAAABzxVIhAAAAsIP6y7edNOn4T/m9a6zYd8RRp2xikgt64O/vumr/v7zlh5uUZHm/e9+rrNr/sTdO+/zd7v6rP38AG03hGgAAAIBt8tm/O3nS8W/+4Kuu2n/8y7+/SUmWd71HX33S8eGiwFIhAAAAAADMFYVrAAAAAADmiqVCAAAAAGATff+FJ0w6/tWfcO1V+7//ki9uUpLlXf3gm0w6PvNB4RoAAAAA2CH84LCPTzr+1R5z61X7T/6rD21SkuVd9VH7Tjr+elK4BgAAAADYAk5+xTsnHf+qj7jnzNta4xoAAAAAgLmicA0AAAAAwFxRuAYAAAAAYK4oXAMAAAAAMFcUrgEAAAAAmCsK1wAAAAAAzBWFawAAAAAA5orCNQAAAAAAc0XhGgAAAACAuaJwDQAAAADAXFG4BgAAAABgrihcAwAAAAAwVxSuAQAAAACYKwrXAAAAAADMlbkvXFfVXavqa1V1XFU9eeo8AAAAAABsrLkuXFfVxZK8Isndkuyd5A+rau9pUwEAAAAAsJHmunCd5JZJjuvub3b3z5K8Kcn+E2cCAAAAAGADzXvherck31l0/8SxDQAAAACAi6jq7qkzrKiq7pPkLt394PH+Hye5ZXc/asl2ByU5aLx7gyRfW8cYV0nyw3Xc33qTb23mOd88Z0vkWyv51ka+7TfP2RL51kq+tZFv+81ztkS+tZJvbeTbfvOcLZFvreRbG/m23zxnS7Zevmt3967Ldey8joNshBOT7LHo/u5Jvrd0o+4+PMnhGxGgqo7t7n02Yt/rQb61med885wtkW+t5Fsb+bbfPGdL5Fsr+dZGvu03z9kS+dZKvrWRb/vNc7ZEvrWSb23k237znC2Rb7F5XyrkU0n2qqrrVNUlktwvyTsnzgQAAAAAwAaa6xnX3f3zqnpkkvcluViS13b3lyaOBQAAAADABprrwnWSdPd7krxnwggbsgTJOpJvbeY53zxnS+RbK/nWRr7tN8/ZEvnWSr61kW/7zXO2RL61km9t5Nt+85wtkW+t5Fsb+bbfPGdL5Ptfc31yRgAAAAAAtp55X+MaAAAAAIAtRuEaAAAAAIC5onANAAAAAMBcUbjeQVXVNavqZlX1G4svU+difVXVxafOsKOoqp2qaqdF969eVQ+uqt+cMteOoqr2rqobLLp/56p6Y1U9paouNmW2eVdVu1bVrovu36SqnltVfzhlLtaXv7vARquq61fVpabOAfPG78baeP4uGub9/VpVPWSVvldvZhYuWpyccYmqut0KXZ3k3CTHd/ePNjHS+VTVzZO8McmvJakl3d3dk79gJcMb/CRXzZIPR7r7M9MkSqrq60k+kuSYJMd090lTZVlOVT06yXe7+63j/dckeWCS45Pcs7u/NnG+SyV5TJJ9s/z39qZT5FpQVe9NcnR3H1ZVl0vy1SSXTXK5JAd29xsmznf7JOd29yfG+w9K8uAkX0ry+O4+a8J4qar/THJYd7+pqnZP8vUMvys3TfL33f2UCbPdN8np3f3+8f4zkhyU4bl70NS/y1X1kQzP0Wur6ipJvpHke0l2T3Jod7944nxXSvLnWfl39/JT5FpqfI3ZL8n1kvxNd59eVddLcpq/u2yUqrpZd39u6hw7sqq6TJKbZfnXl6MmCbUDqKq/SPK17j6iqirJ+zO8Tp+R5K4Lxwssb56PS8fjguXeZC+8nzwuyRFTvS+a9+OCef/dqKr/zoV/f1/T3e/c1GCjeX/+5l1V3SvJwUn2Hpu+kuQl3f226VIN5vn92pjvtCQPXqhnLGo/PMlduvva0yTbMcz7a8uUFK6XqKpf5rwfloU3qIvv/zLJO5P8cXefvcnxUlWfSnJqkkMzFEbO9w3s7hM2O9Ni8/wGf/wE8Pbj5ZoZfvGPyZwUsqvquCR/2t0fGz9AeXeSA5PcO8llu3u/ifO9NsnvJfnnLP+z9+wpci2oqpOT7NvdX6yqByR5cpJfT/JHSQ6eg8L6Z5M8q7vfMX5S/oUkr0nyW0n+vbsfNnG+05Pcsru/XlWPy/BhyR2r6o5JXtfde06Y7ctJHtvd7x9nuP5HkmckuWuS73f3/5sq25jv1CS37e4vV9VDM3xQcouq2j/JC7v7VyfO97YkN09yeJb/3T1iilyLVdX1k3wgya8k2SXJr3b3N6vqRUl26e4HT5ht3v/uvnaFrsUHuW/u7u9tXqrz7AAFnF8m+WySv0vyj919xhQ5VlNVt8rKBaZHTxJqVFV3SvJPSa68TPfkH+yMH3QuZ/HP39Hd/ZPNSzWoqhOS/EF3f7yq7p7kiCT3yHDcctPuvuNmZ1pqzn/25va4tKpemeT/JTkpyafG5lskuXqSt2coMt00QxHxQxPkm+vjgnn/3RhfVw5O8onxkiS3SnLLJK9OcoMk90xy/+5+0wT55v35m9viXFU9PslfJHlDkv8cm2+T5P5JDunuF212psXm+f3amG/fJEcl+f2F17axaH3XJHfo7m9OmW9BVf1BVv7bds9JQmWHeG2Z7JhK4XqJqrpbkhdm+BR68Q/LU5I8M0Ph+qVJ3t3dj5og39lJbt7dX9/ssWcx72/wF4xFkjskuXOGg96dunvniTP9JMkNuvvbVfXCJFfu7j+tqhsm+dfuvsrE+X6U5L7d/cEpc6xkfP5+tbu/U1VvTHJCdz+tqq6V5CvdfdmJ8/04yc3GYtxTk/zf7t5vfFP41u7efeJ8Zya5SXd/q6releSj3f3C8fn7WndfesJsZyfZu7tPqKrnJNmru+9XVTdL8r7uvtpU2cZ85yT5tfF398gkn+/u51TVHkm+PuVzN+b7cZI7z/MMm/Fn7ntJHpbk9CS/Pv6u3C7Dgfj1Jsw27393/yXJbTMcn/zX2HzjDB8efzrJjTL858ltp5hZvAMUcPZK8qdJ/jjJlTK84XpNd39ks7Msp6r+LMkLMrwZWHpc1d3925MEG1XVlzJ8X5861Ycjq6mqLya5Vob/wFrId80kZyc5JckeSU5OcvvNfkNdVecmuX53n1hVf53hfdkjxmPUY7t7l83Ms0y+ef/Zm9vj0qp6SYb3Fo9d0v7iDM/dn1XVYRkKULeZIN9cHxfsAL8br0/y1e5+3pL2J2Y4Xn3QeKx/n+6++QT55v35m9viXFWdlOQZ3f23S9ofkuG/KK+xmXmWmuf3awuq6t4ZJgPcNcN/F/9OkjvOUdH6hUkem+E/8ZerV/3JFLmSHeK1Zbpjqu52WXTJ8CZv32Xa75Tk0+Pt/ZL890T5Pp7kdlM/T6vkOztD8XDyLCvk2ynDH8YnJXlfkrOS/HeGwsjU2X6Q5P+Mtz+X5I/G29dPctYc5DsxQ2F98u/jCvm+luR+4wvpKRk+1U2Gf18+ZQ7ynZGh4JokH0ryyPH2tZP8ZA7y/WeS52cogP0kw0FRMswy+M7E2U5NcuPx9n9k+Be0JLlOknPm4Ln7fIYDoD2S/DjJrcb2fZKcNAf5jktyo6lzXEjGHy387UhyZpLrjrf3nPr3Ywf4u/vkDDNeL7Oo7TJJ/iHJE5NcIsmbknxoonwvSfKyZdpfnORF4+3DkvznxM/jThlmpB2Z5KcZlul6WpLdJ871nYW/F/N4GY/7rjd1jlXyPSjJBxd/HzMs4/T+JA/IMFP8Q0neMUG27yb5zfH215Pce7z9a0nOmIPnbt5/9ub2uHQ8btlrmfZfTXLqePtGU32f5/24YAf43fhxhsLw0vbrJ/nxePsGmej92w7w/L0+yZOXaX9iktePt5+a5LMTZDtzle/tmXPw3M3t+7UlOR8yHkt9K8meU+dZku0HSQ6YOscK2eb9tWWyYyonZ7ygvTO82C/13Zy3ztEXM8wUmsJTk7ygqu5UVVerqistvkyUabEpn5tVVdW7k5yW4Q3+r43XN+ru6/SEn6wt8v4kf1vD2tbXT/Lesf1GGYrrU3tBkoNr0QkQ58xLkvx9hjcy303ysbH9dhl+Lqf2qSSHVNUfZzjYWPj+7pnk+1OFWuRJGQ4yjknyT9298JzdM8knpwo1+tckL66qQzIUg98ztv9qhjfWU3t2hoPIbyX5eJ83g+kuGZYgmNrTkhxaw9rv82y5k9FeK8OHPlOa97+7j8kwC+ichYbx9p8neVx3/yzDz+fNJsr3wCSvWKb9b5Is/O09POcdY02iu3/Z3e/O8O/AT06yW5LnJPlmVb2pqnabKNrlc95r3jz69wxvoubVMzMsF3biQsN4+4kZfm9OzfAaeesJsr01yT9W1QcyzPY/emy/WYbC4tTm/Wdvno9LK8Px+1J757ylFP8nw3/KTGHejwvm/XfjnAzH8kvdduxLkotlKCxOYd6fv9/P8CHxUkeNfcnwNey1aYnO8/YkByzTfu8My8VObe7er1XVy5dektwkQ4H4ixlepxfa58FOGSYJzqN5f22Z7Jhq0qUR5tSXkzytqh7c3T9Nkqq6ZIY3rl8et9kj0xWaFv4d7v05/7811Hh/09cSXPLGfeEN/tMzvFD9z+Jte8ITbGVYFuT0DAXDj2RY1/qHE+ZZ6hFJnpthBu4Bi56r38hQZJ/anTO8aN61hjWHl35vJ1sPahz/b6rq2AyFrg9098KbgeOTHDJdsv/12CT/mGT/JH/e3ceP7ffJMIt4Uj2srb5rkst392mLuv4m5/2hnMojk7wqw4HkQ/u8f0e/W4b/nJhUdx81/oveNTPMvl7wwQwH3lN7eoYPSE4e1z1c+rs76frvo/dn+LfRA8f7XVWXz/ChwLsnSzWYu7+7S1wuyTUynDxosauPfckwg2OqY76FAs43lrTPSwEnSVJVt8ywZMgfZHi+npfktRme2+dkeDN7iwmi/VOGf7d95QRjz+LVSV5Uw0m5lzvum+yk3KOrJbnUMu2XzLC2ZTK8ub7MpiU6z8FJTshw3PLEPu/cOdfI8DdvavP+szfPx6VHJHnNuBTRpzL8rbhlhqLT68dtbp/zlnfabPN+XHBwhskA1858/m4cluSVVbVPzv/9fVCGvxfJ8LszVXFs3l9bFopzS4vokxTnqurgRXePS/Lkcc3ohTWubz1eXrIZeVYzp+/XbrJC+/EZjkMX+udljeLDM0xSeNbEOZYz768tkx1TWeN6iXG92X/J8EnMf2X4YblJhjdU+3X3J2s48dvVuvuFE+S7/Wr93f3RzcqyoM5/Qsvkgie1XGjrnvbkjJdO8psZ1ra+Q4aZm9/IUMT+SE94puCq2jnJQUne3nO4RmSSVNXrVuufetZ6VV1vUTF4ad++PcHaqbOoqksl+UV3/8+FbsxcqqpbdPenVui7f3e/cbMzLcnwzNX6e+ITqybJWPRaWFP4uv8/e+cdLldVvf/PS6RXRanSi9SAVBEkFBEEDEVBERAB6QiKFGmCKL0jIC2gKEWqVAXEBAQbRaSEDjFU6aK0UN7fH2ufe889d+YmfjWz9/ibz/PwZHLmXrKemVPWXnut9yU61Rcmkp/Vbb+YMbbinrt1JJ1PLPb2ZWCSewxwq+1tJG1BdEh0vPAq6URifPAoBhdwzre9V9KO/JrtVl0mkzu+vYiC9SLEJsk5hLHMB7WfWZjQHOx48V/SgcTG542EqW+zwJR1IZ1ywHZkzfsAJF1NNJzsSMgBAixPLPKfsr2RpJHADztdrJM0rdsYGEn6eL2jKQddcO4Vm5dKGgbsA+xB/yTq80RR4jjb76cN7w9yfM8l5wWSpiQmhk5zId5IrZD0FeL7XSwdegg42fYv0vvTEvfAtzOFWCyS9idM1s+lRXHO9lHp2fx52+t0IJ5JnWy27QUnazCTiKSPAgsB91TNlj0mDUmnEd4rY2n9bMttPFzsvSVnTtUrXLdA0vTELswniILrg8AFtd3KHjUmtqivk3uBXyctRA8kvuspClhc9RnQ5YyjW5H0OKHn9nzj+GeBK23PmCey7iAV0PekvcNy7u6bYpH0AlFcfahxfGvgDGc2Bu0WUiK2BTFlMgVwN/HszTUO1xULaEnTEV1A29LfVf0esSDc2/YbCiNTnMecsfQCzqPAKMLr4u9tfmYqYAvbP+1ocEx0QZ19ES1pvqHez33dSJoNOJ8wh3o/HZ6CKMZuY/uF1Fk3pe0bOxzbr4Av2H6vcXwe4Le2c4zJ1+Mo+tzrFtL0ELZfzx1LtyDpX4S3ybjcsXQLkpYjipgfpNdD8S/CxD5bwbPk4lzJSJqRyO++SBT8F3GYmZ8BPG/70JzxNak1Dj6aOx+okDSU+bad2Xi4ZHLmVL3CdRciaXZCVmIJ4ob1APDjdguuHkG60NYA1kx/Lkq4nt5CdFyfmS04QNLNRHHkipxxTAxJC9J/7j3ochyCTyP0rFez/Y90bB1ivHuv3N9vimdbojA3L2GY1kfuBaCkc4FNgEtp7bCcs/umOdVR521irG+U7SzaaZL2I+7Jq9p+Kh37GjGO+WXb1+aIq4mktag9N2yPyRtRd9AtC+i06b4QseH+WImb7SUWcCTND4yvd1in4wLmsT0+R1w9/rtI+gS1hhTbj2QOCUm3E+feFrVj8xDapb+3vXWu2LqJUvPSbqDUvEDS5cB1ts/NHcvEkDQLg5s9Oi6NmXLlOVLhqMqbNcSvvA7sVBWKe3QHkk4HliHWHbcBw1PhekNCinKZzPH9BPiz7dPTpv9dhFzcBGAT278a6vd79FPKvaUVOXKqXuG6BSlp/Aytuw5zj8WtShgs/J1+3aVViFjXtf2Hdr/bCSTtDrzWHI2XtBWhxZRNJy89xJ8nTPvGEBrXDw35Sx0k7TwfAZxC3OQHFB2cWScyFRxGETu81QJfhIbv9rb/mSs26CsyXEgUhT8LrEYUrb9t+6ycsQFI2gfYnxil+TahGbkwUWw/zvYPM4aHpFeAzW3/ZqI/3GEk7UrokF0JVMaHKwMbE6Zz8wA7AfvZ/lGmGI8DNiTOuw2IovVmDrO3rChM5a4kRrkqKaK5gDuJJDK7PJGkw4kRszMax3cG5radTae+mxbQPf59JL0PzGn7hcbxWYEXck9j1VEYqbm0TQlJw4G96S+AjSWeayUYIxeLpA8TzRO32t49TR6MJnwvvuaCFmklnnsl56UK/5/DaT/FNlOOuCpKzwtS3vc94GJar4myNvmkSZMziGaourF0NmnMFNN4257YJAyhR7sZsIPt+Sd7cENQQnEuNe9MEra3m5yxTAxJTxPX6B2S/gkskwrXlWxI1gljSc8BG9i+W9KXgOMIGZjtiLhXzhlfnTRtvDCRtzxeQnd/ifeWUugVrhtI2pIYv3gPeJGBXX7Zx+Ik/YEwv9m56g5SuGmfQXSEfTpzfI8RyeItjeOrEWO42cYeJS1WUqG6icrXiTwP+DShaVSZCa5KnHu3296+3e92CoVW+DXAh4GliKL12XmjCiQ9Ahxg+7JGonEwMK/tHTLH9zSwtu2Hc8bRCklXAVfbHtU4vj0w0qGntTPwTdtLZgmSvmtkdUIOYTPb1+eKpU4qvM4FfNX2k+nYgsDPgWdtt3JP7yiSxhOf2Z8ax1cELrM9sUXYZKMLFtBFy/x0QQHnA8K35MXG8fmAsS5A6kfSboQm+Nzp0NPA0TmbASoUWoZXAL8jur8gNvBWAza1fU2u2CokfZn2519WY2lJcxKf2/XA+sDtxLhtEQu0ws+9YvNSSVcCnyRMwFpNsXVcdqhO6XlBF6yJfgvMQhTlWn2/xUhjtiNtnI2yvWmGf7uo4pyk5nNqdWIzrNp8XYp4dtxawDPjDWDptIasryeXJZryZskc39vAwraflnQO8A/b30nTbfflLqwDlQzgEcDuxAS0gHeAHwEHOqPvVDfcW3LlVL3CdQOFTu4vgINtvz+xn+80kt4Clm0WlyQtBvzF9rR5IuuL421gseZIdbpZPZg7Pih3pHBiu+POrxP5MrCx7d81jq9OaEjPmiGmVhpu0wMXANcSJltAER3rbxLXxniFJvLnbN+j0Fr/s+2PZI5vD2KUaxc3RuZzk6QalrX9WOP4wsBfbU+fOg3us/1fdzFuE1OrRH8YcDyh89VXtC6gsPk6sEbzGlA4Vt9se+Y8kQ2I5W1C4/+JxvEFieJhKwfrjtAFC+hiZX6g3AKOpEpaaDfgPODN2tvDiA6hCbZX7XRsdSQdQEzrHEd/YfgzwF7AEbaPyhUbgKR7iRzgkMbxw4CNnH9s+VjCYHA0rc+/rMbSAOn5dRtwg+2vZw6njy4494rLS2sxvA6s09yMLYVuyAtKJuWln7J9f+5YJoak+4D1naTsSqDk4pzCOPKTwLbVhIlCim0Usc44PFdsKZYxwC9tn5QK18NtPynpx8B8ttfPHN84YGfgJmAcsKPtX0laiij8Z13vAkg6gZDu/C4Dn21HEt46e2eMreh7S86cquPu6F3A7MA5JRatE/8AFgCaXZELAK91PpxBPA8sS9yo6iwHvNTxaGq0GylMXQfZpS5yF6YngWmBl1scfwXIVVS6k8EabtXfdybkI5SO5R6teR74KDAe+Bsh8XMP/SNKHUfhDFxndWA9SWMZ7LCcs8PgZUIW5LjG8Y3pv6/MQNwfO8VlQ7y3XfoPyjj32lHSBsV4ImlsbiSuTnT4ZcP2FBP/qaxsTHSrFyfzk1ibMgs4S6c/BSxO6C9WTCDMQZv3nBzsTCz8Lqodu1lhKnkEkLV4SPiF/KzF8Z8B+3Y4llZ8jTDWHOqe3TFSoaHVM39q4Ev1TdHc0wiUf+6VmJdWvEAY4HUbJeUFJfMkcc12A/MzsKu5BFai3OLcHsQEat90ncPk+gfAzcQEWU4OAG6QtCRRy9srvV6JyJlzcy7RBPosYd53czq+MmHAWQJfBbZrTMY+LulFouktW+Ga8u8t2XKqXuF6MNcTF1YRXbgtuBgYJWlfYizOxDjmUcBFQ/1ih7gQOCWNsYxJx9YETiK6YHNyMjCciKc5UngSUILUxeeJ7q8FCc3ypyR9A3jS9s1D//Zk53bgB5K2tv0m9O1Af5/+z7PTLJDp3/2/8FtgJFEMGQWcKGlzYlPnkkwxNRd8V2aJYuJ8HzhbYSL0Z+K+txLhaFxJrKxDaIV2hC4oZta5mbgvb+F+88h5iXti7vtKxZnENTEVca1AFDyPJHTMe7TnTaCYTqoWFFnAsb0m9MkN7OmCDCMbzAbc0eL4n4lmi9y8QOjkPtY4vjzhx5KbKYhN4lLYPXcA/waln3sl5qUVBwKHSdrGdnH3PwrPCyQJ2IVYEy1AyGE+Iem7wBO2c+XNFXsCR0ratTkN2GOSKLk4NwMhozO2cXxOoCNTnUNh+/eSPk0UVx8ncuW7gVVcgK+E7cMkPUB4Tl1qu2oKeI9y8vmZic+uyePEJEBOSr+3ZMupelIhDSTtABwMnE/oGjW7DnOPfE8FHEt0QVQbD+8SRmD71W4OWUiaQecDXyZ22SBO8EuBrTNrBhU7Upji2JIoop9DfL9LpiRtJ0Inct3M8S1FGINOD9xLFA+XIfRe17X9QMbYpiR0+Q6w3epBlB2FFv0Utt9Lf/8ysXHyCHBmzmujG5C0CvBNYDGiQ/JB4BTbf8wc15TEmNnXmhJOpaAwHL6K6DCtxrrmJq7jjWxn7WiukHQkMX42VTo0ATjZ9nfzRRUkneb1iER8qvp7tg/LElSiZJkf6LvXbU7o9pZYwCmaJMVxWfM8k3QIkRvkluI4GPgOkZvWGyr2Bo4tYKz6cOBd24fmjKOJwpNjR2LkO7tBbiu64NwrOS+9j+h0HUZM2TXXk7m9B4rOCyR9i5jYOJpozqrWRFsThoJZO0vT5MTUxPf7DlGU66OAaYk+JF1PTBY/lzuWitSI8l2guOKcpJ8QxeB9gGqN8SniXBxdkpxTj/8bkv4I3GV7t8bxHxPSlKvkiaz8e0vOnKpXuG5QupZlhaTpgIWIAs5jVadBKSTt2U8S8d1dwkMpaQyvYHts4/hSwJ+c2YBJ0l+BI21frIFmC8sAN9rO3t0iaVpgK/qLh2MJLai3sgYGSHoVWL6pkVsCqbh5OHBa6ZIwSWtz8fTXsSV+nqWRNMtXs/1I7liGQtI61K7dEqUlUrfcEvTHmL3QKelTwHVEAvkx4Bmi8+YdYFwBBYhrCJmVfxD35JJkfoos4CSZpK1sv95CMmkABXx+mxJTOWOIDtOqMDyCkIj5Zb7o+jojv0UUr+dKh58lCtmnOPNCQ9JpxFjwWKIo1zz/9sgRF1CZbC1Ral5Q+rkH5ealqbjfFmf2HqgoNS+Q9BDwHdvXNdZESxI6ubmbjbYZ6n1nNt8snZKLc+mecjwh+VdJrLxHTMvunaPmIukjtl+pXg/1s9XP5SRtzK5E62aP87MEVSM1LV5P5Cp/IJ5tqxA5zOdt3zbEr0/u2Iq+t+TMqXqF6x7/NVJx7ilCFypbl0M7JN0EvE50ftdHCs8HZrK9Tub43gQWt/23RpK2EHC/CzC2LBlJ0RrDsAAAIABJREFUowizzRI0SQeRzBaWcsO4tBQkzUokZSOpacATJpfb2W6lI9lRJM1Fawfj3Mabx6Y49skZR4/Jg6TfAX8hxvdep7+j7yJglO2sMlhJ6qItzmw+V2IBJ31me9j+Z+mfH4Ck5YFvE5uKVXHueNt/yRpYA0kzAjizZ0gdSaOHeNu21+pYMA0k3UxsaGed5hyKbjn3evxvIektwtC8uSZaFLjHHTLi7lYkPUKYp40huoSfzxvRQEovzkFfjaDeJPjGRH5lcsbyPjCn7RdSk2WrAp4ooMlS0mLANYTEj4gJ/A8RBc53cncMV6Q15W4M3PQ8vdQJqFLImVP1CtddQJd1Bj1FjOc1daGyU/JIIYCkx4hR75saSdq2RNfBUhli2hS4xva7qhkGtSL3wisVR75N6BzfSXyvfdg+IUdcFQoT0Otsn5szjnZIuhJYhDC0rEzUViZkiB6zPeT3P5lj+yQhBVMlF3VKSNJOB7YkNPvuYvC51/GOPkl7EQnY2+l1W3JfGxWS1iRcvlt1aOQsLv0DWNH2I5JeI3QEH5S0InCh7UVyxdajR4/uRdJXCJPDU2j97Mi6KVsi3ZSXlkY35QUKjdyDbF/ZWBN9i1gTr5Ahpq7pelVIn45I/80NPEoUsccAY0qSDekxcSSNAG63/V563RbbHfP7aYWkXwOvEd5hzwPLEprSPyau6Zsyhlck3XRvyUmvcE35D/Ju6gxSmEYuDWzrpOVbEqWOFELfZ7ct8A2iwL4hMV59HHCo7dMyxPQBMEdth7cdJRQPnxzibdtesGPBtEDSrsD3CIPVVgvU3IX/N4lpiT80jq8C/CanlI6kOwgjycPo12LsI/eYdYkdfel6WMH2y6VfGwCSvk5o/F8JbEJoby5KdGz83HY2QzOFy/iqqXD9MGHk92tJiwN35rw2evxvUvoiRqF9PML2q0kKpu1iwpmldEqmxLyqC869YvNSSa8DC9p+KRVbh7ouOt512E15QWra+SGhc30m0VSxcPr7drZ/kSGmrul6raOQ71yDMDHfhPDb+dCQvzR54ij63lIhaRpiwm5tWk95ZnumqTu8EV4m8oP7U+PHSrYfTgX3H+X6/CQtR0xrfJBet6XTm8bdem/pNB2/aRXKN4GfAm+n1+0w0PHCdb0YnbswPQl8htjdfUbS/QwuzmXtCE8F6rNzxtAO28dImhm4CZiGGPF6BzguR9E6xTRFq9clYnuB3DFMhFPTn626b03ovOXkRRrXa+JNomickyWAT7pQDWnba+aOoUn9euiCawPCyG132+ekBf/+qbvqVCC3zvXdwIqEkeoY4IeSZic2Qe/NEVDphcMuKOAMOb1WJ1Pe8qKkOW2/ALzEEIsY8jw7Lifyk+p1UV0wXTSpWOK9uehzr/C89JtAJZOTbbO1Hd2UF9g+LxXpjgCmA35G+EvskaNonVgLqAqqxeV9TRSm8CsSReu1CEP4Z4g8JgdF31tqnE4U+C+l33C4CFLH9bGE70qpiFg7Qqwt5wYeBp4mNp9ycScwB/BCem0GT/FCnvOv6HtLKTlVr3BNdz3Im6QO4lWBR3N3HSZeIhYxRTCxMcI6uTteUwwHKtxalyB2eIswJ4M+I4PfNzvpJQ0juhFvzRPZYCTNQOxKZtMja1LgAqvJYcBJkra2/QyApLkJg5LDskYG9xHJRpGF64rUpbEwkfQ8bvvtzCEBIOlrwC9sv9M4PhXwFRdglAIsCFSmUO8AM6TXpxKLrO9miKniQGDG9PogwhfhR8T5mGszuV44vCxTDENRdAGH/JtxE6O+iFmLghbOMFCX3Bmc5SeBl+n/zIr9rgvJ25sUvYCuU1pe6pourwvQ6B2KbsgLbJ8NnC3po0SX8AuZ47ml1esSkXQdYaT6MiGheBGwY+Z7TrfcWzYmzGeLMCptwR+B5Qmz6xK5n5BifQL4M7Bf6ijeAXgsY1wLEIX06nUxVPeTtFm3JOV11BeRU/WkQiYBSVPafnfiPzn5kfQT4M+2T0/JxV3ECT4B2MT2r3LGVxoTGSOs8//16MWkUB9jaRyfFXihhM9P0m7AfsTuLsTu7tG2T88XVXeQujbnJ7r9n0mH5yYmUQaMlHa6g1PSWkTXzUFEEbvpYJxbS3BKIr7dCW1mEUXFHwEH5n5+dMm1+xSwvu37JP2VuG4vlLQqcL3tmTOH2KNHjxZI+i2wqe3XGsdnIhZf2fTpuwFJw4mJkyWIheFYYtLuvqyBdQHd8GwDkDQLg+UGcuctXfHZlY7KNQ2fQOgMX0oyabT9Us6YugVJTxPSiQ/njqUVKtwbQdK6wPS2r5C0IHAtIdH6ErC57TE54wOQNC/wlFsUQiXNa3t8hrCqf/8NYIlCN7az0uu4biBpD+AZ25env58LfE3S48DIAm5i6xI3KoCRRBfYHMB2wKFAEYXrdKOqkvAHbT+RI44u6HLtQ9LUwK7ELnSrJGilHHHVqMa3msxKa4mJjiLpAGB/QhP8tnT4M8BRkmayfVS24ABJAnYhHIwXAJZKUgjfBZ6wfUnO+Ciza7Oi6nq4kYHnYAkjhQBHE6aCOzPw3DuSuI73zhRXRbtrd17gHx2OpR2/Az5HbExcApwiaR1CY7AIIxdJKxAO89fafkPhOP9Os9uvR3tKLOCUThcUmNagYaaamIa4D/Zog6SRwBXE/a/K31cD7pa0qe1rsgUHSFoCeL9a+6R78jbAA8Axtt/PGR8F56WS5iN8G9YEpqy/RRl5S3F5QdLdnqSOOufX4B7SNJz83+/MxET2GsC3gJ9LepQoYo+2fWXG2PootPB/DLCXpF1sT2oDXCe5MP3ZSr42+7ln+4ba6yeAJZKm+autCsWZeBKYk5AN6SPlVU+S9zMsvaM+G73C9WD2IIrA1QjaZsBXgS8SI/Mb5gsNgA/Tf5GtB1zuEHK/mBhnzkrqsBlFfF4f9B/W5cD2tv/Z9pcnTzxPACs6jEi+R3SxvDmx38vE2cT5dRXRcVPEzb2mZWQi8amPFQ4DliI0wHKzMzEGd1Ht2M0pUTsCyFq4Jow+9iWKnPVYniE6dbMWruuj3wVS8kghxDNiO9vX1449rjD1O4dMheua9rGBWyTVC6zDgPmA61v9bgZ2JwpdEAX/94hF1yWEQVM2kp711YRWpIFFiBHIE4iJhD3zRddncnQ47Y2EOq4hXafEAk4pen2TSCsNRoCpiWm7LDTMjYZLqm9ADCMaLZ4hA12gYV7xQ+Bw24fUD0o6LL2XtXBN5PMnAw9L+jiRn44hNuBnIpoFOk6X5KXnAbMQa8pBptK5KDwvOLX2egZgL0JqoDINXwVYiViP5+Ys4ClC/qCY77fC4en0m/RfZdB4IP0NNFmLm4UX/tchNl3XkzSWwVOeuXOComQuJoUCGxTabdzNQOT1OTkbOC51hZfYUZ/NvLRXuB7M3MC49PoLwKW2L0kP+t9li6qf54GlJD1HLAp2TMdnoHFjzcTJwHBigVoljasSi9aTgO07HM+chKnHy8AhKY5SC9cjgY0K1E2rtIwEvAq8VXtvAtFhWoLh5WzAHS2O/xmYvcOxtGJnYAfb10mqF+LuJuR+erShwGuiyczA4y2OP04sXHNRddEvRRi51PXyJxDPumyeBGmiac+0odlXaEgdLkfniqsFJxLP3lmB+vjgpYQcTG5GAZ8kFtLFLaAps4BThF7fUEjaK700sLOk+vU7jFhYP9TxwPqpzI1MTMM0eYuhDc8nJ0V+py1YlDCda/IzYqM7N4sTOQpEI8+fbK8vaU3ius5SuKY78tKVgE/Zvj9zHE2KzQts9xWkkzTm0baPqP+MpP0pI2cu2jRc0mxEt/Wa6c9Fica3y4mu69yUXPh/CSiiI70VJUpIdMtmsaRKtcDAkZLqNaFhxH37no4HNpCiO+rJaF7aK1wP5nXgY8TidB3g2HT8Xfq7wXJyLvAL4ib/PnBzOr4yeRcwFSOBjW3Xi/xjJO1IPAQ6Xbj+C3CupNuIBHfvxuKvD9u5Degql+WisL0tgKRxRMd6dlmQNjxCdL42v8evEm7GuZmPMKxo8i4wbYdjGUTSzD+QkLyYl4GdkXR6HD11891j+4NGZ98gcu8+A38lpnV2axzfk4wJUNVFn67di5smTAWwFXAAYeI3mhZje4WwNqF3+Goo/vTxOHGt5GZtYB3bf8odSBuKK+BUz7Xm68Koir4CvkHkfBVVgWnnDsdUZwEitieI7/jF2nsTCBmTLFISBX+nTV4gRoKbhlXLA3/vfDiDGEZ/V//a9HfiPk7GhoAuyUufJKYiiqKRF/zChZhIt2BToFXudyn5NkzqlG4a/nz671aiqWyM7RLqBBXFFv674fkh6fPEmmNBYF3bT0n6BvCk7ZuH/u3JwtJEETPbFNgksnT6U8TGbD3eCcRG7XGdDqpB6R312cxLe4XrwdxIOBj/BViYfs25JWkYlOXA9mGSHiAWy5fari649yijQ21aWne6vEKewv+2xLjlxsSO0BeIz6qJGVzw7DQHAEdI+rrtVzPHMojCpSQgNN4vSRI/txPf6WrACKJTKDdPEEl4c6d8fUIaJjc/AL5MyDScCOxDmDV+BTg4Qzx3EouCF+jv7Gs1Ml/C7vO+wPVJ//MPREyrAHMBn88ZWGIc8CnCWb4PSSMIY9pbcwRFxPVNSTcS3+0qklre+zLGCPFca5WMf4z8I4UQ10jLDdlCKLKA00TStISGOcDjadQ6G7YXAJA0mjA/LCovqHV9dYWXiKSPEt/vPQVt4p0NnJnG+KvOpdUIealjh/rFDnE/sIuka4nCdVUwnJsCGi0Kz0v3JDr6drXd3JjIju2f5o5hIrxBdAo3P7s1KGNy9gDgGElFmoYT5m4lFaqblF74LxZJWxIT5OcQ9+Wq0WgYsR7JUbieD1g5ydf2ybRmiGNIbK8JIOk8YuLz9cwhDaLEjvoGbxLTEh1H5Wikl0HSaD6cKAz/2Pav0/HvEyZMRwz1+/+/I+kmomt960pLOhlYnQ/MZHudjLF9AMzRNDgqhXTuXUaMdT3P4CQotxFJ0TqqAJKWB75N7KKKKAgfb/svWQMDJFWbKPsCZwI7EZtj+xL6yL/IGF5lirOL7V9L+iewrO3HJe1CdJt+qcPxzAeMt+30ui0lPOQlzU2Yq1Z6fWOB020/mzUwQNLdwGG2f9k4/gXgUNvLZ4prI6Jw81Hab0xAFNezbU6kos29tg9I18ZwYirrEsK4bPNcsaX4vgxsDmxju7gCtqS1gO8CRRZwkjHy0cQ9eSriPHyHGGXer+COxCKQdDjwlO0zGsd3Bua2nWPjsx7HjMS04hdJGvXJGPkM4Hnbh2aMTYRx2neIjU6IicpjgVNyG1mlRoBfEnJYP7VdeQAdCSxq+4s540uxbEv/pNgAk9CceXN6VkxNFJPeodE0kztnLm3KromkfYmGivMIszKIDfhtiLwla7NWWlNWDDINz/35VUhakOhuNvBgMsvLTsoLjgCKLPyXel8BkPRX4EjbF6f7zDLpmbYMcKPtjk/DSHoJ2MD2n9K1MbvtFyf2ez1aU2BHfT22PYiG3o6bl/YK112ApE2Ba2y/m163xfYVHQqrJZKWJrrUpwfuJR6UyxA75+vafiBTXFMCFwD7226lRZsdSb8kzL8uJEZEB1ycde23HEi6kiF0VLugeyM7knYgkrR50qFniAR8VL6ogqTztZjt8UlDf0Pbd0laAPhr7kVWj/87kt4AlrL9ZOP4/MD9tmfIEVctjlmIqZwlaSMVkrNzQ9ISRLf6PcQEx7VErDMDq+Z+piQPjvmJAsnfGLwAnGxGKZNCFxRwzgU+B+zHQBOwI4HfVMW6nEhaFPgSrRfRWeOTNJ4YG/1T4/iKwGW2h9x4nNxIOp3IQ3cjtI+Hp0X+hoQx4jI546tIBXbcYRPziSFpGNF48mrt2PzAm7kbQSTtQ3SBn0k0LZxONASsTkiIZDP2lbTNUO/nzpklHc3AKbuDqE3Z2T4zX3SBpM2JzvXF06EHgZNtZzUzh76JtbY4szdLaoYaRWzYVcUlERrX2+e+z5Rc+C/5vgJ967XFbf+tUbheiMjpOy4/KelM4OtEfWBe4GkGypv1kbvwX5G8GtptTqyVJSgGddTvDCyZvt+diOm7dXPFluK7hvBY+QfRpNUx89KeVEiDtEB93/bD6e/rELu7DwDHOI9e32X0j8xfNsTPZR+Zt32fpEUI7dKq8/DnwAU5x25T0X8douurVNYB1mou/gqiaB3V9NAcTei4PZ87niaSdrB9NiFF9FFgimrRJ+kM2zm1SiE6SOdKfz5GmL/eRRRwOn7tpk6vSSKzjASSbgDGEOffHZmeE0PxFvHdNuWuPk4BenS2X0sJ5KO2W0k5ZcX22LQpuwtReJ2G0Nk8zfZzWYMLhsoLSmD33AFMhM2IxcBNtWNPSKqMrHIXhjdIcfyF0D6+g5C8mJoyTMNnY6C+dcXLlGGMPBLYxPY9kuoFkgeJbqZsSNoCGG37+dyFpHak59mr0Censypxry5henEHYEfbl0naHTg1LfAPJkbXsyDpQ0QDzy9LmLpqw+bAzmnK7jjgqjRl9yCxHsleuE4F6uxF6iapGWoDIgfIPvHXhpOJ6bA1ScbXxLV7BnASnfecarJm5n9/KIq8r9R4ljDbbJ57q9PaKL4T7AxcDSxCmAqeR/jXFImkrxPXwpWE/NBVxGe6AFG3ysm+wA6po/4bteN/JL+sLWQ0L+0VrgczirjZPyzp48SJPIbo1JiJDIYQtqdo9bpEUrHp96lAVz/+IUmrZy4wXUGYfeQW3W/HeKIoUiql66hOT4zXziXpMeK6HUMUsksoLh0j6RXbl9vu04ZMBff1MsZVcSWxOfFH4h54UeoQn5s8WptjGCgfURUcmn+H/BrXdxKLmEOBCZJ+T//59+cCCtk3AEdJGll1zSXpnyPSeyXwJHHtNo8beDv3yGHaDDskZwytSAWSO4A/5exKb0eXFHDeIKZfmjxDhk27FhwGfN/2kam7amti4foz+jvEczKe6L5pjqCvTnRd5ebDtPZemZE2HWEd5BjKzVmQ9BPiGXZ6kpb4MzFtMkHSJrZ/NeT/YPLz8RQTxLVaTW9clI7vkCMo2+9JOha4Lse/P4nMTr+/yr+AWdLrX1OGZxLQJylRSV08YHtM3oj6mqF2JTpxS2UksLHt+ubmGEk7Evl+tsJ1FxT+i7yv1DgLOKVW1JxH0meI58mhOQJKslbXASTJkuNL3YxN7A3sbvuclFftnzYnTiV/rWMRWud2/6L/XMyGM5qX9grXg1mccBSF6ML5k+31UzfYeWR2Mpb00XrRq0BGA3MyeNx75vRezgLTeOCgdHO/k1is9mH7hCxR9fNtorhZpA4ooYV3mKRtXKCOqu2tAFLH/whiB/VoYG5Jj9peLGN4EGPeV0h6rdKnknQWUbReI2dgALb3r72+TNJTRHfGI7avzRDSx2qvVyY2nA5n4Cj/AcTOdFZsHwgDutHWIJLy7xPmfbkTjb0JZ/lxku5Nx4YT9+mvZItqIONoyA/VkfQ68QzetxNd2ZKWm9SftX33xH9q8pAKJFcQE07FFa67pIDzI+AQhTHyW9B3LR+c3svNJ4DKA+FdYDrbb0s6jPhcc+cuZwInpsLmb9OxtQkJghIKYHcQRZyT0t+r+8xO9HciZsH2PClnWYPIW+qF7NEFTGKtC5ySXo8kiv1zEFMIh9JvYJ+L5wmPhPFE9+EqhKTTwgzxPOkQfyQmJEoszEFhU3ZNFL4hVxKfYbXpOZekO4kJitwboTcAaxH6+SUyLa1zgleIqbFsdEHhv+T7CraPkTQzcBPxXY4mGt+Os31a1uDIW9j8N1gQ+E16/Q5QSSaeSmwg55zQL7Gjvgh6hevBDKN/dHpt4Pr0+nHKGHl8No2l/wy42uWZBonWN/VZaRSKM/B1YtxxePqvjsm/+LuUGP19WFJxOqD069+9IKk4HdUajwMfIQqfsxEbKVNnjQiwfbOk7YDLJK0HfIPQVV3Dmc1SUvfDz4EDKr3eJAmTTRam3j0q6QeE+3OrUf5jKKcoNhNxr6vOvfeJhWBWbD+XOiC2BJYl7tM/BS50MtEtgC2I7/IM+s+7lYEdiQLJLMQ96J90pvP5ToY2jKzILtEF/JVYUI3LHEc7iivgSLq6cWgN4Jnaxs7SRI48fSfjasM/6S80PEd81/cT8X04V1AVto9P8len0K8TOYHQoj0mX2R9HADcIGlJ4jPbK71eiVgIZsX2o8CjSWt9JeKetxXxPecuXH+Y/kaU9YDLbb8g6WKimSE3vyUK6ncTE7MnJl3k5cgvMXE2cJykeYk8oNksk23DM1HalF2TU4gcamEnfw6F0eDP03sdNQxvwc3AEZKG0/r7zeo5BdwO/EDS1lWeJ2l6oqEi64ZdouTC/2jKva8A0TCjMEZeApgCGFtiU1nBvExsxEJM1y1FeLPNSmz65KS4jvqUG4+w/arCV6ftBs7krAf1zBkbSPoD0Zl2LXAjsFLSbV4FuMT2PEP+DyZ/fJ8Dvgpskg5dQRSxRzvjl1lbBG5A7GDVJS+GETeEB22XIIlQJCrfyGXIYpHt73cqllYkM401gdUI/aVbSbrDJY2ipYXBqUQBYg3b4/JGFEh6FVg+dxG9FZLeApaz/WDj+BLAXc5gRNKI4zTi3JuPGCO8hTj3/mC7ZPmfYpA0BjiludhTGBLvaXtE0oP9vu1FOxDPJOsY5r6/KNzHjyIK+q0W0K/kiKtC0lcIWZpTKKSAI+m8Sf3Z3N1DCuPm622fJekYwmzrfCIPfMH253LGV5GKIksQmz1FLaKTRv3exAbKFERB4mjb92WOa0Xi2bEmMa1TVO4iaRxRPL+J2Bjb0favJC0F3Gr7IxnDQ9IUhF/Ie+nvXyZNigFn2n53qN+fzLF9MMTbdkbzuVZIWpm8U3bNeF4ncuS7G8dXAG62PXOeyPriKPr7Tdfor4nN13uJQtMyxPN3XdsPZAyP1HH9PeBiCiv8KzTrhpV4X2mFBnoPFLPeLRlJFxLrx+MlHUhMvV9DbOb92XbWjbG0KfFt+psWqo76gzPFcwhwrO03c9aDeoXrBkmj+ZeEtMVPndzaJR0JLGr7iznjq5A0DbARUcRejzDGudB2lrH52iJwG2I3sj5mNoFIeM8uReZE0gxEYpG7Cxzo0wHdkbJ1QIsmJZEvEpISP8mtiQsg6ZQ2b21MdEn2meXZ3qMjQbVB0ihic6k4Dfg0GvoYsG1jlP88ohtnhczxVefeqcTo9F05NxJbkYqbuxHjcevafirt5j9ZSdfkJG1ODE/dh/XjiwL32J5O0vxEQWy6DCEWS2MBXT/vRBkL6KIX+KWTugxnsH2vpOmA4+lfRO9le3zWAHv8n6k9O44HLi7tu5T0PeA7xOjytMQ6aIKk7YHtbX86c3zzAk81n7ep8DRPzs9zYpufuQtM6vckeq9x/EPAp53f9Lpd4Xo5YlMna+G6G0h58laElJgITfMLqjw6JyXnBWmyfTTRhFKCT80ANNh74E6iQXACIaOTW8KpeBQ+P9PYfjZtgO5Df171Q9uvZQ0QSPler6O+Rq9w3QJJw4CZnEys0rH5gTddhov2ACQtDlxILPpzL1APIXaEiigIN5G0G7AfMQoHYRx0tO3sOluS3gCWyJ3MDkXaMNkQWIjYdX5N0kLAqwV09X2WGPdeg+iqqsyORgO3OINxmaTRk/ijtr3WZA1mIqRr99tEolaUBnzqSrsWmJLoHIEY5X8f2MD2HbliA5C0MP3n3ghCK+024twbk3skWNKWhATHOUT33JIOE5KdgE1tr5szPgBJDwHX2N6ncfxY4Au2F0vnwZW2P54lyIjnPmB920/liqGJpBFDvW/7lk7F0orSCzg9/jNayK4MwPbITsVSkQqak0Tm4ubhxDNjBSJnGU2/QWMRmvWSvgjMC1xq++l0bBvgNdtXZY7tfWDO5tpM0qzENEJvU6wNpX92kq4kZNe2qJ636bq+AHjR9qY54+vxv0vtvrwiUQwuynBd0nPE2uduSV8iNj5XJLwHNrG9cs74evxnJJWH7OdZifQK121Q6PUtRHR6FTfqnUYyNyE0S9cmDAQusN0J7c+h4joJOMf2/TnjaIWkAwhzzeOIohLAZ4C9gCNsH5UrNgBJNxMOy7l10VqSinO/IYpysxCdN09IOg6YxfY3hvwfdJDa2NSW6T/ZnjJvVGUj6ckh3rbtBTsWTAvSznOzc+TCEjfJ0mbivkS8UxSwAPwrcKTtixXu2cuka3cZ4Ebb2f0bJG0AXE5o1N9BdA6vSDyHv2j7+jRaurDtvTLG2ff55Yqhx38fSdsSOuvz0q/TDEAB974RKY5bWhx3AZ2RTdmVKYmR9HmAK6rJxQ7H9AGTaKKV+/4MA3KWNYiCyUrAQ7aXyRlX6aTvefbmhF3aLBtrO6tGfepeXonW95XzswSVGOKzWxS405l9dSTNA1xFNCk8S1zPcxPNCxtVmyg5SV2b69H6+z0sQzyTXMwvda1ZEo378hrEtfx2AdfG20Qu/LSkc4B/2P5OarK8z/aMQ/4PeiBpM2BCc/NV0khgKtuX5YkMFD5nxW2Y1MmVM/fMGRtImpEwCvgi8ZBchDABOwN43vahGcOrFvdbEqYBbxGGfmvYLsFoAaLQ8E1JdxHdfRfbfj1zTBU7E/p8F9WO3SzpUUJ/M2vhmvKNXE4idN93AeojNFcTkg3ZkTQ7/QnGmoQr79+Jm36PIbC9QO4YhsJhLnNW7jhakcbMViDOuTWIRHca4jqe1K77yckiwB9aHP8XYSiZHdvXSVoE2BX4BLE5cTVwRtURWcJkTMlImovWSWTWwiYUX8DZh9jUPpMw6zudMMZbndjozs2JQKsiyEyEUc/yHY2mgdtogEs6njCWzMGKtdeL0m/8Wt0HVwF2IibwSqBu7Ds7Ufz/aNaIEiXKTNVk2AwcKaluMjyMuNfc0/HAakhajNAMeEaTAAAgAElEQVRMXYB4nr1PrLvfJfRKs9z3ahMSBn6eiiQVlSdR9jVl6rJeTtI61BoWbP8mb2SBpE8RxuDvENftM4QZ/DuEPGbHC9fApBbbSjCVLq7w34IiDdeB54GlUuf1uoTUKERjWVH62wVzKNG42ORNoiaUrXBNNAeuRmxib0DEOkHS7wmZpNyNltly5l7HdQNJpxOdIrsRXbnDU2fahsDhubsfUnJ2LeGqfH1Tm6wEJH2CGFfZitAKvwIYVcC48tvAUrYfaxxfhNihnKb1b3aGkvW+ACS9AnzK9iONrs35CW3k3AZ5Y4mC1wv0m+ONsf1Qzri6BUntnL0NvE2MMf/CHdJgl/S1Sf3ZAgpfrwNTA3+hf3f8d6V0g0t6DNjF9k2Na3db4Du2l8ocYtcg6XpC2/W53LFUpIL1hUTSaJK2dfV+Ac+OIQs4BXQvPQIcYPuyxvVxMDCv7R0yx/cGkbs82Tg+P3C/7RlyxDUxUufmbbZnyxzHLcCPmh1UacR6T9ufyRNZ35pjDQrNXUqVmarJsI0gNiMm1N6ufHWOc8MzoZNI+jXR5LE9UWhallgT/Rg4yPZNmeLqKk+iUpH0OyLn2xN4nX7jw4uINe8FGcMrnokV/m0Pzxhb0YbrKtx7oBtQ+Oosbntc4/j8FOalkybeD6ScKd5sOXOv43owIwl9oHsk1av6DxLdBrmZo6AO5pbYfhjYT9L+wPpEEftGSeOBUcBZzqOH/AhhZtncxf0q8HDnwxlE0R2viVZyG/MC/+h0IC04hUIWe13KxwjpnA+ASupnKaLQdBewKXCYpM/Y7kQn02mNv09FnH/VBs8UZO5cqrE5BRWqW3AWcErqkgOYR9JniC7EQ7NF1SDJwSxLdLZMUX8v51iraiZWttevHS/CxIqYhnmfMHG5g+hgmp141n07Y1wVJxH3kGVpUcDJGFfFx4nFKUQRpyqkX5SOZy1cEzHNRc3MN/FxBhbsSuMTuQNIrES/N0Kde8ncrQ58hLJzl32BHZLMVF0O7o/k6SgFwPaa0FeE3bPQddGKwAjbb6TGlA85NGn3BX4EZCnMVRMSksZRtifRucADto9vHN+L8APKLU84nCgSWqEXPnUq3uxHbCR3vHAt6QlgRdsvp+LmcWlasUSOJT6jqvC/FrXCf8a4ICaLXyQmsYszXLd9mKQH6PceqPKA94Cj80XWVbxKTKOOaxxflHyTYgBImo3+yfE16N9AOZwypniz5cy9wvVgPgy0MkSZkVgYZqWenEm6DvhGSZ1fDaYkTuaZiZGk8cDWwEGSdrR9YYfjORS4JBUhbic60qpRjM06HMsgXL5B1Y3EWM326e+WNBPwfWLXPDezEefYAJJG2T6FjJ2VzO2EdMT2VaKbColnA38lNqHOJ0xA1p7cwdQ12pJE0qHAt4A/pcMrAycAP5jcsUwCmxOf3wAUXgQ/yqHxWsf2MZJmBm4iJExGEwX/42w3NwiyoDBXvYgYy2ySe6x1NNEJ1DRnnjm9l3vkdgRh1PNQ2nB/0fbtaQT8B8T3npMiCzg1nidkGcYDfyNkJO4hRh9LWKzeABwlaaSTaXgasT4ivZeVmmxD3yHievk8Ib2Xm3GEBNG3Gsd3Jb7vnJxO2hSrHyxoU6xomal2MjWFIGLsHKIINjfRJPM0cW/Jiu3v545hIqxPPB+a/BbYu8OxtKK+afh3orj0IHFtzJUlorjvTkfUMQ4hpiVKLVwXV/ivsSj9spM7AjNIKsZwHcD25S2O/TRHLF3KVcCJkja1/Qj0KQacAPwya2SRk75INB3tDPyxhE7/Gtly5p5USANJY4Bf2j4ptb8Pt/2kpB8D89W7rXKjQk2iJK1AdFl/hXhg/pQwbHwyvb8nMWLQcUMwScsTHWiL02/wdrztv3Q6llZIGk4kZEsQF/9Yorh0X9bA6BtHr3b6FiRG5BYmErbV3TB46TQq3CG9dJJW2lq2H2wcXwK42fackj4J/MZ2q+Li5IztQWA7239oHF8F+IntrJ19Q5x7HyW8EYrYJE4bEUsQ3cxjbf8rc0h9pO6RO4hnQ0fkaCYVlW9i9TqRq4xLXXRb2b5N0gJEx1rWkcckM7VCWpQ+RnhN/FbSQoRMV+74zgGetn2opJ0JTek/AssBlxQgFTIncCuxOVt1Dg8nNlJG5L5earINFR8Qi67fAuc2i7KdRtJ6wJXEAuuP6fDKwPyE3MWvMoVWfN5SusyUpGmIjs21aT2pk1Nu4FbgRNtXSrqQ2JQ9guhGG54zthTfR4gOvnafXe7n2tvA0k25l4LkHW8Azrd9gaQziemNHxHj/DPYXiVDTL8nupZvIwrXxxGF9EHkbuaR9CKwqkN+8mFicuLXCnPzO53ZWLWOCjNcr5Pyv2VLqwWVjsLT7tdELlA1gM5JdAyvl3OKR9IFhPTfzETuN5qQqrm7hM7/nDlzEYvpwjgAuEHSksTns1d6vRJxEvUYAkn3EeOhNwBfB67zYBfUC4mTvOPYvot48BSHwsn2CuB3xGgSREf43WlH8JpswQG2n5W0LOEiuxyR5J4FXGD7rSF/uTMM0HWt8UkghzRNtzED8dB+sHF8jvQexDhfjufG/DTMShNvEqNyWUgLP6X/PiypXqAZRphq/D1HbHXSyO2etv8J3Fk7XkRHeGJ+YGTuIlwddYmJFfAQYV41juh62FnSU4RXxzMZ46q4n9D/fIJYFOyXCnY7ENr5udmRVLSxfYakVwmD1csJ85ms2H5O0jKEMfeyxP3mp8CFJYyBV7INpZKKIZXxa2XydgVh/PpU1uDa5y2z0vqZ12lKl5k6HdiEMKr/PWVMSFQcDlTFt4MIf6LRwEvElFZuRhH58VmEVm5Jnx2EvOP6wMmN4xtQxnPjQGIaG+L7PZ8oXD8C5JoE2Bb4IbAx8X1+gZCPaGIySv0k7iamsR4hinI/lDQ7sUZvJe3UMVS+4Xod5Q6gG0nroVUV5q9VXnU30aiV9V5oe0vo26QbQZyDewIzSrrV9kYZw4OMOXOv47oFkpYmul6XJ76Yu4GjS+h6rSPpfuDzBSTefSiE2c+1/YykGQBK6epLnaPvOzS4STerbYAHgGNaFNg7Hd+9wJW2D2kcPwzYyPmNQft0XhvHs460pi4gEwuENxmYfA8jko0zbO+WIbyuQdL5hMb1vkTnq4kNu2OAW21vI2kLYC/bK3Y4tjHp5Za2n0nH5gZ+RjzHshROUifuUA9RA4fYPrxDIbWkGzrCJd0InGT7+tyxVKhLTKwUBmpT2v6JpOWILpJZCTmYbWxfmjm+dYHpbV8haUGigLMYqYBje0zm+OYFnmouViQJmMf2IAmqHj3+E2qbYhsAvyGu1YpqU+xB2+t1OrYmkg4nJhWrDtdKZurgfFEFaZpjc9u/yR3LpJA2u1/NXRhJsbwOrGP7TxP94QxI2oaQujiBmN6A6A7/FrCb7fPa/W6Pvvx0jmbeVwppOntG26MlfYwo/K9KKvznrLmocMP1OqVO3/f4z0kbKCsS+u9rpv8+sD11xpimJDZlT3MGidte4bqLyH2yTAppobcnoYU8dzr8LJF4nJQzWZP0B+Bkh8nMx+nf5R0O/Mz2/rliS/G9DSxl+7HG8VLG4oocaU3JrQgtzW8x0ChyAuFO3UqjsUeNJCNxAtGxURUy3yM+170d+rTLArgz5oz12BYiNMcWo7+DtNKL3Lh5zXQwrhHEufdb4IsM7OyfAPwtZwdxrSP8RUIeqS51UXWEH2577ha/3lEkbUp0Cp0A3EcYb/bhjJqCkg6hYBOrJulaXgwYn7uo3o7CCjjFPdvS9XCN7XfT67Y4g3FpkgeZpO/O9lqTOZyJkjr5dqNfhu0B4Me2s0zEdNGm2HREoXpqCpSZkvQ0sHbVkFIiaYN4IeAeF6RTmmRgNrL9QO5Y2iFpJ6KbucpRniFyljPyRTWQVIBdCLg25cnTA+/klEhK9YILgP1tP54rjm4lyUsVWahuopCyPTj3s6IbUBi7nm777fS6LbZP6FBYg5C0D1GkXo149t4N3EIhGyiS/kXUq8Z1/N8uYM1QFJI2AybYvqpxfCOio+myPJH1xZHtZJkUJB1DjBAcS7+hyypEB/vZtvfNGNtrwEpJT+vbxFj6mpLWBM6zPX+u2FJ84wkTwV80jn+F6PifL09kfXGUrvM6gugIf3eiP9yjLSnpXogoeD6W+wFZkTbF1qF/1Hssobed/SEmaT6iSJg9ljrd0hEOfbG2w7k2xrqNVKB70fZQn2cWCi7gtHu2zUcU6TqutVnvlivx2pBUN00bRsiYPE+/ee5KhPTUz3NPO0lalZhC+DsD89LZgHVzbmyXvCkmaRjwNtHNNzZ3PK2QtAewJKHDXdQ9T6GhOgr4EvGsXcShD34GMel0aOb4vkxIlmxTykZEO1JHrkrqHk7P2quJjsj693sm8LbtPTPH9yqwfOmduCUW/nv8byLpScJv5eX0uh22vWCn4moi6Y8U3Okv6XJCCrjj5tu9wnUDhUHUXrZvaBz/LNExnNuIJNvJMimksb0dmwV+SV8CznSHTd0aMfyTMPoYJ+la4Bbbx6Yx4YdtT5srthTfwcB3iKJ/pdW3GlH0PzZXcanLRlpnB7YmkqCDbb+UFq3POpmD9ugxOVBITO1EnHvbOXRpNya6rrOYv5beEV4nFQnb0ukpI4V00wjbryq8G9omS85vslVNY+0CTAssmhbQRxPf8emZ4yuygCPplPRyN+A8QmqqYhhRfJ1ge9VOx9ZNSDqR+Lz2rG/eSTqJWGfkLuD8gZji2LkqbqYR3DOIRpBP54wvxVNk8SZ15X6p01NWk4qkawiJs38Qm9nNSZ2ROeICkHQ6oe2/G2GWNzzd9zYkuoZzy//dR3hLDCOMS5ufXdbnWukoDDenJ/ycxtNvXPpZwjtk8czxjSLWZsfljKMdpRf+S0fSrsS9ZQHiOfaEpO8CT9i+JG90Pf6XSefe94CLCd33AYX1yTkFmF3XskAWJMbPmzyW3svNzcARkobT4ZPl36CVqcK9NByrM3A/sEsqWq8NVNIgcxNam7n5IeH+/B3gB+nYs4Qz9CntfqkDvJz+FPAqg0dabwPO7nRQTSQtT1wfTxIdOMcS3+s6wKLAV/NF1+M/peQkTdLniAT8V4QWWbUJthCxqNk4R1y2b0nxLUCBHeF1Ol2YngQup3+T7nLKM66qcwhhwrQVYX5c8WdgP8LALCdHE8/Z5YjnRcW1RMH90AwxASyd/hQhpTOh9t4EYjwzy6Jf0hPAiqkz6HtEV252I8Y2fA1YpcX95XTCaT53AWJZ4Ov1jlzbH0g6gdAwzUar4g1hYnoC0e2c+7P7AXCUpK0KHUV/CbgydxBtGAlsYvseSfVr40HKWE9mnSCeGElO6nBivTYbjTVk7ilPIq610+Z2/fjjZDQNrzEeOEhhpnong+sF2aQQEicSUzqzErFWXEqYXPZog6RvEX5ERwNH1d56BtidkJ/q0aAuC6eBpvVFkZqO3nbyH5D0deAbhMTZdwqYkDk1/blHi/dMbIZOFnqF68G8SiSO4xrHFwVKOLmznSyTyPlEcamZbO9CGKnlZD9CJ3dv4KfuN34YSSzwO46krwG/sP1OWvSdCJyYOtQo4YZqe1sASeOIzu9SF8/HERrmh6Tu+oobyOfw3eO/QBckaT8gJnVOb5x7Y4iNqKzY/pukpZNeZEkd4cXq+Nr+fu31oZ38t/8PbEF8p7c0ZCXuJ3KX3BRZwHEydVXoDe9p+/VcsbRgTmA6YuP4EKI7uNRnr4hNgEcax5du8bM5+Aex4dlsSlkAeK3z4Qyg9OLN3sTn9IxCT7pZ/MralVvlp4XyYfobP+rMCGQ1g4eBz7hCGQV8EjiLaOIpbfN4WgZudlZ8jNh0ys3XiZrG8PRfHRObYzkpvfBfMjsDO9i+TtIPa8fvJhq3erTmLWAG4AXCX2I/yqjtNTmJ1NAh6RPAmcT9cDWiKW+XbJEBtrM1ovYK14O5iigcbmr7Eeg7aU4gip5ZyXmyTCJTA1+VtC7RaQOwMjAXcEFtNBfbrYrvkw3btyadtJlsv1p760zyLQjPI7o0X6zvBJZQsG7BCOBkGp+VpJmAXzq/AdPywPYtjj8HzN7hWHr8dyk9SVsSuL7F8VeAj3Q4lkGU2hFOdHzNQSSRQ3V/Zd2UlbSD7ZZTJZLOsL1zp2NqMBcx6t3kQ5SR55VewKk2Z0vS4P4LcK6k24jC8N4Kj5NB2D6so5EN5lzgHIWRdJX3fYrYbDyv7W91jouBUZL2ZaAM21HARTkDo/ziTdFduRWFSq3cQWzanZT+XhVedyLOw+xImgbYkPjszrT9msIM+1Xbrwz925OdtYF1qq7DArmVyJ8OSH930oXfj5j+zIrtBXLHMBFKL/yXzHxEY0KTd+nP73sM5vfALyXdReRVp0h6q9UP2t6uo5ENZCFC3gxC5vEm27tKWpmYAM1auK43XTaOTwV8xfb5k+vfLmFBUxr7EiYuYyU9l47NSXTk7pMtqu5hMaKYBHFjhegmeZ4Yxa3IsnNu+31iB7p+bFyOWBIvEiZBVxM30dI6CuqMAKZqcXwaQmMwN28RBZImixGFsR7dS+lJ2quEFMK4xvHlgKc7Hs1giuwIr2/EFr4pe4ykV2xfXj8o6Sxg3Uwx1XkAWJ3B59/mhKRYboou4EiagSi+9mlwA7k1uLcl5MM2TjF9AWhVhDOQu3C9L/GM3RM4Ih17jigMH58rqBr7EvnVufSve94Ffgx8N1dQiaKLN6V35RYutXIAcIOkJYnzbq/0eiXifp0VSQsTvjUzALMQXf6vEUWRWYjR9Jy8QMgnlsq+wC2SViSato4nmhhmBoryRUjPOLssk7eiC/+F8wSxvmg2LKxPaP33aM3WxBTRwsTzYlYG+naVQr1ZZ2365bCq6azcnEfUSpu1lRnTe73CdadIna6rSlqH0MUTUYi9uQR9UEVLxi4UqvVajd6WSOos2JP2emk5Rh7PIHb/TNyonm903fRhO0vHoaTlqpfAcIUBZ8UwonDzTMcDG8xVwCGSNkt/t6T5CXmJy9v9Uo+uoPQk7ULgWEmbE9fxh5JG2XGU0XFYdEd4F/Al4ApJr9m+GfqK1usBa+QMLPF94OeS5iHuyZtJWozQ9d8ga2RB0QUc4BgK0+C2/TCwGUCSfxlhu8gN2KQdfQyxwTNTOlaM7IrtCcCekvYnOpkEPFaI7FmvePOfUazUiu3fS/o0USh5nFh73E3owd835C93hpOAG4k1ZV0y52rKyFsOBA6TtE0Bmq6DsD1WYcq9C1H8moY4706z/dyQv9whJO1G3EvmTn9/GjjamQ2bE11T+C+Q44BTJU1HPM9WkbQ18Znm7BQuGtt/JzWhSnoS2MJ2q2nA3NwBHCzpJqIxcMd0fH7ieZebdo2W8xLSbJONXuG6DbZvAm7KHUcL9qRsrdeSOR3YhEgsqnHRrNg+VNKlRJfIFcAO5NdcbHIn8VmZSHKbvAV8s6MRtWZvojj3IqENehshEXI7cFDGuHr855SepB0E/IQorIsoposoaB+eL6w+Su8Ir0bPWmGic+6xXFrctm+WtB1wmaT1iE60zwFr2H4iR0x1bF+TNk0OAD4gNJHvBr5g+zdZg6MrCjhFanBXFD6N0IekBYEliOLrWNtP5o6pTipUl3C+1Sm6eJM2Tdrlym8TxvWjbOcyEC9aaiXd37bJHUcbPg18yvb7jc9uPCE/lZuDiELNC5L+RkxJ9JFbXz3F8DzxvC0OSQcA+xP5c7Uh+xnCbHUm20e1/eUO0A2F/1KxfZ6kDxETTtMRHmLPAHvY/kXW4LqEwqV0vkWsHzcCDrf9eDq+GRmnFCXdR3896BZJ9SnAYcR0dKsmqf9eDAU0EReHpA2IHcoliC9nLLFDOVm/jElB0kOEo+h1aeR7mdRxvSRwq+0SRgiKJHUKb17CQr4Vkg6hQPNDSfMRRbgniA65F2tvTwBeSBIsRSBpLaIgNwVwd6nfd49/D0k7EAuZedKhZ4BDbY/KF9VAkjbkJ4lz7y+2H80cEgCSjiYWLJsTz7MVCAmsnwDnFaCRS3qeTQVMSRRfIT7HarE6JaH7u57tFwf/HyY/6Rw8lZBBWCOzzFSP/xKS3gCWTrlUPa9aFhhje5YMMRVrXNokdVmPIrQYq2tXxKTT9rk9OyRNDewKrEnrabuVcsRVIWkOonizPClvoZDijaRdiYmDK4FKa3hlQsLmaOJ5vBOwn+2OdzhLeh1YwfYjjWt3JeBXnV4TSZrkCabcGtJpTfQZ2w80PrvVgUtsz5E5viELwjlkbGoTqBPF9t0T/6nJh6TxxHV5UeP4lsARtudr/Zs9uonkzTFFqRNZJSFpL+B022+n122xndu8dBBJOeB92+9O9Icnz79f3ZMPITbZ65MwE4jmqMvTlNvkiaFXuB6IpG8QnbkXMHCHcgtgF9vn5ooNIInIL2b7b41EY1HCUGi6nPGVTBqRWjuN4BaHpCmgb+y2WsxsCIy1nV0HFCDt8K5EdLIM0LuenGL8PXpU9JK0fx9JUxJF6q8QBaUP6O8I/3oJG0+SPk8kQ98mxuQgdEuPJ7R+nyHGlx+wvXUH4mnXQbgx8Fegr5vUHTYa7ga6rIAzhjAYPinlVcNtPynpx8B8ttfPENMHwBy2X0iv2+FcMmIVks4jujd3pL8baFVCCu12261MkzuGpPOJXOoq4O80Ooht758jrm5A0lXA1c0NYknbAyNtbyRpZ+CbtjtulCzpWuBe2wdU1y7RMXwJscDfvMPxDNWh3vdjlHHdXgy8YXv72mf3MnGdPJH7ui2R2vfbWtOxnxK+37cJOdHHGscXAe6zPU2GmLqm8N/jf48kD7KC7ZfT63bYdvZpu1KRtA1hzthxH45e4bqBpEeBk22f2jj+TSIxWzRPZH1xPAAcZPvKRuH6W8BWtlfIGV/JSNqDGMHcpSoOl4SkXwG/tn2ywkjjIWB6wjhl+9yFYUmfIDQ/FyCStvcJuaF3Cff2mTKGB4DCcbedhnmvuNTlpI7myuR1bAkyDRWSvkz7c29klqAalNoRDiDpQaKI/qfG8U8RXeGLS1oT+Jntj3cgntGT+KO2vdZkDaYF6fk/SQlcjntzlxVwPg3cAFwMbAWcQ+QKKwGr9xbQQyPpZWBj279rHF8duDL3JKCk14CNbN+SM452JAmsZWn97MjdTf8vYNkWxa+Fgb/anj49V+7L0TgjaQngFuAewkD8WmpSK7UR607FM2JSfzb3+ShpLqB6zi1ITDQtTGzurJ5rsqlO6jDckNCmP9P2a+l8ezXHhmeaQJ0kbDc9WTqKpHuBy5oTdalrclPby2SIqWsK/yUj6cPEJEy7KaLZMoTV479Eavw4nPZryhLqLVnuzT2N68HMSzhlNvkVoROVm9K1XktmHaJ7fj1JYxmsl5a7uLQ88T0CbAq8ThSJtyS0QXN3NJ8M3EUssJ5Pf84M/JgCNKQl7U0YRD0GPMvAoklvh66LkTQrMYo+ktooeuq22s6ZzTUkHUtoko1m8LlXDGkR39GF/L/B/EArmaQ303sQXc4f7kQwLthoOLF77gAmQumfXx8ODe5VCNOeEjW4S2daolOzySuEbmluXgBeyh1EKyR9FriIMBdsYkI3MicvE1MmzfXPxvR/pjMwmQ2Z2vH/2DvzeNun+v8/X7iSqUESMkVclLGMGSJDfH8qRQlFCCkkEZJLhsxDklnIkDIkikwhRcaQKfNFZU7mi9fvj/fa9+zzOfucc7n37rX2Pev5eNzH3Wd99r739dhn7/VZ673e79fbhfnk5g5Gvx1sP5HskDaiz17veOAM269kFcf4w5HLic/Xe4nf6/PE7/q9RK+JrpI7GP02GQOckw4QryPmk08RBzwbDPG6yUnJvsK9xGnEAd2pdKgiqrwzJI3KZcPR4CQiyeh4CtxT5pyba8Z1g5RxfZjtnzfGvwXsmDvjOmkp3uu1RFI566DY3rxbWjqRbGAWtD1W0i+BR2zvIWlu4G7bM2TW9wywiu07Jf0XWMb2vSnD5KfO3ChF0ljCi/7oYZ9c6SkknU80MN2a/j6bPyea9g3pATu5kfQfYDvbv8mpox1JE2xrZTv7oaekqwmPtE0dDY9adkmnAdPaXlXSGsDRthfKKLUyBZHsr75JWIU8kVtPJxSNN5+3/cf0848Izf8gqhSyeiErOt+/QHx3X05jMxDf3Zltr5FZ35eATYn36rmcWpqkKsobgd1L/PwpmtKeQFQk/I3YQC9DNKfdyvYvUtLAJ2x/JZ/SspF0MbBl7u9qL5ESE54ggiHP09+D+xTb82cV2Iaiadk6tsfm1tKOpKUJ+7WFYXzj8EOdqdF1ZdKQKu5WqdVg75xUhf+47XPTzycDXyOSF9ZzRltZRe+GNZoVqKWQc26uGdcDOQT4afJh+gt9J5SbAt/JKayF7ROAE6rX69sjd2B6AngUWFHS74C16DsRfz+dMxG7jejT8RQwJ3Av8BhRXpibmZnM3Wwr2ViL8Kf/a9vYdZK2Jk59czMVUapcErM2fl6ZyFZvZZB+jNB9TTdFDcGWwAXAo5JaGQZzAvcR2X0Q1kn7dkOMpAsJ+60X0uNBKaBaZzySjgF+ZLvUDNOiAji230gVExfn1jIEY4iKjpZH6O7Aj4C1CQ/4r2ZTFuxEVCU+nsrTDSwOvETM3bn5IxHof1LSvxlYbZfTy3JeYpNcXNAawPbJycbpO0TFk4C7iaZ+16fnZK1GlTQtcT/rVFJdyppwZaIyoSjS4fAKdH7vjskiqo8VgOVsvyn1c5Z4FJgjj6RBmZdoIF0Utm8m7K+KptTAf8E8QOP7WnnbbE9yKkgB1w2ItdQXiXXV/+WTxpP0b3xYGtnm5hq4bmD7OElPAt8j7BogFmkb2v5tPmWBpCsJb6rn2zemiq7uF+Tw2uw1JH2C8OS5yPZLKTPoNdtvZJZ2GHA6MVk9Ql9AaWX6gk05uZPYjIQr5mQAACAASURBVD5IZN7sKulNYCvCniM3ZxEb+dyL7cqk5ykiCNLkZTqXqHeb44nNwZjMOsZj+/+1HkvaDXgF2Nz2S2lsBqIcrYS5Bdv/lPQxIpNvIfoCJJc5lYbZvqCLkp6hrzyvhM/YhLIJcQBfZOCaMgM41xNWXaWWgc9DHBIDfIFY6x0k6Y9EJmxWbN+RGn5tAowmvru/pBDLAfrKqo+gvLLq64j5rlQLJ9KB8V+HfWIGUhXO6UTgtUkJVivFIqnl5y/gOQba65Wwlu4UDJ6bTNY0vUTyf3+zlTmavitfJyp1DnIBTbnbmJcCA/8FswNwQKp2ubOw32WvMCfwcHr8/4Bf2z4nHaJcO+irusMewD6Svm671AB2lrm5Bq7bSCWjawLX2D4/t55BWBWYtsP4dIR/c2UQJM0GXAh8kliUfZQIwh4GvErcCLKRDk1uIr74l7mvgeQDwJ75lI1nPyLjEcKq5iLC0/dpoKud2wdhLLC3pBWB2xmYVXVYFlWVScE+wBGSNrX9OICkOYlT8X2GfGV3eC/w1bQx6PTZy90YdHsiY3188D8d2v0YuIL4bmcnBagvpYxg3OadHvcAwzU9qgzkBOCQZMt1M41DsgLKcV8FZkqPVwdaNkD/bRvPgqRRRJB691QNWCJrAKsVWnZ7LPHZm4M4RGzeO3J/9lpr502JBn4/sv10Wmc9YfuhvOr4GbEW/THlHUq08wiN320B7Ef0hdmngMSdTvyRqObYIv3slKS1N+VVyFxLJAeUxElEb6J7JX0Y+C3wJ2A7okJ1t3zSKhPJ/UQCwC0AjaxXamPLCeIFojL1UWKNcHAaH0f+3hw/JA5znpQ04N6R25qVjHNz9bhuIOlVYLTth3NraSeVhwLcRATX2zt2Tk2UY25pe94uS+sZJJ1JBF43IyaqlifPZwiP5oVz6utFUufb51zARCJpqA2UM5cDV94m6dS7/XM1H7GYeDz9PCcR0Hko901c0lVDXHbuSpjkh/cF25c3xj8DnOcyOlT/aJBLJn7P9wOX5MjgTOXU09h+rDH+YWCc7f90W9NgpN/14rYfzK2lE5LuBD5bUkmwpLeGuOzcm0BJFxCb1D8Th9jzOhqrrQUcldvzXdJzwNIFf+buBjayXZqdUy989pYmDjcfIrLWR6d18xiiJ0tWm5pS57t0oLMf0SSyyEqOHvjezkEkx0AcmtxK2BL+B1jZ9lO5tPUCkp4nehHdJ+m7hCXRpyV9mvChnTevwj4k/R7YohQLsdKRdA3RqPxYOhzYtXybK4Mj6XTinnYr8GVgbtvPSvocsK/tj2fUttdQ123v3S0tncg5N9eM64H8nXjzH86so8lNxMRk4qSjySsU4sFdMKsTWYfPNU4nHyCynLMjaVlCZye/udxZmwOw/ezwz+oOtmu36imLYhodDoftT+fWMAznAqdI+j5hiwCwHHAgcF42Vf3ZgJiHZyCafkB4pb1EWMXMRWQfrJJho306cA6RmdvOWsSCd80u6xkU21kzcIfD9sdya+hA6feObxONaL8EbNPmh/xZCqhOIOaQ9QmLmhL5LnCQpG/ZLsHWrJ3SP3uHAEfa3isFiVtcCpRQiXIR4bdZVPDV9jhJ36IMu43BOANYF/hpbiGdSIdzSwAbAUsRe6LjyWhBJOlrE/pc26dNTi0TwNREw2uIfWXL7/0BYLYsigbB9jq5NfQYnyAOJe7MLaSH2Y44XJwb+FJbPGMpwno0G7kD08ORc26uGdcNJH0W+AmwF51LRrME6iTNQ5QAP0h09G4/zXgdeLJ6HA1N6tL6iXT6PD5LQ9IywB9sz5JZ385E2d79ROCmn99c7qzNXiKVsd5k+7XcWiojC0kbARe223LkRtK7CVuVb9DnS/YGUUq6s+3szV8lbUZ45G7WymxOGc0nE1YEFxPB4xdtf67L2p4HlnWjy7ikBYHrbb+/m3qapF4Ds7vRqFnSLMTaIHvZqKTpgSXofChbyuFJ5R2QsoO+C1xNJFk0181ZbbrSeu9dRCDnNWLuG08JFSelktbNS6S1cvu6eV7gHttZS6olvYcIwP6T6MPSLKnOFjyUdC5wse2Th31yBlJTywuIPWQnm5oSbNiKonF4A2HdOYpofA1xbxtH9E3KOq9I+ivRK+kiIuFtmdSPYHngHNtzZdDUS4H/Ykm2otvb/ktuLZWRRcMeruu9OWrgukGjbK/9zREFlO1V3jmSLgJut717WnwsRliGnEM0sMjq0yxpLHCg7aNz6pgSaN9s5dZSmbRImo6Bga/sgdcWJX/2UkPG+Yn72f2FBdcfAj5n+/bG+BJEM7p5JS0H/NZ2V7OFJL0IrNBB22LAX23P0PmV3SGtWz7UIXA9B/CA7azNEJMlzVlAp8PhItZVyY5tR2CRNHQ3cHghHsOzArTKLyV9nMj0/4ftrJlBSU/RNl2Svj7UddundkvLUJR475D0H2Ad2zc3AtdrA8fbzlqtKGlD4FTiYOJlBiZ8ZAsepozrHwFn0zkRKuuBnaTvEB7ITwNPMvC9y+2j2rrH7kzMywbuAg6xnb2ptKR1iYbcOwIt//xlib5JP7Z9USZpAEhamTiYeA9wqu1vpPEDCJufL2bQ1DOB/5JJ8+8Ywgu506FTMdXQpaKCm5emQ8U9iIzmuWk0Qsy9Zs5pM1UD1w0krTLUddtXd0tLJyStP9T13AuhkkmT1NXAbcAqxCn0osRNfcUcJ0ftSPovsGRJm5ZepVTfw8o7I1WcHAV8mr4GoePJfRNvp/TPXokZ4QCSXgZWtf23xviywFW2p5c0H3H42FU7DElXAPfZ3rYxfhywkO1Vu6mn7f/fKT08mGiK0t59fGqiYfNctpfstrZ2JP0DuJHI0HhiuOd3G0kbA6cBVwJ/TcPLAasRFQC/zKUNxnvon277ZEkfILJLnwA+TDRWOzSnvnYkzQhg+8XhnlvpT4n3DknHAx8irJyeJhI+TDR6u9L2dzPKQ9KjwK+AMQXe00r3L38SOMD24Tl1DIak9QgbomsJf3+AT6U/69v+XS5tMN47/xu2/9oYXx74hTP3HkhapgZmtv1c29i8wMvNg+5uU3rgv2RqkuXEkyoSjrR9dqruvI9oXroYsd7K1rxU0oFEcsIBwOH0NWv8CrCn7eNyaQOQdBJwt+2u28PVwHWPMcRCyFBWAKc0JM1NlIhuDSxNnOzeQnQlH2X70YzykHQsEZQp2ROvJyhxA1h550i6lmjMeDSdG5GU4PMKlP/ZKzGrD0DShYSP9TeJ7DSIefo4YKztz6WN7L7dzgRLmd5XEoeeV6Th1Qhvt9VzlWu2ZbrOAzwGtGeIvE706viR7RvIiKSXgMVyHw4PhqSHiezR/RvjuwFbO3MTK0nPACvZvkvSNkQTq08qmggdbHvBnPoAJO1IdJmfMw09QQQgjnBBGw1JFxONzItrAlbivUPSzIRN0+LEofG/CX/c64hM7KzB4nQ/W7LUuaVk0ryyTKnvnaTbgfNt79UY34eozlo8j7LxOl4hLMSalViLExZiWSudSqcXAv+lUnqSZS+ggpuXprX9trYvSeuCJWw/IGlbYs/xpVzakr5s9nC1OWMHJM0ObEv/ktGfl5ApZLtfibykaYAliYyrPbKI6h0eInxAm4ugWYhNf+6g/1hgb4U/8+0MLP3J6hPZY2xNBDgrUwZLAp+0fXduIRPAZ+lrLlgiGv4pWdiSyHq9gb4A7FSEN+NW6ef/EWXDXcX29Sl4vQvRhE5EcP1btv/ebT1tuuaD8Rm567dnVRXGdcBCRFOoEpmVsAxr8mtgzy5r6cS76cum/wxwYXp8C3HYkxVJBxEHTgfTl7G+PGGTMDvxvSmFlYn3s0R+CbyQW0SL5GV5GfA14kCi1YTpFtuX59TWxrnEd6LUuaVkTgE2Bkr1sl6QaIzc5HTKmFNuAI6StLHtxwEkzUlkSF4/5Cu7QLLV24FozNipt0RuK5h5aQS8Ei8T9giVQaiB6UlCyc1LZyNskSDWfu9Njy8BDsyiqD+bAc8R2enNecRE0sJkoQauGySPm98SQcRWltIGwPckfd72H7OJ64DtN4AbJe1OdJ3PegJdOKKRqZmYEXi1y1o6sSUxQa2Q/rQzWSeCKQ3bZ+bWUJmk/J0ILhUfuLb95+GfVWmSylbXlrQQEeQUUYp2X9tzrsqhLdlMvWZ7k/TzmkQwZx1Jd+b0wgOw/emc//8EcCxwSPLc7uTHmNtH+ipgVaIxcjurEhklufknsL6i2duaRIAYYnPzfDZVfWxJZDH/pm3sSkn3EhUTJQSZiqdpRZQb2+OSPZNtX0lUnZTGg8B+yc+3qIQPSSKSoLYD5gM+lvzBfwA8aLvTYVk3mR7YUtJadH7vts+iqo8niaqr5ry8NGUkpmxBeEg/LOnxNDYncC/w+Wyq+jgG+AJxAPsXOu9/c1J04L80Uh+O22y/lR4PSgFrql7gTmBbRf+z1YGWNcichC1WTh4F5kh/3w+sRSTLLA+8klEX0Jc0A923h6tWIQ1S6cplwA7t5Y2SjgTWtL1wNnFDkDbWf7M9Y24tpSHpqPRwOyLDoL2R29TAMsDrtlfstrbKxJHsBSYI2+tNTi2VyYekRQmP66OIxUZzg9V1mx9JdzCBG4ECMlvGI+lTwI22X8utpVfo4IV3LxHQzO6F10LSlxk8syrr3NcDXq/bEV6b59K3YV6OyK4fQ9gjAHn6iKTeJmcRySZX2F4zje9B9OdYp9uaGvqeBZZrP2RK4wsCN9h+Xx5lA5F0J/BZ22Mzathp+GcFuSvtJB2cdHw/p47BUMGNQZN9zi5EhtxPgEVT4HpTYCvbK+fSlvQNdRBs26t1TUwHJO0JfI84qGsFXj9FVF0dbHu/jPKA8YcTawCjicP2u4DLS7BHSvPyhgVVR/RD0vxE4H80MCDwb7t5YDGiUVsT7vTYdK6gzL6m6gVUYPPSNm0HAC/a3k/Sl4j132PE9+Ng29kdFnLZw9XAdYPkWbX4IAvw22xPn0fZeB3NUzYRpZi7AtheqeuiCqdtcbYKUcb6etvllg/oIbb/2WVplYlE0ikT+lzbm09OLZXJh6SPA2cSzVSLaESSPL4mCNt7T04twyHpSsJK4vnG+MzABbk3qEnLUUNdz5n9VbIXXtJ3MNHg6Cpi8dj0gM869ymaqw6K7Ue6paUTwwTW28m2IZQ0G5GB83fbb6WxZYH/2r4nh6Y2bUcQ+4kdGuOHA1MXkLlZFMMEW9vJGngFkHQMYSfxEJHx1fSyrL/bQZB0D/A92xe3+5eng/hrbM+SWWLRpKDwjkTweo40/AQRyD6qhOBwyUh6jPDDvTe3lsEoOfBfGmkd9ahtp8dT07+vCUTSwlu511S9ggpuXtpOsipcgWgSn71p6RD2cDsDJ9iebFV2NXDdQNEE7Ajb5zbGvwjslDsrd4hTtuuBzUu+QeUmBTl3sF2Mh2CTdEDyJcLfa9r2a63TwEplpCHpFqIk/lA6N2e8udPrKkF7pkZj/IPA47ZH5VHWT0sz+2sUsZmZhvBUzRZcT0GHj9t+OJUVXm37YEXD33uduQmTpP8A2zWsGiqVriDp58BXgX/Rl7G+LBFsOoNoig3kC3Smw8+tgY8QzS3/JenzwCO2b82hqRcoPSu3ZFIi1GjbjzQC10UkQvUSkmYCsP2/3FrakfQtCrWCkbQ9keyxbeuwszJlIOlNomdXc00/C/BkzbiuTE5SNcc3m3uOlB1+3OQ8lK0e1wM5Bjhc0kfpXzK6LfCD9oznTB5C8zV+fgt4ynYJHs1FkzvrbDgkrUuUKt9KeLjdCMwPvAu4NqO0SiU3o4muyvcN+8zKeBoVOoulxUaLqQnftMcpAHfwaVY0FzqJ/PNfyV54EFk2t+UWMRSSFiOyMRYhDp7uIiqd7sisaxTwZ+BrJR/8F36oPZpoFAnQyq7/d/rTbq+XJVNG4Ul/IfAH4vvbOmian2gyVIIfbZF0mpdLQtLJg1wy0bvmfuBXtnM0TH6QaGjZzH5ch77GW9lIhxKdvpPt792pufxy2yvF2gPWpVSKdbCCafE48G06N/ztJmsAKxG9Q+5ioMVedvvEkgP/hVN6z66eIFVNbkTndVURh7KSXiD2vw/m1tLg9kHGpuowPsmogeuBnJH+3n+IaxATRtdPtGr5xxTNPsDetg9I2RmbEmVxp9NXilEZgl64CVXeEX8jFrbFBq4lbc7gn71c5d43EfcqA50aC78CfKerit4Gtl+VtB9wKdHgLxe7El54OxMb+VawdT3is5mb44FNCD/m4pC0HnAecQDxhzT8KeAWSevb/l0ubW5rQJdLw3CUfqhdenAT+DFRMXlMWlu1+BNhQ5AVSe8H1qbzvWOfLKJ6h1mJ4NxbxAEjwMeIwM7NhE/9PpJWst3tw71DgKMlTZ/0LJ/8rXcBch82QTS7blVK3JjGPgl8iLjffQr4lqS1bV+RQd+qNL4PiemI33lutiG8yi+WtG/b+C1EpnNungbOzy1iMHog8F8cbZZ6Bg6Q1KlnV9FJDKUgaTNiX3E+Mdf8FliQ2Gv+MpuwgXTyMc/NacSB0w6N8W2JmNVkowauB9LMaC6OlEW3I5G5BLH4ODzXqXhlkrEQ8Kv0eBwwfQrc7ANcTJjeVwahh25ClbfPz4EjJB0K3MHAzJGsc5+k7xNZuMcBKxOVOwukx4dklDYfseh5kFjQPtV27XWipLDpkVcasxJZJNmwfY2kWWl44RG/75cHeVk3eS/wVUlrEBkPze9Hbh/afYH9bPfzhU/3tn2BbIHrxKnAVkCRDeioh9oTy6LA7zuMPwu8v8ta+pG8Ky8GXiPmuseJvjWvEf1XauB6aK4DXiTsX14GSIHiE4C/E9nNpxE2Y6t3U5jtUyRNQyRCTU98Xx8Htrf9qyFf3B1eBX5he8f2wbTOsu2lJR1JzNFdC1z3UKXYPPQdlrQzjr6qjmyUXmVM+YH/Evl4+ltENVOzZ9ct5N1z9BI7A9+2fWJaV+2WMv6PJu4plcF5F7HnWIsO9nDtPYsm9f6jelz3GJI2JhZhV9K3YVkOWA3YzHYN0PUokv5FNNK4S9I/gD1sXyBpSaKRy0yZJRaNpDsJf/rWTWjx9puQ7R9kllh5hwzTPC1bw7QWku4Ddrf9m8Znb09gbttb5dTXC0jaqTlEBHA2Bq60vXH3VfUGpfvQSnqVKAO+vzH+UeAO29PlUTZeR9EN6CS9CCyW5pRngZVt35l8my+2PXdOfaUjaSzwFdvXNebnLwIH2l4go7ZriUz6HYAXgMWJz99ZwEm2zxji5SOetG5ezfbdjfFFgCtsz57W0Jd3oxmipK8R1iSvNcY/AExVWMOvZ4Dl3GhMn2yJ/mp7FkUjyb/Yfk8XdbV6OUHnbMNXgO/YHswmpiukfdoPbZ/fmFd2BDax/Ymc+lpI+gRRoXOR7ZckzQC8ZvuNYV46uXVVD/h3SC/07CqdlK2+SOpd8zRxH7ld0mjgT7Y/lFkiML6HyJ62S7AlBIbdc7QzyfcfNeN6CAr1ldmP+AD3szKRtBtxKl4D173LDURp3l1EBs6hkhYHvkDNqpoQPgJcnh6/Rl+W5tFESXANXPcupVfCfJg+y4hXgJnT47PSeNbAdbLbGGv72Mb4NsCctvfMo6wfTcuSt4gM8VOAA7ovp3foAauGJwmLi/sb40sTzVZzszB9Hs1NW58Ssjv+R5THQ5T1L0Bk+k0DvC+XqB7iTOBgSRsSv89pJK1CZKadklUZLEZkCzs13HpXCt7sSuiugeuhmZE44Ly7Mf4h+taAL9C9/e4phB3SU+0N1EoKOrQhIrP1n43xRegLGI8j7sXdpFcqxYq2gpE0G+Ht/0li3vso8Z4eRmTbN8v8u03RHvAl0wPZ9L3AM0ArIfBxwmLqdmAWCqiYaGF729wamuTcc9TA9dCU6CszK519n34NlBB8qLxzdqJvoT2GmFC/SPj6NrMRKwPpiZtQ5e2TMjI+S3hqfQRYy/ZYSVsSWZK5vf//DXwAeDRpWZ7wmVuAMgJfmwIbdBi/mbA4yX7vsF364UTxpKy++YlspdeGe34XOQE4TtICwF+I78SniFLNg3MKg54I/NdD7Ynjh8AviLlZxPsoIjC8Xz5ZQP9S7/8Q9gN3E6XKc2RR1FucD5wkaRfCp9lEsPMgwlef9HO3+mM8Rdz/L2TwBmqlcCrx3n2U/u/drsT3BWAVOtthTDbaejlN1iZfE0sPWMEcTqxNZyHWpi1+Dfw0i6L+FB34r0zxXAusSdhPngMclez2VgcuyykMQNKyScsHacyFuasAc1ID173HVYR/bzNzaVXg6m6LqUwa0uJnNLFBJXn1FXfKVjhF34Qq75xkkXQscCLx+xyVLk1NLHJzNA5q50qiUd8twEnA4Sm7bynKaDDzQfpnLbV4Bpity1oGRdKXGXyhtl4WUT2ApJmAk4mDzvGZVZKOBf5te0xGeRDVYC8SjfB+nMaeAPYCjhrsRd2m4MB/PdSeCGyPAzaW9CNgSWJuubVpkZCJW4iMyPuIyrB9U6bkJsTBe2VotiEySH9J3572DWI+3Dn9fDfdq3o6FrhAUqsp8r+lzjlQuS3OiPfnP8B3iQx1iEDnwfT55F5KX0PdriJp/aGu2z5vqOvdwPYJwAklWsEQa6nVbT/X+Aw+QDSCzUoPBP4rUzbfpq+S7QDivrEisWfbd7AXdQNJOxOHr/cTa+X2A9CSD0MnO9XjeggK9ZXZjti4nEufIfpyROfsMcSiAyjjpl6ZcJIP6GjbD+fW0otIej8wne0nJE1FNNpakdgQ7mv7+awCK+8YSX8HDrB9dsMLb3Hgj7azBl/T522qlmdgCsC2PnvHpcBJTn33Ec3xTm2Mb0Z4NGbzeG3TcjDRdPgqBi7UamnkECSP5sWJioQ/0+eH/H/E733xrALbSEF2bP8vt5YWnQL/6f0rJfBfmUJJ/rMz2b4qNYA9jb57x+a278gqsEdIvr3zE5mb99t+aZiXTE4tixKHh+cRAfOOa0/b53ZT11BImhmgJM/cIXqbGPIH/tO6D9tvpZ8/BPwfcJftv+TUlvS8AHzC9n2NdfMywB+64fk+oRQa+K9UspD6chxo++jcWkqjBq57jGGalLWTvWFZ5e0h6QaiIePlwz650o90av9N4ALbT+TWU5m0pCYaC3do4jI/cKftagUzBJK+B+xBlABfmYZXJ7IMDrR9UC5tLST9B9jO9m9ya+k1JD0GfMH2jR2+H7fVxr5D00uB/8rbR9JgTdxMeL3eTzTU6+raIQW+RgOP5Ay0TgmUWC0haS/g4FRBWZlI0jp/SSIjfA/b12XW8wfgEttHSpoRuAeYgaiO2cL2aZn1XQTcbnv3tC5YjLAMOQd40/aGmfUVHfivTHmkBLcJwvazk1PLUEj6L7BkYT32iqBahXRA0reIDcx8wMfSBuYHwIO2s5Z92y7a86syUYwhvCv3Irxn+21kck6ipWP7jZSxeXFuLZXJwhPAggz0sl6ZKHvsOpKWIjbJb6XHg2L7lqGuT25sH5o29kcB06bh14EjSwhaJ6YifMErb5/3EbYvTWYCsjSxknQ7sEoqU76DIcobbS/WPWUdWY8I/N+WSvxb3M3AZo1dIQUaJiizxPbMwz9rRDMrsBLRZK7l1/sxIjv3ZqJicR9JK9nu5hxkYs5bhIH2f5UJoHCbpB+3/1BaYK4H5uV+pIq2GyXtDvycOGzMydKEVR3EHPICETfYmLBhyRq4JrRdLemTwLuAQ4lmnO8hqjpyczFwCdAK/N9ECvxLyh74r0yRPM3w66pWb4KcyZ9nAWsDx2TUUCQ1cN1A0o7EZH8g8JO2S48Tfjgl+JVWpkxaQdfz6D+xljCJ9gLXEwvJ3I36KpOe4wnP8i3Tz3NJWonwABuTSdNNhC/kk+mx6dzQt4jvru3dJO1LBElEbJ5fzCyrneMJX9cxmXX0IjcSwdcj0s+t+8fWRDPEHJwLvNb2uOTyvuIC/8R6szJpuI7wWN+ilf2aGoKdAPwdWIcIMh1KVKJ0BduWdC8RWK+B63fGgUQTy6WIaokWFxGNN8dk0NSi9MBcs7ppFLAEEdT8WfflTDDPE9n1uZmJPhuYNYHzbY+TdCVlvH8vEsH9rYl78XREY8af0dcnJielB/4rUx6lN+JuMRbYW9KKRK+LfnaTtg/LoqoAqlVIA0n3AN+zfXGj5HZR4JoSPKEkLUl8+To1sNql44sqxSPp68Rk1dwoTwXM3fSnrfRH0leIJh9H0TljPWvWa2XikLQf0USo1UzjNeAQ23tm0jMP8GgKPswz1HNtF3GYUlo5taT2xnxTERuWu+i8UBuxXbSHQ9IKRBOts4ng/4lEZtUywMp17hsaSX8ibKaOaJVU234o9TmZx/Y6eRVWJgZJ/wJWs313Y3wR4Arbs6d19eXdXuNL+ixh4/Rt4O+um7K3Rck2SZKeJJrj3SHpa8APiEDixsBOpWU0t5D0fWLey3p41qGSTcDshOUZtlfquqh2MXHotBfwO+BhYAPbf5K0BHCZ7Vkz63sTmL3pGy1pFuDJ3Haikl4BFrQ9VtIvCcukPSTNDdxte4ac+iqVXEh6aIjLtp2lErAEasb1QOahr5SwnXFAdh9VSbsQmeCPEN2ga6fRKYeTGXyRcTlQA9dDc2b6u9NJZBFZr5V3TlrQ7kdkDE9F5ozhVjBa0ijCWupnpQSomxRcTv3xxs+tMv3RjfF6bxsC239JweudCeuc1YFbgOVLaO6WMtDWd6NBbmoIdoHt1fIoG8/uwKUpQWEaYKf0eBnCjigrklYBsH11h3HbviaLsN5hRiLgdXdj/EPpGkS2X4490TnEYezNwBuS+h0oVhuYYSmxWqJdQ8kZuYNxHpEdnrvqY7BKtuuBb3RfzgAOA04nMpsfAVrz8MpA9vsufdW6TWYkvP1z8yiwoqTfAWsBG6Tx9wPVF74yWZG0AfC67d82xj8HjMrZb8f2fLn+79KpgeuBPEiUxGqELQAAIABJREFUnDUDEOsQmWC5+S6wre3jcgupTHJKX2SUTp3op3BSmfdNuXW0kzai36JsL7Iiy6lt90rZXvGkAPXXc+sYhFXp81ZvZzrCezgrpQf+gcOBfTqMz0x8d5fuqpre43zgpJT4cSOxzlqGsJo6Lz1nGeC+DNpyBwd7nRJtklr0amBuZcrQ11zTvwU8ZbuI/ZDt4yTdDMxFZFi/lS49AGSpBIR+lWwGDkjNzVtMTcx1JfQTKT3wX5myGQPs1GH8JeJ+UkSj+GQz5drAOaiB64EcAhyd/O8ELC9pU8KHqYQT3qmAK3KLqEw6emiRUTSlZrtWRgSXAqsRWc0lUlzzucqkJ3VM72QhluXQvVHqvZik9gbDUxPBnMe7q6ozhQf+FyK8mJvcka5VhmYbIkjyS/r2PW8Q8/XO6ee7ga26LaxawE00g1VLLEv+Q7GiA3OSLmwOEZUJSwJ7d19Rf2w/ImkaYg80N+nwU1LrenYPZNs30UimsJ27SXyrkk3AwkQj7havE4eyh3RbVJNSA/+VEcNHgHs7jN9PAfsiSdsRtkhzpp8fAw60XXKS1GSnBq4b2D4l3Sj3B6YnFh2PA9vb/lVWccHPgc0JT7zKlEFPLDJKR9L6Q123fd5Q1yuVieAKYH9Ji9HZXz33Z6/kcurKRJL8eU+h/73EbX/nsklqlXob+GOH668A3+mqog6U7gVKvE9zAE3fww/Tf71Q6UCq1NlG0vcIj38B97dnMNnOkhyQDpsGxfazQ10f6aRqieWB79NXLXEzsFzuaokUmLuJCLqWGJhrrgneAv4B7G6703zdVSSNJvyj5yO+s28ScYtxRI+TrgeuJe0EHGP71fR4UHI1UGtVskk6BdjB9gs5dEwIhQb+KyOD5wjbxIcb4wsC/+u6mjYk7Q7sRsR+WlWyKwE/kTSz7Z9kE5eZ2pxxCFIjq6mam5mcKI6af0+cit/BwAZWJWSFV94BvbDIKBlJbw1yyQAFBB8qUyhDfPYgSrxyN8H5E7X53BSLpFuBJwjrg2bvC2x3yirphq55iIDDg0TW3FNtl18ngsLZD07S9/dDHQLXcwAP2M7a30TSGUTwaz3bz6Wx9wMXAI/b3iinvso7J332Bt2I5b53lE5qsPlma46TtCbwNSIAe1AJ80vlnSHpEsIjfAvg38ASwHuIBK4f2r4sg6aHgE/YfqY2UHv79ELgvzIySPuflYj+K/elsYWAc4HrbG+dUdujwK62z2qMbwzsb3uePMryUwPXPYak/YnSgVvovEH9fzl0VSqlkSonlgQOBvawfV1mSZVKFpJ/76XA2cAmwInA+OZztm/JKK8ykUh6EVjC9v25tfQSbRvng4nS+PZmr1MTm5q5bC/ZbW3tSJqdsBn4IHB7Gl4MeBJYxfYTubT1CpI+DWxEm+VAi5zNQVuNN9sYRaxbtiWCc2cOfFWlhaS/AkfaPlvSh4nS76uJ78fptnfLrG9ZIgu8k4XT9llE9QiSniHmtzsl/RdYxva96TvzU9uLZZZYeZvUwH+lFFLT+ksIW6l/peHZgb8Ba+dMIpT0KvCx5ppe0keBO2xPl0dZfqpVSIM0kXaK5ptokHc/cJLtpjdYt/gW8NVCbEsqlWKx/QZwYyq5+TmweGZJlUoWeqD5XGXi+DNhM1Vs4LqTV2mLjF6lLZsSAVvS3zbndaKEdJsuaxqA7X9JWhzYmMg6FHAqcGaywagMgaTNgGOJJo2rAr8lyoHnI3yvs2H76g7Dl0t6kPhM1sD10CxM3Msgmh/+zfY66aDiFKLcOguSdiaqYO4nKmLa95ZFZI1J2pzBD3RyBw5FX5PIpwiv13uBx4AFcolqIWlx2516D1QGwfZ8nR5XKt3G9v+I5rlr0LeuugW4wvmzeu8DvsrAptxfpbMv94ihBq4HcgrRZfSG9AfiNGYZYuG7EHCepE1sn51B3yvArRn+30qlV3me8LWsVCYbqXR/bTpvAJuLj65TePO5ysSxBXCipI8AdzLQQuyajq/qEiV6lULfxlnSVUS56HM5dEwIKUB9Qm4dPcrOwLdtn5isknaz/aCko+mfZV8StxFN/CpDMzV9Pu+rE1aKEAe0s2VR1McORH+kozPr6Iik7xOB/eOIz9oxREB4Zcroq3MnkXDyIJEFuWvqR7AVZRzS3irpH0QvrLNsj80tqJeogf9KCdi+TNLdwL8KspYaA5wjaWXgOuKg81PAKsQB7YilWoU0kPQL4J6m8bmkXYBFbG+WMjg3yFE+mnTMC2xXwIlQpVIMkpZqDhFlP7sC2M7dYb4yhSJpOeBiIgg3K9HQd/b088M5Slolvb/V2Gu4BmDAS7Zf64KsymRA0qrAWXQO1JTgsV6cV2mvIukFwhbmwdxaegVJLxPr94clPQ2sZvv2dKDyJ9sfyiyxH5JmBA4A1rA9OreekklWIdcAFxENYJexfUdq2HiO7bkyavsvsGSp31VJ9xGNGH+TDnQWTwc6ewJz294qs761gBlsn5cOZS8CRgNPAxva/lNmfQsSVTAbAR8BriWC2L+pvYqGJ/n718B/JTslrqskLQ18l6gqEnAXcKjtEZ28WgPXDdKHd6kOvjILALfYnjmZt99se8YM+n5HnIY/T3yIm5lV63VbU6VSAm1NjtS4dD2wea4GZZUpH0nXEpUwOwAvEFlCLxHBxJNsn5FB05vA7LafHK4BWOIBYGvbV01+dZVJiaR7gRuJYFen3hfP5NDVohe8SiV9mcG9aItZV7UHmHJr6RUkjQXWSQHNvwMH2j5T0orA722/J6O2/9H/+ypgeuL+sbHt32UR1iOkjLQLiIOwU50a1Es6AFjQ9hczajsWuN32Mbk0DEU60Blt+1FJTwJr2r4t7Xf/Znu4A++ukw7hnystcSt5mW8MbAjMDFxke8O8qsqmBv4rpVDXVb1DtQoZyMtEQ55mGdJK9HltTU1YduTgaeC8TP93pVIyTb+0t4CnbL+aQ0xlRLEYsIVtp4Dxu1Lm0q6ER2nXA9fAasCz6fGnh3nuu4DPExmwNcOv9/gwEZh7ILeQQSjdq/RgYEfgKgZ60VZ6n2uBNYE7gHOAo5Kv5epA7mz/bzd+fov4jtxQsnVNKdi+RtKswMyN9+s4+uacXIwF9k4HJLczMNHosCyq+vg38AHgUeARYHnComYBCp0DW1VkpWH7BuAGSWcQtqLZDkx6Bdv3AXsBe7UF/vcHjpZUA/+VEcXbqZItdR7sBjVwPZAjgWMkfYLIYDLhb70Z8OP0nLWJm3vXsb15jv+3Uikd24/k1lAZsbze9vg/wDzA3YR/6hw5BLU3/RqkAVg/JN1G3OsqvcdlwNJE1nyJlO5V+jVgI9u/yS1kAvglUdVRmXC+DUyXHh8AvAGsSASx980lCsD2qTn//ymB5Ev6XGPs4Txq+rElsQZYIf1px0DuwPWVwHpEQ7KTgMMlbQgsRXw3KhNAsjH5KhF4XYA4KNsyq6geowb+K5nZn75En1w8JWl2208SSaqdDg+VxrPa/+WkWoV0QNJXgO3pyzy7BzjS9q/S9XcTvpHZMjnTjXIR4gN8dy1vqFRA0meB7Yiys7Vsj5W0JfCQ7SvyqqtMqUi6FDjN9hmSjiOCiD8FNgFmtL18VoGApOmIzdUiaeguwlcwV/VQZRIhaRtgD+BUIqu0mdmXtUprGK/SL+e2p5H0FLB80yKuUukGkmYDNiWaSO9p++mUpfuE7YfyqqtMqUiaCpjK9hvp5y8TBzr3AcfZHjfU60c6krYjgtXLEoezZwBn2H48q7AeY5DA/+m2T8kqrDJiSPfgp2y/lVHDKsB1tt9IfWsGDdBOSDLSlEoNXPcYkmYmTsa/SJQUQpzAnEuUqv8vl7ZKJSeSNiZO6k8EtgEWTXYNWwPr214rq8DKFEuq0JnJ9lWpbPk0+jaAm9u+I7O+pYhg4buJwCbAx4jmkevaviWXtsrEkzzMByN7c8ZOlORVKmk/YJztMbm1DIakbxGHsvMBH0v3th8AD9qu2ZFDUHLZbWrAdAXwELAo4Tn8oKQxhEfzV3Npq1Qqg5O8888igqxZ13i9SA38V3IiaRSwH7AtsTdaMN17DwQeKbU3wUinBq57DEmnECVn3wT+koZXJAJ219neIpe2SiUnqenSAbbPbnRIXxz4o+3ZMkusVLIg6SbCpmFz2y+lsRmAk4H5bX8ip77KlI2kk4Edmgfr6TP401ZDtVxI+hmR8XUXnb1ot8+hq4WkHYFdgAOBn9B3KLspsJXtlXPqK53hmtPmPNiRdBVwje29GuuW5YGzbc+TS1tl4kkN6L4EzA1M234t97zXjqQ7iD4JY3Nr6QVS0OsnwFHVpvCdUQP/lZxI2pdIAv0B0Yvo4+ne+0VgV9vZrBOTlV7LNqR9fBbgyRKTUbpFDVw3kDQtUXK7EbHQGNV+PfeHRdIzwOdtX9sYXxk43/YseZRVKnlJHdIXtv1IYwM4P3Cn7XdnlliZQkklt7TKzCR9CPg/4C7bfxnqtd1A0ivA0rbvaowvCtxUvxuVyckQi/APAP+2nbXfSgoeDort4ZqbTlYk3QN8z/bFjXvbokTQs677hiCV4LYzCliSyLT6oe0zu68qkPQCsET6fbb/bucF7rE93ZD/QKVYJK1LVMPeStiH3UjYwbwLuNb2ehnl9aP9s5dbS6+Q3rOPF+Kn3lPUwH8lN5IeAL5h++rGvXchojnyezNqewv4UIc18xzAAyN5z1abMw7kx8CXiQYuhwPfB+YFvgLsmU/WeN4NPNNh/Fn6ms9UKiORJ4AFie7o7axMuU3LKlMGFwOXAEdKmhG4CZgBmFHSFrZPy6ou+jTMQWSUtjM7YWdS6WEkiQjCFWUlkSwalP68T9IbbZenBtYlmplmJXdgegKYhyilbjKOWBNWhmAQP8jLJT1INFHLFrgGXgHe12F8NPBkh/FK77APsLftA1JgZFNinXo68NesyiqTgj8CqxGVa5W3ge1xkr5J9IKpVHIwBwPjBRCx0SzxUUk7pYcGtpH0YtvlqYGViP3ciKUGrgeyIbCN7UskHQL81vYDku4G1gCOyyuP64AfS9rU9sswvtx2b/qsQyqVkcjxwFGpGSPAXJJWAg4CxmRTVRkJLE2U8gOsD7xABBA3BnYmPK+7SsPX9YfEd2Mf4Po0tlwa/0G3tVUmOTvQ30qixePAt4FcHsitzuhm4KEJaXyvripKSLoQ2MT2C+nxYNj257qlaxAeBJZi4CZrHTq/r5UJ4zbiYDsnvwX2krRB+tkp2/pAIlu30rssBPwqPR4HTG/71XQfvhg4LJuygVxLHKJUJpwrgP0lLQbcDLzUfjF3U+QeoAb+Kzn5B3H/f7gxviHxfc7Bd9LfIg7V32y79jqhdZsuayqKGrgeyGz0bQReBFqlApcQC8nc7ERoeVzS7cTGb3HgZWDNnMIqlZzYPkjSe4DLiOqDq4jmc4fY/llWcZUpnZmA59PjNQnbpnGSrgRyffZaQcMWIjIL3fYzROBkxPqlTSFsQ3gdX5x8+1rcQjR8y8Wnic/ZlYSXYHsTvNeJBjhP5BBGVK657XHJHAIcLWl64v1cPvlb7wIU45PbS6TKmB2B3J6+OwO/B54Cpgf+TOxDriMOFiu9y//oq4T9F7AAUTkxDZ2z7LNhe53cGnqQo9PfnXogmLquGo4a+K/kZG/gl5LmIr6rG0gaTfQ7WTeHINvzwXj7uvVtP5dDR8lUj+sGyUtwM9vXS7oW+IPt/SV9FTi8hAZvkt4NbEKUEooItJ9hu56WV0Y8aXO/CDAV4TH84jAvqVQmCkn3EpmjvyNOxDew/SdJSwCX2Z41g6amr+ugDFJKX+kRkof56A7+/gsCt9mePrO+eYCxLQ/4yttH0lZEIHOuNPQ4MMb2SflU9QbpO9E8xJueCJRsbPt3WYS1IWk1Iqt+KuAW25dnllSZSCRdAPze9vGSDiIO704DvkA02MqebCRpGmAZOjePzG1xVpmCST6+g+HcPcUqUz6S1gJ2J6pmpyKSPfax/ceswjogaQHgMduv5taSkxq4biDpAOBF2/tJ+hLR8fYxYE7gYNt7ZNa3H7EBPLYxvg0wp+0SfLgrlUplxCBpayL75kWinH8p229J2p5oprtaVoGApNkID+RF6LNuOMZ2do/hysQh6R9Ek7nzG4HrHQk7jE9kltg6UFwC+CCxQRhPzayacFJDy6maTXsqgyNpM/oHrt8iMpxvqBlNlcmFpI8AM9q+Pc1/hwIrEn0ldrL9aGZ9o4nD9vmIw5w3iWzwccBrtmfOKK9SqVRGJJL2B+61fWrqYXMZYavzX2Bt2zdkFZiRGrgeBknLASsA99m+qAA9jxLZfDc0xpcBfm17njzKKpW8SJqO8Hpdnc7BkcVy6KqMDCQtTWQtXdbK8pe0LvC87esya1sR+APR7KvVFGp54nuylu3aKKqHkbQ5sC9hHXEcsDVRlr4L0TX9V0O8fLIj6TNEEsAsHS7XzKrKiEbSsgy+bulkQ1ApnJTJvCZxOFKkFZGkSwiLsy2AfxMHi+8Bfk4chF6WUV7xlNoUuVKp9DaSHgG+nNwf1gFOJexLNgYW64GG4pONGrjuMSS9Cixi+8HG+EcIW4TpOr+yUpmykXQyUYL5a6Jze7/JzfbeOXRVpmwkjSJ8Sb9m+97cejoh6a/AHUTj4bfS2FTAscRma4Wc+ioTT8lWEikj/EZg94ye1j2LpIdo3M8SBl4F7gdOsj1Uk8kRi6S5B7lk4FXbT3VTTzuSdiYaSN/PwHWLS6jWqbwz0n5ttO2Hc2vphKRngFVs3ynpv8Aytu9NNmM/rckeQ5MqmtqbIi+aAtebEj0ncjd+LZoa+K90m2RPM0GBz5wJFenesYDtxyQdTcRrt0t2ITfZfu8w/8QUS23O2EDS+kNdL6Ck9VFgJaLLfDsrE5YmlcpI5fNENUL1hqx0jdSEcT4mcDGUiSWI3g3jPQWTlclhwK35ZFUmFbZPAE4o1EpiXmC9GrR+x5xCNOa+If0BWJbwpj0WWAg4T9Imts/OI7FoHmaI+VnSC8R7vIvtN7olKrEDsL3to4d9ZqXX+DtR+fJwZh2DIeDl9PgpwhLzXmIvuUAuUT1EqU2Re4Ud6B/4b/E48G2gBq4rk5oN6VsLzAbsA5xP/0rUzxM9i3LyDDAPMRevCeyWxqch5u0RSw1cD+Q3g4y3Pui5S1qPAw6XNC1wZRpbHTiAmPwrlZHKy8DY3CIqI5JTga2A7+cWMgj/JTJamhnh8xGlwpUeRtKiwNS2b7f9dNv4YsAbtu/Kpw6A64jg6gOZdfQqHwF+Yrt9c4+kXYgKvPUl7Q7sCtTA9UA2IrKaj6V/4P+bwBjgvUS1wv/o/oZ1ZuD3Xf4/K91hDHCopL2Am4lmoOOx/WwOUW3cCSxOJEL9DdhV0pvEWub+nMJ6hHmI97DJOODdXdbSi9TAf6Wr2B4f45N0IbBbSvpocbKkvxHB62O6ra+Nc4EzJd0HvB+4JI0vwQifm2vguoHtfv5yyadsSeBgIGtjRgDbh6aMqqPo6wD9OnCk7YPyKatUsnMQsJOkbdszSyuVLjADsLGkNei8Qc3tU3o2cFIKdP2FOIj9FJHlclZOYZVJwvHAz4DbG+OLEJlLn+q6ov4cCxwiaQ7CsmZc+0Xbt2RR1TusDyzVYfw8IuC6GbHR2b2LmnqJbYHvNiomr5R0L7CD7VUkPQnsTfcD12cBa5N3k1yZPFyc/j6P/hn/Sj/nToTaj1i7QMwjFwFXAU8DX84lqod4kJiXH2mMr0M0v64MTQ38V3KyGlHJ1uQq4Igua2myEzGvzE1UgrX2lLMTPQhGLDVwPQypbPDGlM3yc+J0Oiu2d0unk4sQC6C7Ws3AKpURzBqEjc7aku5iYHBkvSyqKiOBhYksEYjsyHZKsBDZhbhXnEzffX8ccU/7QS5RlUnGYkTGXJMbgY93WUsnWlkux3e4VkIAp3ReJu5tzUyblegr9Z8aeKWbonqIZYkDkyZ3Ap9Mj/8KfLhrivoYC+ydGujezsB1y2EZNFUmDZsTv983G+NTEQGJrNi+tO3xg8Aikt4PPOfaAGtCOAQ4WtL0xPpq+eRvvQvwjazKeoMa+K/k5GngS/S3qSGNZet7AeNjj4d2GD88g5yiqIHrCed5YP7cIlqk05cbc+uoVAriacKrqlLpKqV3eLb9OrCDpN2I+5iA+22/PPQrKz3Cm8B7Ooy/jzL88ObLLaDHORI4RtIniHWfCX/rzYAfp+esDdyWRV35PELYgjStnLYi+sYAzArksG7YEngRWCH9acdADVz3LicDszf7DUiaBbicsBjLRmpovoPt/7XGbD8raQZJP7Vdg69DYPuUVJW9PzA9cDrhz7y97V9lFdcb1MB/JSc/Ak6R9Gn6PK6XAz4DbJFNVULSx4GtiT3bN2z/S9LngUdsj9jeRKqHqv2R1CzHFJGavyuA7ZW6LqpSqVQqxZNsnOYHbrP9Wm49lZGBpN8SwesNbL+ZxqYBfg2Msv1/OfVVJh5JXwG2B0anoXsIi7hfpevvBmz71UwSi0XSuoSVygP0Bf4/SczVX7T9e0nfAhaw3al0uFJ520h6C5jN9lON8XmIStkZOr+yOyQ/606B9Q8A/7Zdk9smkEKbIhePpK0Im5q50tDjwBjbJ+VTVRkpSFqWWFctTHIwAI6yfcOQL5z8utYELgT+QFQgLGz7QUnfA1ay/fmc+nJSA9cN0kLDDMxSuh7Y3HazuVWlUikMST8AjrVdG89VJjuSZiKyq75I3D8+mhYZxxIbwDE59VWmbCSNBq4lMjf/nIY/BcwIrGz77lzaWkj6LLAdYaWzlu2xkrYEHrJ9RV51lSkdSXMB3yKahAq4m1gjPDrkC7tIsgu5qR569jaSjkoPtwNOoc/OB8LSZxngddsrdlsbQLIDEVEOvzD9y+KnBtYF9rM9ZwZ5PYek+Yn3EeJA4sGcenqRGvivVPqQdANwqu1jJP0PWDztKZcGfmd7jswSs1FPUwfSLGl9C3iqZrFUKj3F7sA5hMVPpTK5ORCYg/Dr+3Pb+EVEA6QxGTRVRgi275G0GNGIcQkiKHEGcIztJ7KKAyRtTDRoPBFYHRiVLk1NlAXXwHVlsmJ7LLBbbh3D8Afi+1sDX71Nq6+AiIDm623XXif6YRzSbVFtPE0csJvOXsKm+01Ke45k+XISsB4RK0jDuogo7X8mm7geoj3wL6kG/itdQ9K7gI2JnnEG/gGcVcDh8aLA7zuMPwu8v8taiqIGrhvYbjYJqFQqvUcJvq6VkcN6wBds3yapvYzpbgY2a6xUJhmSRhGHIz+zvUduPYOwC7CV7bNTlnWL64F9MmnqGSRNC+wBbEQ0dRvVft12bW45DMlHdQngg0RzvPHYPi+LqIHUdcsUQKvnhaRTCA/pFzJLavJp4rN2JVEl1u7t/jrhoZr9wLMHOBFYgGiS27IWWJZoen0CsH4mXT1BDfxXciJpEeASYGb6mjdvRTRLXjtzpeJzwJzAw43xpYDHuq6mIGrguoGkrw1yycCrREOrEWuKXqlUKpUBvA/otMieifAerlQmC7bHJX/eY3JrGYKP0tf8pp0XiU1DZWh+DHwZOAA4nGgyOC/wFWDPfLJ6A0mfAc4CZulw2UTmf6UySbG9eW4NnbB9NYCk+YCxtt8a5iWVzqwFrG67/d52naStieablaGpgf9KTo4EbgU2bR0uSpoZ+CVwBPH9zsWZwMGSNiTWKNNIWoWo1Dklo67s1MD1QH4GTEtktLRu5lMB49LjUZJuBdZuNtyoVCrFsAhQM0Yq3eJGImvkiPRzK+t6a+AvWRRVRhKXAqsRPusl8gSwINCsaFuZaJhXGZoNgW1sXyLpEOC3th+QdDewBnBcXnnFcyRwMbB74ZmkWwP/yS2iMjJoVRhLmoOo5Ji2cf2aHLp6iKeAlzqMv0znRIZKf2rgv5KTFYFPtlfE2H5B0h5ENWBOfgj8glgzt5pGigho75dPVn5q4HogGxLeXt8lghEQ3ccPBfYlOt6eAhwGbJpDYKVSGZrkZ1mpdIvdgUslLUrcV3dKj5chgnOVyuTkCmD/5HN9M43NdAFWCMcDR7XZhMwlaSXgIKr/+4QwG31etC8C702PLyH89StDMy+wXuFBa2yfmVtDZeSQAtZnEmsUE4GRdquzWokwNPsAR0ja1PbjAJLmJOIF1QJreGrgv5KTV+lbS7XznnQtG7bHARtL+hGwJJFAe6vtf+bUVQKyPfyzRhApg2Uz2zc0xpcDTrG9sKRPA6fb/nAWkZVKBYDUbXeCJjHbtSS9MtmQ9HFgZ2BpYpFxC3Cg7TuGfGGlMpFIGqrU2yV4IEvaj0gImC4NvQYcYrtaXQyDpHuIden1kq4F/mB7f0lfBQ63PVtmiUUj6Y/AEbY7NTvKiqTpgB2IpqWd/LcXy6GrMuUj6RzCPmc7IlFrbeKQbB/gu7YvyyiveCTdQRyKTUcktUH40r4KPNT+3Po9HoikLYjGeM3A/6nA2bZPzKmvMmUj6VQiMXUr+jKslycq2P5WqtXTSKdmXA9kXuK0r8nL6RrEDel9XdJTqVQG59u5BVQqAClA/fXcOiojD9tTDf+svNjeIwWvFyGCc3fZfjGzrF7hfCKweT1he3GWpK2IIMnBOYX1CMcCh6QM0zvos/4DwPYtWVQFxwBfAH5N2ErVbKJKt1gFWNf2Pamp9FO2r5P0GuGrXwPXQ/Ob3AJ6nB2JuMrDkpqB/w9K2r71xBr4r0wGdiAOSa6lrxfR1MBviSSLbEg6aqjrtrcf6vqUTM24biDpaqKr8qa2/53GPgScBkxre1VJawBH214oo9RKpVKpFEDqe3A6cGbrvlGpVDoj6d2Ev+A/Wz6rlQknVQCuANxn+6Lcekqn5IoESc8CG9qunq6VriLpBWAx2w9LehjYxPafU9PGf9iePq/CypSMpL0m9Lm2956cWiojF0kLAAuTvKRt359ZEpLUtjynAAAgAElEQVSuagyNAkYTCce32F6t+6rKoGZcD2RL4ALgUUlPENkPcwL3AZ9Pz5mB8LuuVCqVSuUPRPb/gZL+RASxz6sZpZVuIWldYFcio9mEJ/KBJdgjSPoFUXp5jKRpgRuAjwGvS/qC7T9kFdhj2L6e/M2Deon5cgsYgpeB2pOjkoN7iGDIw8BtwDaSxhLWIY8P8bpKZaKpwehKbiR9mYZNlyQAbK+XS5ftTzfHkq3YSUSG+IilZlx3QPGpXRNYiDiBuRu4zPXNqlSKJQVE9gA2Ijqkj2q/XoLPa2XKRtKngK8CGwDTAxcS/RCyBw8rUy6p6eExwBnAn9PwSsRcuK3tk3NpA5D0L6Ik/RZJXyKaV30S+AbwBdvL5tTXC6TKvxXo7IN8TBZRlYkmlcMvSnxPh8oMr1QmKZI2BkbZ/oWkpYhmrx8g+g98zfavswosnOF67NS+OpVKuUg6mLCruQpoJaqOp0SPa0mLAJfaniu3llzUwHWlUpkikHQg8GXgAOBw4IeEf9pXgD1tH5dPXWUkIWkaotHRj4lS3HpoUplsSPoncKTtoxvj3wG+Y3vBPMrG63gVWMD2Y5JOBP5r+3uS5gXusD1TTn2lI2kT4EQikeI5+m+wbHuOLMJ6iDQnL0Mcak/bfs32aV3WcmFjaGXgv0SVRNN/O1vWV2VkIWl6IgP7UdtP59ZTOpKaPU1GAUsCXwT2s/3T7qvqHWrgv5ITSf8BtrP9/9u793hd6zn/4693CR2EpBSSmgmRCKWhTFHGYRpyCGkiIkMlyjj8qMk06ehsHCPCDClGDuNQjLOUSEVS2pLODp2kw+f3x3Vtre699tp7t/e6v/fh9Xw89mPd1/e6Vvv92O21170+1/f6fMamV32SxwGfraqpnbNnq5BZJNmKxU/4ntqG6NKIezawV1V9OcmRwOeq6ldJzgF2oJsULM2rJPel23W9K91Oum/P/RnSctuAbrfcoC8BRw45y2wuAR7S77x+IvCSfn0NBgp1mtUhwOHAwVV1U+sw4ybJA4HP07UMCd0gpjvQ/d27gW6GzTBdOXB84pB/f02pJEv19E0SqmqP+c4zzqrq2NnWk5xOV0OwcD23Vwwc36bwP/w4mjIr0bVIGjlJXjW4BKxH93PlVD/Ba+F6QJL96X5AOI9FHx1we7o0utal27EEcA1wt/71l4HDmiTSVEhyd7r2ILvSDZ37BV3bhuOqakHLbJoKC+huzg0OldkRGIXhh8cA/033nupm4Ov9+lZ0fVY1tzWBj1i0vt3eBpwGPIzuJsrDgLsC/0n3ZNZQjeIjyJoa9xw43ha4BTizP34IXUHn/4YZasKcQvdvjuZg4V+NvR94PnBQ4xyz2Xvg+BbgcuDDdE+VTy0L14vaF9hn8JFbSSNvAbB+//E8up19pwFbA9c3zKXJdwlwBV1xbr+qOr1xHk2XI4F39n1Kv0t3k/2xwG4s+gZ46Krq4CRn0e0M/3RV/aU/dRPeVFwaHweegj/I316PAh5XVdcmuQW4Q99v/TV0f6YPbRsPkmwMPKg/PLuqzm+ZR5Opqv5x4eskr6N7b/zCqrq2X1udbgDYmbP/F7QUnkP3flC3j4V/DcPdgOcl2QH4KYu26WrWYaGqRnmgdFP2uB6Q5I/Aw33TKI2XJIcC11TVIf0AsE8CFwH3Bo6oqjc0DaiJlWRH4GsO11IrSZ4OvJpbi1/n0P2797l2qbQi9IOHPwv8ha6gNPgD1sEtco2LJFcBj6yq85OcB7ykqk7ui8VnVtVqDbPdg65QuBPdriroHgs+CdijqgbbikgrRN+66fFVdfbA+oOBr1fVvdokGw9JzuS2T2KH7snPteiGrX6gSbAxl+T1wJ4W7zSfkpwyx+mqqu2HFmbA0rZ0AqaupZM7rhf1SbqhWk5pl8ZIVb1uxuvjk1wE/B1wblWd1C6ZJl1VfaV1Bk2XJG8Cjqyq65JsQDewZWR75SZ5EvByYCPgiVX1myQvBi6oqq/P/dlT76V070uvAP6GRVvYWbie28+AzYHzgR8C/5rkZmBPFm2vM2wfpPt/ug3wg35tK7o2Jh8Adm6US5NvDbqnFM8eWF8PaHYzZ4wMDnVb+Dj/N6rKFlhLsKTCf5NQmhpVtV3rDHO4J4tv4/StVqFGgTuuByR5A/BK4CvM/ujA0S1ySZJGR5ILWMq5B1W10TzH0ZRJchOwflVd1hfh1quqy1rnmk2SXYH30hXp9gIe3O9+fSmwc1U9sWnAEZfkMuDQqnpr6yzjKMkTgdWr6oR+l/XngQfS3QjYparm2nk139muo9v1+r2B9a3pnuJZvU0yTbokH6HrJXwA8P1++dF07ZtOqaoXtEmmaZDkwIElC/8Sf23j9HAW08apqqZ2eKmF6wF9MWJxygKENLqS3Jdu59I6dHcm/8qbTlqRkrx6xuEawKvodvMtLEBsDWwJHOWj/FrRklxIV2D4AnAB8EgW01ez9YDQJD+hK7z+V5Krgc37wvXmwFeqat2W+UZdkiuBLavqV62zTIokawG/r8Y/BPVfx/9YVT8dWN8c+HxVbdAmmSZdklWBo4A9gFX65ZvoiiP7V9V1rbKNgyT3BKiqy/vjzYBdgLOq6pMts0kaX7ZxWjwL15ImQr+r7xi6N96XM/A4tTedNF/6nUvnVtV/DKy/jm536fObBNPESvIS4F3AynNdRvdv31zXzLt+V+mDqurCgcL1xsDPqmrVlvlGXZIjgT95A2zpJfmfpb22qnaazyxzSfIiYFdgt6r6bb92b+BY4L+q6oOtsmk69Dv5Nqb7fnHewh1+mlvfI/djVXVMkrWBXwIXA/cBDq6qo5oGHHEW/qXZ9e+Tn15VXxtYfwJwQlWt2SZZe/a4ljQpDqbbPfLGqrq5dRhNlZ2BLWZZ/zTwulnWpeVSVe9P8ilgQ+B0uh7IozrI7WJgE+DCgfVtAXcRL9lqwIv7lheztbDbp0mq0TaqXwuDXkn3NfzrJL/t1+4N/BlYJ8lf/99W1UOHH0+Tri9U/3SJF2rQQ7m1xcoz6Yr+j0ryT8ARdD+PaPE+BXwMWFj4/z+69wp7J1nfwr+m2GeADyeZrY3TCc1SjQAL17NIsgndN6ENgDvOPDdt0zulMbIu8EGL1mrgWuDvWXTQ198DPm6reVFVfwDOSPJC4JtVdUPrTIvxfuAd/TBGgPsm2QY4HDioWarx8SDgx/3rB7YMMi6q6oWtMyylwQFvksbDqsA1/esnAAuf8jgduG+TROPFwr80u5fR/f3/CLO0cWqUaSRYuB6Q5Cl0dzp+DDwCOJXuEao7MeWTPKUR90VgK+D81kE0dd4KvDvJI7nt3fHdsTCneVZVxy58neRuLNrf/6qhh7rt7394krsCXwXuDJwC3AAcWVXvbpltHFTVdq0zaH5U1b+1ziDpdvklsHOSzwA70hVbodtE84dmqcaHhX9pFlV1PfAv/Y5r2zjNYI/rAUlOA46vqkMX9mKke3TlY8D3HPAmjaYkewJvBD4KnMmij1NP9eM1ml9Jng3sS7c7EuAc4O1V9al2qTQNktwPeC+wHbfuzoDR6XG9Gl2h+k7ApnSF9bOr6po5P3GK9T2an19Vf1pCv+aqqn8aVi5JEiTZGfgk3SbAr1fVjv36G4DHVNWTW+Ybdf3Q5g/TbRY8C9ihqn7QbwD5fFWt1zSgpJFj4XpAkmuAh/aDg64Ctq2qn/VDA77ghG9pNCW5ZY7TzYs3kjQfkpwM3A04ku5G+23e2FXVN1vkAkiyMl2/3s0HJ6Rr8ZJ8GNinqq7uXy/WGLXF0IAkdwTeADyXrj3hzBtP+L5FGl1J1gXWB35SVbf0a1sBf6yqnzcNN+Is/EtaVrYKWdTVdI+yAvwO+BvgZ3R/VndvFUrS3KpqpSVfJUkTZ0vg0VX1s9ZBBlXVzUkuZGBeiOY2sxhtYXqivRnYBTiUruXUAXTDGp9D9wSZpBFVVZcClyZ5TJIfVdUNVfWD1rnGQVWdkGQD+sL/jFNfo9uFLUm3YaFnUT8AHtu//gJwVJID6R5n+V6zVJKkkZTkjkn+Lcm5Sf6c5OaZv1rn08S7gK4Nx6h6M/CWJGu3DiKNmGcDe1XV+4Cbgc9V1T7AgcAOTZNJWlpfAu7dOsS4qapLq+rHC3er92s/cLe6pNm443pRrwLW6F8fBNwFeAZwbn9O0ojqh6v+K10f1QLOBg6rqi82DaZJ5645tbQvcGiSf6mq81qHmcX+wP2B3ya5CLjNgJmqemiTVFJ769K9T4FuUNnd+tdfBg5rkkjSskrrAOMoyS7A44F1WHSo9E5NQkkaWRauB1TV+TNeXwe8rGEcSUspyYuB9wAfB47tl7cBTkzysqo6plk4TbqFu+a+nORIul1zv0pyDt2uufe1jacJ9zm6Hde/SHIDcNPMk1W1ZpNUtzq+8e8vjaoFdI/KLwDOA54InAZsDVzfMJckzZskRwCvBE5hltkckjTI4YySJkKSXwJvr6p3DazvDexdVZu0SaZJl+Q64IFVtSDJ74CnVtVpSe5PN7SndeFQEyzJ7nOdr6pj5zovqY0khwLXVNUhSZ5JN6zsIrq2A0dU1RuaBpT0V0m2Bb5bVTcNrD+PbsPCtbN/pgYluRR4eVV5Y1vSUrFwDSS5mqW802cBQhpN/U7DBw8+Kp/kb4CzqmqUe8BqjCX5OfCCqvp+km8BX6qq/+h/mHlrVa3bOKLUXJLt6do4AZxdVSe3zCONmiRbAY8Bzq2qk1rnkXSrfmbJelV1WZLzgUdV1ZWtc42jJJcDW49oezNJI8hWIZ1XtA4gabktoGvLMPgmaEfgwuHH0RQ5ka5P3/eBtwOfTLIn/a65lsE0mZKsVVVXLXw917ULr2ulf/LgBGAzukeCAdZPcibwjJkt2qRpkWQV4Djg9VX1K+gGk9ENiZc0en5PN6/hMro5JivNebXm8n7g+XTzxCRpidxxLWkiJHkp8E66/tbfpXuK4rHAbnStQt7fMJ6mSJJHA3+Hu+Y0TwZ2ft3C7E+NBaiqWnm46QZCJCcDKwO7VdWCfm0Dun+rq6q2b5lPaiXJ74FHePNGGn1J3gfsDvwO2ICurc/Ns11bVRsNMdrYSfJu4Hl0w2l/Ctw483xV7dMil6TRZeF6DkneA7ypqq5onUXSkiV5OvBq4EH90jl0fSI/1y6VJK1YSR4HfKeqbupfL1ZVfXNIsWaV5Hrg0VX1k4H1hwHfq6pV2yST2kryIeCcqjqydRZJc0sS4MnA3wJHAwcDV892bVUdNcRoYyfJKXOdr6rthpVF0niwcD2HJH8CHuZOCEnS0vD7hnRbSX4B7F5V3x9YfzTwUQfnalolORDYD/gm8CPgNsPdquroFrkkzS3Jh4F9qmrWwrUkacWycD2Hfmjj5hYgJElLw+8baiHJunRtkTYG3lhVVyR5DHBxVV3QONtTgTcB+wCn9suPAt4GHFJVn2+VTWopyVxfm2W7AUmTKMn/zHG6quqfhhZG0lhwOKOksdXvbt2oL9Jczex9XgGoqjWHl0yShiPJI4CvAxcAD6YbCHoF3bDaTej6SLb0SeBOwHeAW/q1leh6g368e/q647/TmiZVdf/WGSQtnSUUW2+jqnaazywT4MqB41WAzYH70g1zlqTbsHA9h6q6S+sMkua0N7f2l9ubOQrX0pAcB/ypdQhNlSOBt1fVgf0NvIX+F3hho0wzvaJ1AGkUJTlmMacK+DNwHvDfVXXx8FJJWozBYqtup6qa9b1JkqNYTN9wSdPNViFAkrWW9tqqumo+s0iSJC2tmX3VZ7aqSbIh8POqunPTgJJmleTzwDZ0TyL8rF9+CBDgNLonKNYAtqmqM5qElKQhSbIJ8O2qWqd1FkmjZaXWAUbEFcDlS/i18BpJIyjJ+UnuMcv63ZLYb1jzKsm/JDkryXVJNurXXpvk2a2zaeJdD9x9lvUHApcNOcuskqybZP8k/5lk7X7tMUlslaBp9h3gS8B9qmrbqtoWuA/wReArwP2ALwBHtYsoaXGSrJ1kqyR3ap1lQjygdQBJo8lWIZ3tWgeQtNw2BFaeZf1OdD8ISvMiySuB1wCHAW+Zceq3dG0SPtUil6bG54ADkzyrP65+t/VhwGdahVpoDHpwS63sC2xfVdctXKiq65IcAny9qg5PchjwtWYJJS0iyV2AY4Bn0LX2+Vvg/CTvBS6pqoMaxht5Sd4xuASsBzyJ7s9Vkm7DwjVQVd9snUHS7ZNk5xmHT0nyxxnHKwOPpyuYSPNlL2DPqvpCkn+fsX46XaFOmk/70+3KvBxYDfg2sC7dbs7/1zDXQqPeg1tqZQ26Ys05A+v36s9BNzPBn9ek0XIYsD6wBd333IVOAg4BDmqQaZxsNnB8C917mP2wcC1pFr4RmkOSewF3nLlWVQsaxZE0u+P7jwV8aODcjcCvgVcPM5Cmzv24tT/pTDcCqw45i6ZIklWArwL/DNyb7ofolYDTq2pUdmk+AnjRLOu/oyuwS9PqROBDSV4DnEr3PmZL4HDghP6aLYFz28STtBg7AU+vqjOSzBwYdg6wUaNMY6OqfNpd0jKxcD0gyV2BdwDPZqBo3ZutFYGkRqpqJYAkFwCPqqorGkfS9DmfrmB44cD6k4Gzhx9H06Kqbuz7RFdVnQyc3DrTLEa+B7fUyF7A0cBx3Poz2U10Ow7374/PAfYcfjRJc7g7cOUs63cBbh5yFkmaeA5nXNSRwObA04A/0/VePAC4CNilYS5Jc6iq+1u0ViNHAu9Ksitdn76tkxxI97joEU2TaRocy2gXthb24F44vGqkenBLrVTVdVW1F7AW8HC6G6BrVdXLqura/pozquqMljklLeJUul3XCy3cdf1S4LvDjyNJky1VteSrpkiSi4DnVtW3kvwJ2KKqzkvyXGCPqtqhcURJs0jyqrnOV9XRw8qi6ZNkT7p+wvftl34LHFRVg+1rpBUqyXuAXel6+Z8GXDvzfFXt0yLXQknWpOvBvTmwOnAJt/bgfvLCAp0kSeMgyd/RzWn4L+D5wAfpZppsBWxTVac3jCdJE8fC9YAk1wCbVtWCJL8BnllVP+h3B51VVas3DShpVn2rkJlWoRt6dD1wWVXZc07zLsnawEpVZQsEDUWSU+Y4XVW1/dDCDOh7cH+b0e7BLUnSMknyELqnsh9B933tNODwqjqzaTBJmkD2uF7Ur+iGKiyg6yv3nCQ/BHYGrmoZTNLiVdX9B9eSrAt8GPjA8BNpWiR5K/CxqjrddjUatlEecjQmPbglSVpqSTYFbqyq3fvjHelu0D41ydlVZZ9rSVqB7HG9qI8AD+1fv4WuV9Vf6PqUHtYok6TboaouBd4AHN46iybaVsCPkpyT5PX9EzqSOqPeg1uSpGXxIbq+9CS5D3AiXa/6lwP/3jCXJE0kW4UsQZINgEcCv/TRH2n8JHkEcEpVrdk6iyZXv6t0V7qBvg+gG85zHPCpqvp9y2xSS6Peg1uSpGWR5A/AllV1bpL9gJ2qarsk2wEfrqoN2yaUpMli4XqGmb0Yq+oXrfNIWnpJdh5coutx/XLg/Kp6yvBTaRol2YKugP0c4B5VtWrjSFIzo9yDW5KkZZXkamCzqvp1kpOAb1bVEf2Gt1/4vk+SVix7XM8wsxdj6yySltnxA8cFXE7XU/XVw4+jKbYKcCfgjoB9DjXVRrkHtyRJt8PPgJf1RevHA6/r1+8NOOtEklYwd1wPSHIEQFUd0DqLJGk8JNmEW1uFbAicQtcq5DNVde0cnypJkqQxkWRb4LPAXYFjq2qPfv1QYJOqekbLfJI0aSxcD7AXoyRpWST5Ed2Qnp/QFas/UVWXtE0lSZKk+ZBkZWDNmXNM+uHc11XVZa1ySdIkslXIoh4EnN6/3mjgnFV+aYQkedXSXltVR89nFk21rwC7VdU5rYNIkiRpflXVzcDvB9Z+3SaNJE02d1xLGltJLljKS6uqBm9ESZIkSZIkaURZuJYkaRkleQfwuqq6tn+9WLaYkiRJkiRp2dkqZECS/5nrfFXtNKwskqSRtRmwyozXkiRJkiRpBbJwvagrB45XATYH7gucMPw4kpZWkqcA/wpsSteT/mzgsKr6YtNgmjhVtd1sryVJkiRJ0oph4XpAVb1wtvUkRwFXDzmOpKWU5MXAe4CPA8f2y9sAJyZ5WVUd0yycJlqSNwFHVtV1A+urAgdU1cFtkkmSJEmSNL7scb2UkmwCfLuq1mmdRdKikvwSeHtVvWtgfW9g76rapE0yTbokNwPrVdVlA+v3AC6rqpXbJJMkSZIkaXyt1DrAGHlA6wCS5rQB8OVZ1r8E3G/IWTRdQteaZtDDgauGnEWSJEmSpIlgq5ABSd4xuASsBzwJsNWANLoWADsA5w2s7whcOPw4mnRJrqYrWBdwfpKZxeuVgTsD722RTZIkSZKkcWfhelGbDRzfAlwO7IeFa2mUHQm8M8kWwHfpiomPBXYD9m4ZTBPrFXQ3N48B3gD8cca5vwC/rqrvtQgmSZIkSdK4s8e1pImR5OnAq4EH9UvnAEdU1efapdKkS/I44LtVdWPrLJIkSZIkTQoL1wOSHAPsW1VXD6yvDryzqvZok0ySNOqS3Au448y1qlrQKI4kSZIkSWPLwvWAJDcD61XVZQPrawOXVJXtVaQRlORE4GPASVX1l9Z5ND2SrAm8E3g2A0VrgKpaeeihJEmSJEkacyu1DjAqkqyV5B50/Urv3h8v/HVP4KnApW1TSprD9cBHgUuTfCDJtq0DaWocBWwOPA34M/A84ADgImCXhrkkSZIkSRpb7rjuJbmFbpjb4hRwYFUdMqRIkpZRktWAnekKh08Afgd8Ajiuqs5qmU2TK8lFwHOr6ltJ/gRsUVXnJXkusEdV7dA4oiRJkiRJY8fCda8frhXgZOAZwFUzTv8FuLCqLm6RTdKy65+U2AXYC3igbX40X5JcA2xaVQuS/AZ4ZlX9IMmGwFlVtXrTgJIkSZIkjSELOb2q+iZAkvsDv6mqWxpHknQ7JbkzsD3wRGAT4DdtE2nC/QrYCFgAnAM8J8kP6Xb/XzXXJ0qSJEmSpNm543oxkqwPbMDAoK2q+r82iSTNJclKdO1BdqXrNXwzcDxdmxC/bjVvkuwH3FxV70iyPXASsArdHIl9q+pdTQNKkiRJkjSGLFwP6AvWnwC2petrHWb0vq6qlRtFkzSHJJcAdwW+BBwHnFRVf2mbStMoyQbAI4FfVtWZrfNIkiRJkjSOVmodYAS9jW6n5qbAdcA2wLPoHv/+h4a5JM3tTcB6VbVzVZ1g0VrDkGSVJD9I8oCFa1W1oP87aNFakiRJkqTbyR7Xi3oc8JSq+nmSAi6vqu8kuQF4M/DVtvEkzaaq3t86g6ZPVd3Yz0bw8SVJkiRJklYgC9eLWhW4on99FbAOcC5wNvDQVqEkza0fyLgv8Hi6r9vbPFFSVX79ar4cC+wJHNA6iCRJkiRJk8LC9aJ+DjwQ+DVwBrBXkt8ALwd+2zCXpLm9B3g68Gngu7gDVsOzOrBrkh2A04BrZ56sqn2apJIkSZIkaYw5nHFAkl2BVarqI0m2AL4MrA3cAOxeVZ9qGlDSrJJcBTy7qr7WOoumS5JT5jhdVbX90MJIkiRJkjQhLFwvQZLV6HZgL6iqK5Z0vaQ2klwEPL6qftE6iyRJkiRJkpaPhetZJNmFxffJ3alJKElzSrIP8GDgZVV1S+s8mj5J1gY2Bs6oqhta55EkSZIkaZzZ43pAkiOAVwKnABdjn1xpXOwAbAP8Q5KzgRtnnvSmk+ZLkrsAxwDPoPue8bfA+UneC1xSVQc1jCdJkiRJ0liycL2ofwaeW1XHtw4iaZlcAZzYOoSm0mHA+sAWwLdnrJ8EHAIc1CCTJEmSJEljzcL1olYCzmgdQtKyqaoXts6gqbUT8PSqOiPJzKd0zgE2apRJkiRJkqSxttKSL5k67wee3zqEpNsnyUZJnprkKUksGmoY7g5cOcv6XYCbh5xFkiRJkqSJ4I7rRd0NeF6SHYCfsmif3H2apJI0pyRrAh+i6zN8y63L+Qzwoqq6ulk4TbpT6XZdv60/Xrjr+qXAd5skkiRJkiRpzFm4XtSm3Noq5IED5xzUKI2utwMPBbbj1mLhY4D30hUUX9Qolybf64H/TfJguu+rr+pfbwls2zSZJEmSJEljKlXWYiWNvyRXAk+rqm8NrG8LnFhV92iTTNMgyWbA/sAj6NpwnQ4cVlVnNg0mSZIkSdKYcse1pEmxKrP3Gb4KuPOQs2jK9AXq3VvnkCRJkiRpUrjjWtJESPJV4E/AblV1Xb+2OvBRYM2q2qFlPk2+JOsD6zAw+LiqTm+TSJIkSZKk8WXhWtJE6Fs1fBlYjW6wagGbA9cBO1bVWQ3jaYIleThwHN1chAycrqpaefipJEmSJEkabxauJU2MJKsCz+fWAuLZwMer6vqmwTTRkpxK16bmYOBiBgb5VtWFLXJJkiRJkjTOLFxLmghJDgF+U1XvHVjfC7h3Vb2xTTJNuiTXAg+vqnNbZ5EkSZIkaVKstORLJGks7Ab8eJb104F/HnIWTZczgXu1DiFJkiRJ0iSxcC1pUqwDXD7L+hXAukPOounyeuDwJE9Ism6StWb+ah1OkiRJkqRxdIfWASRpBVkAbAOcP7C+LXDR8ONoinyt//gVbtvfOv2xwxklSZIkSVpGFq4lTYr3AW9Nckfg5H7t8cChwGHNUmkabNc6gCRJkiRJk8bhjJImRpJDgVcCd+yX/gK8vape2y6VJEmSJEmSlpWFa0kTJcnqwKZ0bRrOrqprGkfSFEiyGfBSYGNgj6r6XZKnARdW1WxDQyVJkiRJ0hwczihpolTVtVV1alX90KK1hiHJjsCpwL2B7YFV+1MbAwe2yiVJkiRJ0jizcC1J0vJ5M/Cqqno6XXuahb4BbNkkkSRJkiRJY87CtSRJy+fBwBdnWb8KWGvIWSRJkiRJmggWriVJWj6/p2sTMmgL4KIhZ5EkSZIkaWR8/LgAAAQtSURBVCJYuJYkafl8AjgiyX2AAu6Q5HHAkcBHmyaTJEmSJGlMpapaZ5AkaWwlWQX4CPAcIMAtdDeGPw68oKpubpdOkiRJkqTxZOFakqQVIMlGdO1BVgJ+XFW/bBxJkiRJkqSxZeFakqTlkOSYxZwq4M/AecB/V9XFw0slSZIkSdJ4s3AtSdJySPJ5YBu6FiE/65cfQtc25DTgwcAawDZVdUaTkJIkSZIkjRmHM0qStHy+A3wJuE9VbVtV2wL3Ab4IfAW4H/AF4Kh2ESVJkiRJGi/uuJYkaTkk+R2wfVWdM7C+KfD1qlovycOBr1XVPZqElCRJkiRpzLjjWpKk5bMGsN4s6/fqzwH8CbjD0BJJkiRJkjTmLFxLkrR8TgQ+lORZSTZMcr8kzwI+BJzQX7MlcG6zhJIkSZIkjRlbhUiStBySrAYcDbyQW3dV3wQcA+xfVdcmeRiAwxklSZIkSVo6Fq4lSVoBkqwObAwEOK+qrm0cSZIkSZKksWXhWpIkSZIkSZI0UuxxLUmSJEmSJEkaKRauJUmSJEmSJEkj5Q5LvkSSJEnSipBkafr0XVhVG853FkmSJGmUWbiWJEmShmfrgeMTgZ8AB81Yu2FoaSRJkqQRZeFakiRJGpKq+v7M4yQ3AFcMrkuSJEnTzh7XkiRJ0ghKsnWSE5NclOT6JD9P8m9J7jRw3R2SHJbk0iTXJvlKkockqSSvbZVfkiRJWh7uuJYkSZJG04bAqcCHgGuAzYA3AfcDXjDjurcA+/UfvwFsCXx2eDElSZKkFc/CtSRJkjSCquqTC18nCfBt4HrgvUn2rqqrk6wDvBx4e1W9ob/8q/0QyEOGHlqSJElaQWwVIkmSJI2gJHdPclSS8+kGNt4IfABYGdi4v+xhwJ2BTw98+vFDCypJkiTNA3dcS5IkSaPpOGBr4EDgJ8B1wDbA0XTFaoD1+o+XDXzupcMIKEmSJM0XC9eSJEnSiElyF+BJwGuq6p0z1h81cOnv+o/rAL+asb7u/CaUJEmS5petQiRJkqTRsxoQuvYgwF/7XO8+cN0ZwJ+BZw2sDx5LkiRJY8Ud15IkSdKIqapLk5wBvDbJFcAfgJcAaw9cd1mSdwP7Jbke+AawJfDC/pJbhpdakiRJWnHccS1JkiSNpmcBZwLvA44BLgAOmOW61wJHAXsCnwO2B/boz/1x/mNKkiRJK16qqnUGSZIkSStQkt2AjwJbVtWprfNIkiRJy8rCtSRJkjTGkjwWeDxwKnADXauQ1wI/rqq/bxhNkiRJut3scS1JkiSNt2voCtf7AncBLgWOA17fMpQkSZK0PNxxLUmSJEmSJEkaKQ5nlCRJkiRJkiSNFAvXkiRJkiRJkqSRYuFakiRJkiRJkjRSLFxLkiRJkiRJkkaKhWtJkiRJkiRJ0kixcC1JkiRJkiRJGin/H5j27xBUh3SbAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "all_tags = list(itertools.chain.from_iterable(df.tags.values))\n",
    "tags, tag_counts = zip(*Counter(all_tags).most_common())\n",
    "plt.figure(figsize=(25, 5))\n",
    "ax = sns.barplot(list(tags), list(tag_counts))\n",
    "plt.title(\"Tag distribution\", fontsize=20)\n",
    "plt.xlabel(\"Tag\", fontsize=16)\n",
    "ax.set_xticklabels(tags, rotation=90, fontsize=14)\n",
    "plt.ylabel(\"Number of projects\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Wordcloud\n",
    "Is there enough signal in the title and description that's unique to each tag?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='tag', index=2, options=('natural-language-processing', 'computer-v…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14e3ec019d134c4cb25215083b6a7051"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(tag=list(tags))\n",
    "def display_word_cloud(tag='pytorch'):\n",
    "    # Plot word clouds top top tags\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    subset = df[df.tags.apply(lambda tags: tag in tags)]\n",
    "    text = subset.text.values\n",
    "    cloud = WordCloud(\n",
    "        stopwords=STOPWORDS, background_color='black', collocations=False,\n",
    "        width=500, height=300).generate(\" \".join(text))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cloud)"
   ]
  },
  {
   "source": [
    "# Splitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "X = df.text.to_numpy()\n",
    "y = df.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(object):\n",
    "    \"\"\"Label encoder for tag labels.\"\"\"\n",
    "    def __init__(self, class_to_index={}):\n",
    "        self.class_to_index = class_to_index\n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        self.classes = list(self.class_to_index.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_to_index)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<LabelEncoder(num_classes={len(self)})>\"\n",
    "\n",
    "    def fit(self, y):\n",
    "        classes = np.unique(list(itertools.chain.from_iterable(y)))\n",
    "        for i, class_ in enumerate(classes):\n",
    "            self.class_to_index[class_] = i\n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        self.classes = list(self.class_to_index.keys())\n",
    "        return self\n",
    "\n",
    "    def encode(self, y):\n",
    "        y_one_hot = np.zeros((len(y), len(self.class_to_index)),dtype=int)\n",
    "        for i, item in enumerate(y):\n",
    "            for class_ in item:\n",
    "                y_one_hot[i][self.class_to_index[class_]] = 1\n",
    "        return y_one_hot\n",
    "\n",
    "    def decode(self, y):\n",
    "        classes = []\n",
    "        for i, item in enumerate(y):\n",
    "            indices = np.where(item == 1)[0]\n",
    "            classes.append([self.index_to_class[index] for index in indices])\n",
    "        return classes\n",
    "\n",
    "    def save(self, fp):\n",
    "        with open(fp, 'w') as fp:\n",
    "            contents = {'class_to_index': self.class_to_index}\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        with open(fp, 'r') as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode \n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "num_classes = len(label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention': 0,\n",
       " 'autoencoders': 1,\n",
       " 'computer-vision': 2,\n",
       " 'convolutional-neural-networks': 3,\n",
       " 'data-augmentation': 4,\n",
       " 'embeddings': 5,\n",
       " 'flask': 6,\n",
       " 'generative-adversarial-networks': 7,\n",
       " 'graph-neural-networks': 8,\n",
       " 'graphs': 9,\n",
       " 'huggingface': 10,\n",
       " 'image-classification': 11,\n",
       " 'interpretability': 12,\n",
       " 'keras': 13,\n",
       " 'language-modeling': 14,\n",
       " 'natural-language-processing': 15,\n",
       " 'node-classification': 16,\n",
       " 'object-detection': 17,\n",
       " 'pretraining': 18,\n",
       " 'production': 19,\n",
       " 'pytorch': 20,\n",
       " 'question-answering': 21,\n",
       " 'regression': 22,\n",
       " 'reinforcement-learning': 23,\n",
       " 'representation-learning': 24,\n",
       " 'scikit-learn': 25,\n",
       " 'segmentation': 26,\n",
       " 'self-supervised-learning': 27,\n",
       " 'tensorflow': 28,\n",
       " 'tensorflow-js': 29,\n",
       " 'time-series': 30,\n",
       " 'transfer-learning': 31,\n",
       " 'transformers': 32,\n",
       " 'unsupervised-learning': 33,\n",
       " 'wandb': 34}"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "\n",
    "label_encoder.class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = label_encoder.encode(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n  File \"c:\\python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\python39\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Python39\\Scripts\\pip.exe\\__main__.py\", line 4, in <module>\nModuleNotFoundError: No module named 'pip'\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-multilearn==0.2.0 -q"
   ]
  },
  {
   "source": [
    "Naive splitiing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.model_selection.measures import get_combination_wise_output_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sizes\n",
    "train_size=0.7\n",
    "val_size=0.15\n",
    "test_size=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split (train)\n",
    "X_train, X_, y_train, y_ = train_test_split(X, y, train_size=train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train: 1010 (0.70)\nremaining: 434 (0.30)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (f\"train: {len(X_train)} ({(len(X_train) / len(X)):.2f})\\n\"\n",
    "       f\"remaining: {len(X_)} ({(len(X_) / len(X)):.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_, y_, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train: 1010 (0.70)\nval: 217 (0.15)\ntest: 217 (0.15)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train: {len(X_train)} ({len(X_train)/len(X):.2f})\\n\"\n",
    "      f\"val: {len(X_val)} ({len(X_val)/len(X):.2f})\\n\"\n",
    "      f\"test: {len(X_test)} ({len(X_test)/len(X):.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts for each class\n",
    "counts = {}\n",
    "counts['train_counts'] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "    y_train, order=1) for combination in row)\n",
    "counts['val_counts'] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "    y_val, order=1) for combination in row)\n",
    "counts['test_counts'] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "    y_test, order=1) for combination in row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       (15,)  (19,)  (33,)  (21,)  (32,)  (14,)  (2,)  (20,)  (24,)  (5,)  \\\n",
       "train    314     37     26     26    145     33   274    191     41    55   \n",
       "val       58      8      4      2     29      8    53     33      7     9   \n",
       "test      57      6      9      4     22     10    61     34      9    11   \n",
       "\n",
       "       ...  (1,)  (7,)  (11,)  (18,)  (4,)  (6,)  (29,)  (8,)  (31,)  (30,)  \n",
       "train  ...    27    50     34     21    33    24     24    32     33     23  \n",
       "val    ...     3     7      6      3     3     5      7    10      7      4  \n",
       "test   ...    11    16     11      6     5     5      9     9      6      7  \n",
       "\n",
       "[3 rows x 35 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>(15,)</th>\n      <th>(19,)</th>\n      <th>(33,)</th>\n      <th>(21,)</th>\n      <th>(32,)</th>\n      <th>(14,)</th>\n      <th>(2,)</th>\n      <th>(20,)</th>\n      <th>(24,)</th>\n      <th>(5,)</th>\n      <th>...</th>\n      <th>(1,)</th>\n      <th>(7,)</th>\n      <th>(11,)</th>\n      <th>(18,)</th>\n      <th>(4,)</th>\n      <th>(6,)</th>\n      <th>(29,)</th>\n      <th>(8,)</th>\n      <th>(31,)</th>\n      <th>(30,)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train</th>\n      <td>314</td>\n      <td>37</td>\n      <td>26</td>\n      <td>26</td>\n      <td>145</td>\n      <td>33</td>\n      <td>274</td>\n      <td>191</td>\n      <td>41</td>\n      <td>55</td>\n      <td>...</td>\n      <td>27</td>\n      <td>50</td>\n      <td>34</td>\n      <td>21</td>\n      <td>33</td>\n      <td>24</td>\n      <td>24</td>\n      <td>32</td>\n      <td>33</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>val</th>\n      <td>58</td>\n      <td>8</td>\n      <td>4</td>\n      <td>2</td>\n      <td>29</td>\n      <td>8</td>\n      <td>53</td>\n      <td>33</td>\n      <td>7</td>\n      <td>9</td>\n      <td>...</td>\n      <td>3</td>\n      <td>7</td>\n      <td>6</td>\n      <td>3</td>\n      <td>3</td>\n      <td>5</td>\n      <td>7</td>\n      <td>10</td>\n      <td>7</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>57</td>\n      <td>6</td>\n      <td>9</td>\n      <td>4</td>\n      <td>22</td>\n      <td>10</td>\n      <td>61</td>\n      <td>34</td>\n      <td>9</td>\n      <td>11</td>\n      <td>...</td>\n      <td>11</td>\n      <td>16</td>\n      <td>11</td>\n      <td>6</td>\n      <td>5</td>\n      <td>5</td>\n      <td>9</td>\n      <td>9</td>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 35 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# View distributions\n",
    "pd.DataFrame({\n",
    "    'train': counts['train_counts'],\n",
    "    'val': counts['val_counts'],\n",
    "    'test': counts['test_counts']\n",
    "}).T.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust counts across splits\n",
    "for k in counts['val_counts'].keys():\n",
    "    counts['val_counts'][k] = int(counts['val_counts'][k] * \\\n",
    "        (train_size/val_size))\n",
    "for k in counts['test_counts'].keys():\n",
    "    counts['test_counts'][k] = int(counts['test_counts'][k] * \\\n",
    "        (train_size/test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       (15,)  (19,)  (33,)  (21,)  (32,)  (14,)  (2,)  (20,)  (24,)  (5,)  \\\n",
       "train    314     37     26     26    145     33   274    191     41    55   \n",
       "val      270     37     18      9    135     37   247    154     32    42   \n",
       "test     266     28     42     18    102     46   284    158     42    51   \n",
       "\n",
       "       ...  (1,)  (7,)  (11,)  (18,)  (4,)  (6,)  (29,)  (8,)  (31,)  (30,)  \n",
       "train  ...    27    50     34     21    33    24     24    32     33     23  \n",
       "val    ...    14    32     28     14    14    23     32    46     32     18  \n",
       "test   ...    51    74     51     28    23    23     42    42     28     32  \n",
       "\n",
       "[3 rows x 35 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>(15,)</th>\n      <th>(19,)</th>\n      <th>(33,)</th>\n      <th>(21,)</th>\n      <th>(32,)</th>\n      <th>(14,)</th>\n      <th>(2,)</th>\n      <th>(20,)</th>\n      <th>(24,)</th>\n      <th>(5,)</th>\n      <th>...</th>\n      <th>(1,)</th>\n      <th>(7,)</th>\n      <th>(11,)</th>\n      <th>(18,)</th>\n      <th>(4,)</th>\n      <th>(6,)</th>\n      <th>(29,)</th>\n      <th>(8,)</th>\n      <th>(31,)</th>\n      <th>(30,)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train</th>\n      <td>314</td>\n      <td>37</td>\n      <td>26</td>\n      <td>26</td>\n      <td>145</td>\n      <td>33</td>\n      <td>274</td>\n      <td>191</td>\n      <td>41</td>\n      <td>55</td>\n      <td>...</td>\n      <td>27</td>\n      <td>50</td>\n      <td>34</td>\n      <td>21</td>\n      <td>33</td>\n      <td>24</td>\n      <td>24</td>\n      <td>32</td>\n      <td>33</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>val</th>\n      <td>270</td>\n      <td>37</td>\n      <td>18</td>\n      <td>9</td>\n      <td>135</td>\n      <td>37</td>\n      <td>247</td>\n      <td>154</td>\n      <td>32</td>\n      <td>42</td>\n      <td>...</td>\n      <td>14</td>\n      <td>32</td>\n      <td>28</td>\n      <td>14</td>\n      <td>14</td>\n      <td>23</td>\n      <td>32</td>\n      <td>46</td>\n      <td>32</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>266</td>\n      <td>28</td>\n      <td>42</td>\n      <td>18</td>\n      <td>102</td>\n      <td>46</td>\n      <td>284</td>\n      <td>158</td>\n      <td>42</td>\n      <td>51</td>\n      <td>...</td>\n      <td>51</td>\n      <td>74</td>\n      <td>51</td>\n      <td>28</td>\n      <td>23</td>\n      <td>23</td>\n      <td>42</td>\n      <td>42</td>\n      <td>28</td>\n      <td>32</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 35 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "dist_df = pd.DataFrame({\n",
    "    'train': counts['train_counts'],\n",
    "    'val': counts['val_counts'],\n",
    "    'test': counts['test_counts']\n",
    "}).T.fillna(0)\n",
    "dist_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.936725114942407"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "np.mean(np.std(dist_df.to_numpy(), axis=0))"
   ]
  },
  {
   "source": [
    "Stratified split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import IterativeStratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_train_test_split(X, y, train_size):\n",
    "    \"\"\"Custom iterative train test split which\n",
    "    'maintains balanced representation with respect\n",
    "    to order-th label combinations.'\n",
    "    \"\"\"\n",
    "    stratifier = IterativeStratification(\n",
    "        n_splits=2, order=1, sample_distribution_per_fold=[1.0-train_size, train_size, ])\n",
    "    train_indices, test_indices = next(stratifier.split(X, y))\n",
    "    X_train, y_train = X[train_indices], y[train_indices]\n",
    "    X_test, y_test = X[test_indices], y[test_indices]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "X = df.text.to_numpy()\n",
    "y = df.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize y\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "y = label_encoder.encode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_, y_train, y_ = iterative_train_test_split(\n",
    "    X, y, train_size=train_size)\n",
    "X_val, X_test, y_val, y_test = iterative_train_test_split(\n",
    "    X_, y_, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train: 1000 (0.69)\nval: 214 (0.15)\ntest: 230 (0.16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train: {len(X_train)} ({len(X_train)/len(X):.2f})\\n\"\n",
    "      f\"val: {len(X_val)} ({len(X_val)/len(X):.2f})\\n\"\n",
    "      f\"test: {len(X_test)} ({len(X_test)/len(X):.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts for each class\n",
    "counts = {}\n",
    "counts['train_counts'] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "    y_train, order=1) for combination in row)\n",
    "counts['val_counts'] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "    y_val, order=1) for combination in row)\n",
    "counts['test_counts'] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "    y_test, order=1) for combination in row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust counts across splits\n",
    "for k in counts['val_counts'].keys():\n",
    "    counts['val_counts'][k] = int(counts['val_counts'][k] * \\\n",
    "        (train_size/val_size))\n",
    "for k in counts['test_counts'].keys():\n",
    "    counts['test_counts'][k] = int(counts['test_counts'][k] * \\\n",
    "        (train_size/test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        (2,)  (4,)  (15,)  (14,)  (30,)  (34,)  (1,)  (26,)  (32,)  (20,)  \\\n",
       "train  272.0  29.0  300.0   36.0   24.0   27.0  29.0   32.0  145.0  181.0   \n",
       "val    270.0  32.0  298.0   28.0   23.0   28.0  32.0   37.0  112.0  177.0   \n",
       "test   270.0  23.0  303.0   42.0   23.0   28.0  23.0   37.0  126.0  182.0   \n",
       "\n",
       "       ...  (28,)  (11,)  (22,)  (8,)  (29,)  (23,)  (31,)  (10,)  (18,)  \\\n",
       "train  ...  149.0   30.0   34.0  38.0   26.0   41.0   32.0   45.0   21.0   \n",
       "val    ...  149.0   46.0   32.0  32.0   32.0   42.0   32.0   46.0   23.0   \n",
       "test   ...  149.0   51.0   37.0  28.0   32.0   42.0   32.0   42.0   18.0   \n",
       "\n",
       "       (12,)  \n",
       "train   38.0  \n",
       "val     42.0  \n",
       "test    37.0  \n",
       "\n",
       "[3 rows x 35 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>(2,)</th>\n      <th>(4,)</th>\n      <th>(15,)</th>\n      <th>(14,)</th>\n      <th>(30,)</th>\n      <th>(34,)</th>\n      <th>(1,)</th>\n      <th>(26,)</th>\n      <th>(32,)</th>\n      <th>(20,)</th>\n      <th>...</th>\n      <th>(28,)</th>\n      <th>(11,)</th>\n      <th>(22,)</th>\n      <th>(8,)</th>\n      <th>(29,)</th>\n      <th>(23,)</th>\n      <th>(31,)</th>\n      <th>(10,)</th>\n      <th>(18,)</th>\n      <th>(12,)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train</th>\n      <td>272.0</td>\n      <td>29.0</td>\n      <td>300.0</td>\n      <td>36.0</td>\n      <td>24.0</td>\n      <td>27.0</td>\n      <td>29.0</td>\n      <td>32.0</td>\n      <td>145.0</td>\n      <td>181.0</td>\n      <td>...</td>\n      <td>149.0</td>\n      <td>30.0</td>\n      <td>34.0</td>\n      <td>38.0</td>\n      <td>26.0</td>\n      <td>41.0</td>\n      <td>32.0</td>\n      <td>45.0</td>\n      <td>21.0</td>\n      <td>38.0</td>\n    </tr>\n    <tr>\n      <th>val</th>\n      <td>270.0</td>\n      <td>32.0</td>\n      <td>298.0</td>\n      <td>28.0</td>\n      <td>23.0</td>\n      <td>28.0</td>\n      <td>32.0</td>\n      <td>37.0</td>\n      <td>112.0</td>\n      <td>177.0</td>\n      <td>...</td>\n      <td>149.0</td>\n      <td>46.0</td>\n      <td>32.0</td>\n      <td>32.0</td>\n      <td>32.0</td>\n      <td>42.0</td>\n      <td>32.0</td>\n      <td>46.0</td>\n      <td>23.0</td>\n      <td>42.0</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>270.0</td>\n      <td>23.0</td>\n      <td>303.0</td>\n      <td>42.0</td>\n      <td>23.0</td>\n      <td>28.0</td>\n      <td>23.0</td>\n      <td>37.0</td>\n      <td>126.0</td>\n      <td>182.0</td>\n      <td>...</td>\n      <td>149.0</td>\n      <td>51.0</td>\n      <td>37.0</td>\n      <td>28.0</td>\n      <td>32.0</td>\n      <td>42.0</td>\n      <td>32.0</td>\n      <td>42.0</td>\n      <td>18.0</td>\n      <td>37.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 35 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "# View distributions\n",
    "pd.DataFrame({\n",
    "    'train': counts['train_counts'],\n",
    "    'val': counts['val_counts'],\n",
    "    'test': counts['test_counts']\n",
    "}).T.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df = pd.DataFrame({\n",
    "    'train': counts['train_counts'],\n",
    "    'val': counts['val_counts'],\n",
    "    'test': counts['test_counts']\n",
    "}).T.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.142338654518357"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "# Standard deviation\n",
    "np.mean(np.std(dist_df.to_numpy(), axis=0))"
   ]
  },
  {
   "source": [
    "Iterative stratification essentially creates splits while \"trying to maintain balanced representation with respect to order-th label combinations\". We used to an order=1 for our iterative split which means we cared about providing representative distribution of each tag across the splits. But we can account for higher-order label relationships as well where we may care about the distribution of label combinations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Preporcessing "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\KATTUBOINA\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS=stopwords.words('english')\n",
    "porter=PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text,lower=True,stem=False,filters=\"[!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~]\",stopwords=STOPWORDS):\n",
    "    \"\"\"\n",
    "    Conditional preprocessing on our text unique to our task.\n",
    "    \"\"\"\n",
    "    if lower:\n",
    "        text=text.lower()\n",
    "    \n",
    "\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
    "    text=pattern.sub('', text)\n",
    "\n",
    "     #spacing and filters\n",
    "    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\n",
    "    text = re.sub(filters, r\"\", text)\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric chars\n",
    "    text = re.sub(' +', ' ', text)  # remove multiple spaces\n",
    "    text = text.strip()\n",
    "\n",
    "\n",
    "    text=re.sub(r'http\\S+', '', text)\n",
    "\n",
    "\n",
    "    if stem:\n",
    "        text=\" \".join([porter.stem(word) for word in text.split(' ')])\n",
    "    \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Checkbox(value=True, description='lower'), Checkbox(value=False, description='stem'), Ou…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "619745a5d4f342b9884a375bd232d3d2"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(lower=True,stem=False)\n",
    "def display_preprocessed_text(lower,stem):\n",
    "    text=\"Conditional image generation using Variational Autoencoders and GANs.\"\n",
    "    preprocessed_text = preprocess(text=text, lower=lower, stem=stem)\n",
    "    print (preprocessed_text) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Albumentations Fast image augmentation library and easy to use wrapper around other libraries.\nalbumentations fast image augmentation library easy use wrapper around libraries\n"
     ]
    }
   ],
   "source": [
    "#Applying the changes to our dataset\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True, stem=False)\n",
    "print (f\"{df.text.values[0]}\\n{preprocessed_df.text.values[0]}\")"
   ]
  },
  {
   "source": [
    "# Model Building"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Some helpfull functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=1234):\n",
    "    \"\"\" Setting seed for reproducibility \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) #For multi-gpu training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(dftrain_size=0.7):\n",
    "    X=df.text.to_numpy()\n",
    "    y=df.tags\n",
    "\n",
    "\n",
    "    #Binarize y\n",
    "    label_encoder=LabelEncoder()\n",
    "    label_encoder.fit(y)\n",
    "    y=label_encoder.encode(y)\n",
    "\n",
    "    #Spliting the data \n",
    "    X_train, X_, y_train, y_ = iterative_train_test_split(X, y, train_size=train_size)\n",
    "    X_val, X_test, y_val, y_test = iterative_train_test_split(X_, y_, train_size=0.5)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test,label_encoder\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\n",
    "\n",
    "        # Set params\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train_step(self, dataloader):\n",
    "        \"\"\"Train step.\"\"\"\n",
    "        # Set model to train mode\n",
    "        self.model.train()\n",
    "        loss = 0.0\n",
    "\n",
    "        # Iterate over train batches\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Step\n",
    "            batch = [item.to(self.device) for item in batch]  # Set device\n",
    "            inputs, targets = batch[:-1], batch[-1]\n",
    "            self.optimizer.zero_grad()  # Reset gradients\n",
    "            z = self.model(inputs)  # Forward pass\n",
    "            J = self.loss_fn(z, targets)  # Define loss\n",
    "            J.backward()  # Backward pass\n",
    "            self.optimizer.step()  # Update weights\n",
    "\n",
    "            # Cumulative Metrics\n",
    "            loss += (J.detach().item() - loss) / (i + 1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def eval_step(self, dataloader):\n",
    "        \"\"\"Validation or test step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        loss = 0.0\n",
    "        y_trues, y_probs = [], []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Step\n",
    "                batch = [item.to(self.device) for item in batch]  # Set device\n",
    "                inputs, y_true = batch[:-1], batch[-1]\n",
    "                z = self.model(inputs)  # Forward pass\n",
    "                J = self.loss_fn(z, y_true).item()\n",
    "\n",
    "                # Cumulative Metrics\n",
    "                loss += (J - loss) / (i + 1)\n",
    "\n",
    "                # Store outputs\n",
    "                y_prob = torch.sigmoid(z).cpu().numpy()\n",
    "                y_probs.extend(y_prob)\n",
    "                y_trues.extend(y_true.cpu().numpy())\n",
    "\n",
    "        return loss, np.vstack(y_trues), np.vstack(y_probs)\n",
    "\n",
    "    def predict_step(self, dataloader):\n",
    "        \"\"\"Prediction step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        y_probs = []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Forward pass w/ inputs\n",
    "                inputs, targets = batch[:-1], batch[-1]\n",
    "                y_prob = self.model(inputs)\n",
    "\n",
    "                # Store outputs\n",
    "                y_probs.extend(y_prob)\n",
    "\n",
    "        return np.vstack(y_probs)\n",
    "\n",
    "    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n",
    "        best_val_loss = np.inf\n",
    "        for epoch in range(num_epochs):\n",
    "            # Steps\n",
    "            train_loss = self.train_step(dataloader=train_dataloader)\n",
    "            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = self.model\n",
    "                _patience = patience  # reset _patience\n",
    "            else:\n",
    "                _patience -= 1\n",
    "            if not _patience:  # 0\n",
    "                print(\"Stopping early!\")\n",
    "                break\n",
    "\n",
    "            # Logging\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1} | \"\n",
    "                f\"train_loss: {train_loss:.5f}, \"\n",
    "                f\"val_loss: {val_loss:.5f}, \"\n",
    "                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n",
    "                f\"_patience: {_patience}\"\n",
    "            )\n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(y_true,y_pred,classes):\n",
    "    \"\"\"Getting the per class performance\"\"\"\n",
    "    #Getting metrics\n",
    "    performance={'overall':{}, 'class':{}}\n",
    "    metrics=precision_recall_fscore_support(y_true,y_pred)\n",
    "\n",
    "\n",
    "    #overall performance \n",
    "    performance['overall']['precision'] = np.mean(metrics[0])\n",
    "    performance['overall']['recall'] = np.mean(metrics[1])\n",
    "    performance['overall']['f1'] = np.mean(metrics[2])\n",
    "    performance['overall']['num_samples'] = np.float64(np.sum(metrics[3]))\n",
    "\n",
    "\n",
    "\n",
    "    # Per-class performance\n",
    "    for i in range(len(classes)):\n",
    "        performance['class'][classes[i]] = {\n",
    "            \"precision\": metrics[0][i],\n",
    "            \"recall\": metrics[1][i],\n",
    "            \"f1\": metrics[2][i],\n",
    "            \"num_samples\": np.float64(metrics[3][i])\n",
    "        }\n",
    "\n",
    "    \n",
    "\n",
    "    return performance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Random \n",
    "What will the random performance look like\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train: (1000,), y_train: (1000, 35)\nX_val: (227,), y_val: (227, 35)\nX_test: (217,), y_test: (217, 35)\n"
     ]
    }
   ],
   "source": [
    "#Getting the data splits\n",
    "preprocessed_df=df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True, stem=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)\n",
    "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<LabelEncoder(num_classes=35)>\n['attention', 'autoencoders', 'computer-vision', 'convolutional-neural-networks', 'data-augmentation', 'embeddings', 'flask', 'generative-adversarial-networks', 'graph-neural-networks', 'graphs', 'huggingface', 'image-classification', 'interpretability', 'keras', 'language-modeling', 'natural-language-processing', 'node-classification', 'object-detection', 'pretraining', 'production', 'pytorch', 'question-answering', 'regression', 'reinforcement-learning', 'representation-learning', 'scikit-learn', 'segmentation', 'self-supervised-learning', 'tensorflow', 'tensorflow-js', 'time-series', 'transfer-learning', 'transformers', 'unsupervised-learning', 'wandb']\n"
     ]
    }
   ],
   "source": [
    "# Label encoder\n",
    "print (label_encoder)\n",
    "print (label_encoder.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(217, 35)\n[[0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1]\n [0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1]\n [1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1]\n [0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0]\n [0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate random predictions\n",
    "y_pred = np.random.randint(low=0, high=2, size=(len(y_test), len(label_encoder.classes)))\n",
    "print (y_pred.shape)\n",
    "print (y_pred[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"precision\": 0.0662941602216066,\n  \"recall\": 0.5065299488415251,\n  \"f1\": 0.10819194263879019,\n  \"num_samples\": 480.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "performance = get_performance(\n",
    "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "print (json.dumps(performance['overall'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.06291428571428571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Percentage of 1s (tag presence)\n",
    "tag_p = np.sum(np.sum(y_train)) / (len(y_train) * len(label_encoder.classes))\n",
    "print (tag_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate weighted random predictions\n",
    "y_pred = np.random.choice(\n",
    "    np.arange(0, 2), size=(len(y_test), len(label_encoder.classes)),\n",
    "    p=[1-tag_p, tag_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.06240947992100066"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "# Validate percentage\n",
    "np.sum(np.sum(y_pred)) / (len(y_pred) * len(label_encoder.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"precision\": 0.060484184552507536,\n  \"recall\": 0.053727634571230636,\n  \"f1\": 0.048704498064854516,\n  \"num_samples\": 480.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "performance = get_performance(\n",
    "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "print (json.dumps(performance['overall'], indent=2))\n"
   ]
  },
  {
   "source": [
    "## Rule Based \n",
    "We need to signals our inputs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data splits\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "400\n35\n"
     ]
    }
   ],
   "source": [
    "# Restrict to relevant tags\n",
    "print (len(tags_dict))\n",
    "tags_dict = {tag: tags_dict[tag] for tag in label_encoder.classes}\n",
    "print (len(tags_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention': 'attention',\n",
       " 'autoencoders': 'autoencoders',\n",
       " 'ae': 'autoencoders',\n",
       " 'computer vision': 'computer-vision',\n",
       " 'cv': 'computer-vision',\n",
       " 'vision': 'computer-vision',\n",
       " 'convolutional neural networks': 'convolutional-neural-networks',\n",
       " 'cnn': 'convolutional-neural-networks',\n",
       " 'data augmentation': 'data-augmentation',\n",
       " 'embeddings': 'embeddings',\n",
       " 'flask': 'flask',\n",
       " 'generative adversarial networks': 'generative-adversarial-networks',\n",
       " 'gan': 'generative-adversarial-networks',\n",
       " 'graph neural networks': 'graph-neural-networks',\n",
       " 'gnn': 'graph-neural-networks',\n",
       " 'graphs': 'graphs',\n",
       " 'huggingface': 'huggingface',\n",
       " 'image classification': 'image-classification',\n",
       " 'interpretability': 'interpretability',\n",
       " 'keras': 'keras',\n",
       " 'language modeling': 'language-modeling',\n",
       " 'lm': 'language-modeling',\n",
       " 'natural language processing': 'natural-language-processing',\n",
       " 'nlp': 'natural-language-processing',\n",
       " 'nlproc': 'natural-language-processing',\n",
       " 'node classification': 'node-classification',\n",
       " 'object detection': 'object-detection',\n",
       " 'pretraining': 'pretraining',\n",
       " 'pre training': 'pretraining',\n",
       " 'production': 'production',\n",
       " 'pytorch': 'pytorch',\n",
       " 'question answering': 'question-answering',\n",
       " 'qa': 'question-answering',\n",
       " 'regression': 'regression',\n",
       " 'reinforcement learning': 'reinforcement-learning',\n",
       " 'rl': 'reinforcement-learning',\n",
       " 'representation learning': 'representation-learning',\n",
       " 'scikit learn': 'scikit-learn',\n",
       " 'sklearn': 'scikit-learn',\n",
       " 'segmentation': 'segmentation',\n",
       " 'image segmentation': 'segmentation',\n",
       " 'self supervised learning': 'self-supervised-learning',\n",
       " 'tensorflow': 'tensorflow',\n",
       " 'tf': 'tensorflow',\n",
       " 'tensorflow js': 'tensorflow-js',\n",
       " 'tf js': 'tensorflow-js',\n",
       " 'time series': 'time-series',\n",
       " 'time series analysis': 'time-series',\n",
       " 'transfer learning': 'transfer-learning',\n",
       " 'transformers': 'transformers',\n",
       " 'unsupervised learning': 'unsupervised-learning',\n",
       " 'wandb': 'wandb',\n",
       " 'weights biases': 'wandb'}"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "#Map aliases\n",
    "aliases={}\n",
    "for tag,values in tags_dict.items():\n",
    "    aliases[preprocess(tag)]=tag\n",
    "    for alias in values['aliases']:\n",
    "        aliases[preprocess(alias)]=tag\n",
    "\n",
    "aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(text,aliases,tags_dict):\n",
    "    \"\"\"\n",
    "    if a token matches an alias ,then add the corresponding tag class\n",
    "    \"\"\"\n",
    "    classes=[]\n",
    "    for alias,tag in aliases.items():\n",
    "        if alias in text:\n",
    "            classes.append(tag)\n",
    "            for parent in tags_dict[tag][\"parents\"]:\n",
    "                classes.append(parent)\n",
    "        \n",
    "    \n",
    "    return list(set(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['data-augmentation',\n",
       " 'computer-vision',\n",
       " 'object-detection',\n",
       " 'generative-adversarial-networks']"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "#Sample for above function\n",
    "# Sample\n",
    "text = \"This project extends gans for data augmentation specifically for object detection tasks.\"\n",
    "get_classes(text=preprocess(text), aliases=aliases, tags_dict=tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction \n",
    "y_pred=[]\n",
    "for text in X_test:\n",
    "    classes=get_classes(text,aliases,tags_dict)\n",
    "    y_pred.append(classes)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Labels\n",
    "y_pred=label_encoder.encode(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n    \"precision\": 0.3958730158730158,\n    \"recall\": 0.10910285028326902,\n    \"f1\": 0.15424993307346246,\n    \"num_samples\": 480.0\n}\n"
     ]
    }
   ],
   "source": [
    "performance = get_performance(y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "print (json.dumps(performance['overall'], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"precision\": 1.0,\n  \"recall\": 0.12,\n  \"f1\": 0.21428571428571425,\n  \"num_samples\": 25.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Inspection\n",
    "tag = \"transformers\"\n",
    "print (json.dumps(performance[\"class\"][tag], indent=2))"
   ]
  },
  {
   "source": [
    "### Stemming "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "generative-adversarial-networks\ngenerative-adversarial-networks\n"
     ]
    }
   ],
   "source": [
    "print (aliases[preprocess('gan')])\n",
    "# print (aliases[preprocess('gans')]) # this won't find any match\n",
    "print (aliases[preprocess('generative adversarial networks')])\n",
    "# print (aliases[preprocess('generative adversarial network')]) # this won't find any match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "democraci\ndemocraci\n"
     ]
    }
   ],
   "source": [
    "print (porter.stem(\"democracy\"))\n",
    "print (porter.stem(\"democracies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data splits\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True, stem=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attent': 'attention',\n",
       " 'autoencod': 'autoencoders',\n",
       " 'ae': 'autoencoders',\n",
       " 'comput vision': 'computer-vision',\n",
       " 'cv': 'computer-vision',\n",
       " 'vision': 'computer-vision',\n",
       " 'convolut neural network': 'convolutional-neural-networks',\n",
       " 'cnn': 'convolutional-neural-networks',\n",
       " 'data augment': 'data-augmentation',\n",
       " 'embed': 'embeddings',\n",
       " 'flask': 'flask',\n",
       " 'gener adversari network': 'generative-adversarial-networks',\n",
       " 'gan': 'generative-adversarial-networks',\n",
       " 'graph neural network': 'graph-neural-networks',\n",
       " 'gnn': 'graph-neural-networks',\n",
       " 'graph': 'graphs',\n",
       " 'huggingfac': 'huggingface',\n",
       " 'imag classif': 'image-classification',\n",
       " 'interpret': 'interpretability',\n",
       " 'kera': 'keras',\n",
       " 'languag model': 'language-modeling',\n",
       " 'lm': 'language-modeling',\n",
       " 'natur languag process': 'natural-language-processing',\n",
       " 'nlp': 'natural-language-processing',\n",
       " 'nlproc': 'natural-language-processing',\n",
       " 'node classif': 'node-classification',\n",
       " 'object detect': 'object-detection',\n",
       " 'pretrain': 'pretraining',\n",
       " 'pre train': 'pretraining',\n",
       " 'product': 'production',\n",
       " 'pytorch': 'pytorch',\n",
       " 'question answer': 'question-answering',\n",
       " 'qa': 'question-answering',\n",
       " 'regress': 'regression',\n",
       " 'reinforc learn': 'reinforcement-learning',\n",
       " 'rl': 'reinforcement-learning',\n",
       " 'represent learn': 'representation-learning',\n",
       " 'scikit learn': 'scikit-learn',\n",
       " 'sklearn': 'scikit-learn',\n",
       " 'segment': 'segmentation',\n",
       " 'imag segment': 'segmentation',\n",
       " 'self supervis learn': 'self-supervised-learning',\n",
       " 'tensorflow': 'tensorflow',\n",
       " 'tf': 'tensorflow',\n",
       " 'tensorflow js': 'tensorflow-js',\n",
       " 'tf js': 'tensorflow-js',\n",
       " 'time seri': 'time-series',\n",
       " 'time seri analysi': 'time-series',\n",
       " 'transfer learn': 'transfer-learning',\n",
       " 'transform': 'transformers',\n",
       " 'unsupervis learn': 'unsupervised-learning',\n",
       " 'wandb': 'wandb',\n",
       " 'weight bias': 'wandb'}"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "# Map aliases\n",
    "aliases = {}\n",
    "for tag, values in tags_dict.items():\n",
    "    aliases[preprocess(tag, stem=True)] = tag\n",
    "    for alias in values['aliases']:\n",
    "        aliases[preprocess(alias, stem=True)] = tag\n",
    "aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "generative-adversarial-networks\ngenerative-adversarial-networks\ngenerative-adversarial-networks\ngenerative-adversarial-networks\n"
     ]
    }
   ],
   "source": [
    "# Checks (we will write proper tests soon)\n",
    "print (aliases[preprocess('gan', stem=True)])\n",
    "print (aliases[preprocess('gans', stem=True)])\n",
    "print (aliases[preprocess('generative adversarial network', stem=True)])\n",
    "print (aliases[preprocess('generative adversarial networks', stem=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['data-augmentation',\n",
       " 'computer-vision',\n",
       " 'object-detection',\n",
       " 'generative-adversarial-networks']"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "# Sample\n",
    "text = \"This project extends gans for data augmentation specifically for object detection tasks.\"\n",
    "get_classes(text=preprocess(text, stem=True), aliases=aliases, tags_dict=tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred = []\n",
    "for text in X_test:\n",
    "    classes = get_classes(text, aliases, tags_dict)\n",
    "    y_pred.append(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "y_pred = label_encoder.encode(y_pred)"
   ]
  },
  {
   "source": [
    "#### Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n    \"precision\": 0.4197732426303855,\n    \"recall\": 0.09591888816059155,\n    \"f1\": 0.14942598936879561,\n    \"num_samples\": 473.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "performance = get_performance(y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "print (json.dumps(performance['overall'], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"precision\": 0.8888888888888888,\n  \"recall\": 0.2962962962962963,\n  \"f1\": 0.4444444444444444,\n  \"num_samples\": 27.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Inspection\n",
    "tag = \"transformers\"\n",
    "print (json.dumps(performance[\"class\"][tag], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP, FP, FN samples\n",
    "index = label_encoder.class_to_index[tag]\n",
    "tp, fp, fn = [], [], []\n",
    "for i in range(len(y_test)):\n",
    "    true = y_test[i][index]\n",
    "    pred = y_pred[i][index]\n",
    "    if true and pred:\n",
    "        tp.append(i)\n",
    "    elif not true and pred:\n",
    "        fp.append(i)\n",
    "    elif true and not pred:\n",
    "        fn.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 46, 94, 165, 169, 190, 194, 199]\n[49]\n[4, 14, 15, 18, 28, 54, 61, 63, 72, 75, 89, 99, 137, 141, 142, 160, 163, 174, 206]\n"
     ]
    }
   ],
   "source": [
    "print (tp)\n",
    "print (fp)\n",
    "print (fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Insight Project Insight is designed to create NLP as a service with code base for both front end GUI (streamlit) and backend server (FastAPI) the usage of transformers \ntrue: ['attention', 'huggingface', 'natural-language-processing', 'pytorch', 'transfer-learning', 'transformers']\npred: ['natural-language-processing', 'transformers']\n\n"
     ]
    }
   ],
   "source": [
    "index = tp[0]\n",
    "print (X_test[index])\n",
    "print (f\"true: {label_encoder.decode([y_test[index]])[0]}\")\n",
    "print (f\"pred: {label_encoder.decode([y_pred[index]])[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted tags\n",
    "sorted_tags_by_f1 = OrderedDict(sorted(\n",
    "        performance['class'].items(), key=lambda tag: tag[1]['f1'], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='tag', index=3, options=('interpretability', 'segmentation', 'produ…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c03103d5d2640c98c856c3c48cb6cf6"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(tag=list(sorted_tags_by_f1.keys()))\n",
    "def display_tag_analysis(tag='transformers'):\n",
    "    # Performance\n",
    "    print (json.dumps(performance[\"class\"][tag], indent=2))\n",
    "\n",
    "    # TP, FP, FN samples\n",
    "    index = label_encoder.class_to_index[tag]\n",
    "    tp, fp, fn = [], [], []\n",
    "    for i in range(len(y_test)):\n",
    "        true = y_test[i][index]\n",
    "        pred = y_pred[i][index]\n",
    "        if true and pred:\n",
    "            tp.append(i)\n",
    "        elif not true and pred:\n",
    "            fp.append(i)\n",
    "        elif true and not pred:\n",
    "            fn.append(i)\n",
    "\n",
    "    # Samples\n",
    "    num_samples = 3\n",
    "    if len(tp):\n",
    "        print (\"\\n=== True positives ===\\n\")\n",
    "        for i in tp[:num_samples]:\n",
    "            print (f\"  {X_test[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")\n",
    "    if len(fp):\n",
    "        print (\"=== False positives ===\\n\")\n",
    "        for i in fp[:num_samples]:\n",
    "            print (f\"  {X_test[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")\n",
    "    if len(fn):\n",
    "        print (\"=== False negatives ===\\n\")\n",
    "        for i in fn[:num_samples]:\n",
    "            print (f\"  {X_test[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")"
   ]
  },
  {
   "source": [
    "#### Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "transfer learn transform self supervis learn\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['natural-language-processing',\n",
       " 'self-supervised-learning',\n",
       " 'transformers',\n",
       " 'transfer-learning']"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "# Infer\n",
    "text = \"Transfer learning with transformers for self-supervised learning\"\n",
    "print (preprocess(text, stem=True))\n",
    "get_classes(text=preprocess(text, stem=True), aliases=aliases, tags_dict=tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "transfer learn bert self supervis learn\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['self-supervised-learning', 'transfer-learning']"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "# Infer\n",
    "text = \"Transfer learning with BERT for self-supervised learning\"\n",
    "print (preprocess(text, stem=True))\n",
    "get_classes(text=preprocess(text, stem=True), aliases=aliases, tags_dict=tags_dict)"
   ]
  },
  {
   "source": [
    "Limitations : Failed to learned or generalize any imlpicit pattern to poredit the labels becaouse we treat the tokens as isolated entiries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Simple ML"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting seeds\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting data splits\n",
    "preprocessed_df=df.copy()\n",
    "preprocessed_df.text=preprocessed_df.text.apply(preprocess,lower=True,stem=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Albumentations Fast image augmentation library and easy to use wrapper around other libraries.\n(1000, 3605)\n  (0, 1861)\t0.35101282857580157\n  (0, 2276)\t0.3322638000685066\n  (0, 260)\t0.312389028946533\n  (0, 3573)\t0.3634469148077632\n  (0, 3414)\t0.22983052113836\n  (0, 3256)\t0.10614373238476807\n  (0, 1012)\t0.2636058411307363\n  (0, 192)\t0.10345779731276887\n  (0, 1862)\t0.2072370467198702\n  (0, 304)\t0.2659773634649721\n  (0, 1596)\t0.18753138760686572\n  (0, 1222)\t0.2861993155492261\n  (0, 153)\t0.4020707144370318\n"
     ]
    }
   ],
   "source": [
    "#Using Tf-idf \n",
    "vectorizer=TfidfVectorizer()\n",
    "print(X_train[0])\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "print (X_train.shape)\n",
    "print (X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(model):\n",
    "    \"\"\"Fit and evaluate each model.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    performance = get_performance(\n",
    "        y_true=y_test, y_pred=y_pred, classes=list(label_encoder.classes))\n",
    "    return performance['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"logistic-regression\": {\n    \"precision\": 0.3312317312317312,\n    \"recall\": 0.06427712804436943,\n    \"f1\": 0.10210451648045632,\n    \"num_samples\": 480.0\n  },\n  \"k-nearest-neighbors\": {\n    \"precision\": 0.6531082519964507,\n    \"recall\": 0.3227342860341629,\n    \"f1\": 0.41299426353846336,\n    \"num_samples\": 480.0\n  },\n  \"random-forest\": {\n    \"precision\": 0.5304325238867477,\n    \"recall\": 0.20684071484071484,\n    \"f1\": 0.2838581472925646,\n    \"num_samples\": 480.0\n  },\n  \"gradient-boosting-machine\": {\n    \"precision\": 0.7445866383444024,\n    \"recall\": 0.4956736941903197,\n    \"f1\": 0.5807018407182406,\n    \"num_samples\": 480.0\n  },\n  \"support-vector-machine\": {\n    \"precision\": 0.8026174159136747,\n    \"recall\": 0.3700476348364279,\n    \"f1\": 0.48829320873570664,\n    \"num_samples\": 480.0\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "performance = {}\n",
    "performance['logistic-regression'] = fit_and_evaluate(OneVsRestClassifier(\n",
    "    LogisticRegression(), n_jobs=1))\n",
    "performance['k-nearest-neighbors'] = fit_and_evaluate(\n",
    "    KNeighborsClassifier())\n",
    "performance['random-forest'] = fit_and_evaluate(\n",
    "    RandomForestClassifier(n_jobs=-1))\n",
    "performance['gradient-boosting-machine'] = fit_and_evaluate(OneVsRestClassifier(\n",
    "    GradientBoostingClassifier()))\n",
    "performance['support-vector-machine'] = fit_and_evaluate(OneVsRestClassifier(\n",
    "    LinearSVC(), n_jobs=-1))\n",
    "print (json.dumps(performance, indent=2))"
   ]
  },
  {
   "source": [
    "Limitations:\n",
    "TF-IDF representations don't encapsulate much signal beyond frequency but we require more fine-grained token representations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## CNN w/ Embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the seeds\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data splits\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)\n",
    "X_test_raw = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "cuda = True\n",
    "device = torch.device('cuda' if (\n",
    "    torch.cuda.is_available() and cuda) else 'cpu')\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "if device.type == 'cuda':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, char_level, num_tokens=None, \n",
    "                 pad_token='<PAD>', oov_token='<UNK>',\n",
    "                 token_to_index=None):\n",
    "        self.char_level = char_level\n",
    "        self.separator = '' if self.char_level else ' '\n",
    "        if num_tokens: num_tokens -= 2 # pad + unk tokens\n",
    "        self.num_tokens = num_tokens\n",
    "        self.oov_token = oov_token\n",
    "        if not token_to_index:\n",
    "            token_to_index = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Tokenizer(num_tokens={len(self)})>\"\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        if self.char_level:\n",
    "            all_tokens = [token for text in texts for token in text]\n",
    "        if not self.char_level:\n",
    "            all_tokens = [token for text in texts for token in text.split(' ')]\n",
    "        counts = Counter(all_tokens).most_common(self.num_tokens)\n",
    "        self.min_token_freq = counts[-1][1]\n",
    "        for token, count in counts:\n",
    "            index = len(self)\n",
    "            self.token_to_index[token] = index\n",
    "            self.index_to_token[index] = token\n",
    "        return self\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            if not self.char_level:\n",
    "                text = text.split(' ')\n",
    "            sequence = []\n",
    "            for token in text:\n",
    "                sequence.append(self.token_to_index.get(\n",
    "                    token, self.token_to_index[self.oov_token]))\n",
    "            sequences.append(np.asarray(sequence))\n",
    "        return sequences\n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = []\n",
    "            for index in sequence:\n",
    "                text.append(self.index_to_token.get(index, self.oov_token))\n",
    "            texts.append(self.separator.join([token for token in text]))\n",
    "        return texts\n",
    "\n",
    "    def save(self, fp):\n",
    "        with open(fp, 'w') as fp:\n",
    "            contents = {\n",
    "                'char_level': self.char_level,\n",
    "                'oov_token': self.oov_token,\n",
    "                'token_to_index': self.token_to_index\n",
    "            }\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        with open(fp, 'r') as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<Tokenizer(num_tokens=136)>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "char_level = True\n",
    "tokenizer = Tokenizer(char_level=char_level)\n",
    "tokenizer.fit_on_texts(texts=X_train)\n",
    "vocab_size = len(tokenizer)\n",
    "print (tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " ' ': 2,\n",
       " 'e': 3,\n",
       " 'n': 4,\n",
       " 'i': 5,\n",
       " 't': 6,\n",
       " 'a': 7,\n",
       " 'o': 8,\n",
       " 'r': 9,\n",
       " 's': 10,\n",
       " 'l': 11,\n",
       " 'c': 12,\n",
       " 'd': 13,\n",
       " 'g': 14,\n",
       " 'h': 15,\n",
       " 'm': 16,\n",
       " 'u': 17,\n",
       " 'p': 18,\n",
       " 'f': 19,\n",
       " 'y': 20,\n",
       " 'w': 21,\n",
       " 'b': 22,\n",
       " 'T': 23,\n",
       " 'v': 24,\n",
       " '.': 25,\n",
       " 'A': 26,\n",
       " '-': 27,\n",
       " 'L': 28,\n",
       " 'k': 29,\n",
       " 'P': 30,\n",
       " 'S': 31,\n",
       " 'N': 32,\n",
       " 'C': 33,\n",
       " 'M': 34,\n",
       " 'D': 35,\n",
       " 'I': 36,\n",
       " ',': 37,\n",
       " 'x': 38,\n",
       " 'R': 39,\n",
       " 'F': 40,\n",
       " 'G': 41,\n",
       " 'E': 42,\n",
       " 'B': 43,\n",
       " ':': 44,\n",
       " '2': 45,\n",
       " 'O': 46,\n",
       " 'z': 47,\n",
       " 'H': 48,\n",
       " 'W': 49,\n",
       " '0': 50,\n",
       " 'V': 51,\n",
       " '(': 52,\n",
       " ')': 53,\n",
       " 'j': 54,\n",
       " 'U': 55,\n",
       " 'K': 56,\n",
       " 'q': 57,\n",
       " '1': 58,\n",
       " '\"': 59,\n",
       " 'Q': 60,\n",
       " '\\r': 61,\n",
       " '\\n': 62,\n",
       " '/': 63,\n",
       " '&': 64,\n",
       " 'Y': 65,\n",
       " '3': 66,\n",
       " '+': 67,\n",
       " \"'\": 68,\n",
       " '?': 69,\n",
       " 'J': 70,\n",
       " '5': 71,\n",
       " '9': 72,\n",
       " 'X': 73,\n",
       " '8': 74,\n",
       " '6': 75,\n",
       " '4': 76,\n",
       " '’': 77,\n",
       " '!': 78,\n",
       " 'Z': 79,\n",
       " '—': 80,\n",
       " '7': 81,\n",
       " '“': 82,\n",
       " '”': 83,\n",
       " '|': 84,\n",
       " '🤗': 85,\n",
       " '️': 86,\n",
       " '%': 87,\n",
       " '=': 88,\n",
       " '–': 89,\n",
       " '💻': 90,\n",
       " '❤': 91,\n",
       " '_': 92,\n",
       " '🎨': 93,\n",
       " '>': 94,\n",
       " '🌊': 95,\n",
       " '^': 96,\n",
       " '🚗': 97,\n",
       " '🕶': 98,\n",
       " '<': 99,\n",
       " '\\u2060': 100,\n",
       " '⚡': 101,\n",
       " ';': 102,\n",
       " '@': 103,\n",
       " '#': 104,\n",
       " '🏥': 105,\n",
       " '💤': 106,\n",
       " '✨': 107,\n",
       " '😎': 108,\n",
       " '🔍': 109,\n",
       " '[': 110,\n",
       " ']': 111,\n",
       " '🦠': 112,\n",
       " '🍼': 113,\n",
       " '\\u2009': 114,\n",
       " '基': 115,\n",
       " '于': 116,\n",
       " '📈': 117,\n",
       " '⭐': 118,\n",
       " '🔥': 119,\n",
       " '*': 120,\n",
       " '…': 121,\n",
       " '🎐': 122,\n",
       " 'ç': 123,\n",
       " '🦄': 124,\n",
       " '✏': 125,\n",
       " '✂': 126,\n",
       " '🏡': 127,\n",
       " '𝘢': 128,\n",
       " '𝘯': 129,\n",
       " '𝘺': 130,\n",
       " '🐳': 131,\n",
       " '🤖': 132,\n",
       " '💬': 133,\n",
       " '🎥': 134,\n",
       " '👀': 135}"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "tokenizer.token_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text to indices:\n  (preprocessed) → Albumentations Fast image augmentation library and easy to use wrapper around other libraries.\n  (tokenized) → [26 11 22 17 16  3  4  6  7  6  5  8  4 10  2 40  7 10  6  2  5 16  7 14\n  3  2  7 17 14 16  3  4  6  7  6  5  8  4  2 11  5 22  9  7  9 20  2  7\n  4 13  2  3  7 10 20  2  6  8  2 17 10  3  2 21  9  7 18 18  3  9  2  7\n  9  8 17  4 13  2  8  6 15  3  9  2 11  5 22  9  7  9  5  3 10 25]\n"
     ]
    }
   ],
   "source": [
    "# Convert texts to sequences of indices\n",
    "# Convert texts to sequences of indices\n",
    "X_train = np.array(tokenizer.texts_to_sequences(X_train))\n",
    "X_val = np.array(tokenizer.texts_to_sequences(X_val))\n",
    "X_test = np.array(tokenizer.texts_to_sequences(X_test))\n",
    "preprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\n",
    "print (\"Text to indices:\\n\"\n",
    "    f\"  (preprocessed) → {preprocessed_text}\\n\"\n",
    "    f\"  (tokenized) → {X_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "class counts: [120  41 388 106  41  75  34  73  51  78  64  51  55  93  51 429  33  69\n  30  51 258  32  49  59  57  60  48  40 213  40  34  46 196  39  39],\nclass weights: {0: 0.008333333333333333, 1: 0.024390243902439025, 2: 0.002577319587628866, 3: 0.009433962264150943, 4: 0.024390243902439025, 5: 0.013333333333333334, 6: 0.029411764705882353, 7: 0.0136986301369863, 8: 0.0196078431372549, 9: 0.01282051282051282, 10: 0.015625, 11: 0.0196078431372549, 12: 0.01818181818181818, 13: 0.010752688172043012, 14: 0.0196078431372549, 15: 0.002331002331002331, 16: 0.030303030303030304, 17: 0.014492753623188406, 18: 0.03333333333333333, 19: 0.0196078431372549, 20: 0.003875968992248062, 21: 0.03125, 22: 0.02040816326530612, 23: 0.01694915254237288, 24: 0.017543859649122806, 25: 0.016666666666666666, 26: 0.020833333333333332, 27: 0.025, 28: 0.004694835680751174, 29: 0.025, 30: 0.029411764705882353, 31: 0.021739130434782608, 32: 0.00510204081632653, 33: 0.02564102564102564, 34: 0.02564102564102564}\n"
     ]
    }
   ],
   "source": [
    "# Class weights\n",
    "counts = np.bincount([label_encoder.class_to_index[class_] for class_ in all_tags])\n",
    "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
    "print (f\"class counts: {counts},\\nclass weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_seq_len=0):\n",
    "    \"\"\"Pad sequences to max length in sequence.\"\"\"\n",
    "    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n",
    "    padded_sequences = np.zeros((len(sequences), max_seq_len))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        padded_sequences[i][:len(sequence)] = sequence\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, max_filter_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.max_filter_size = max_filter_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Dataset(N={len(self)})>\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        return [X, y]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Processing on a batch.\"\"\"\n",
    "        # Get inputs\n",
    "        batch = np.array(batch, dtype=object)\n",
    "        X = batch[:, 0]\n",
    "        y = np.stack(batch[:, 1], axis=0)\n",
    "\n",
    "        # Pad inputs\n",
    "        X = pad_sequences(sequences=X, max_seq_len=self.max_filter_size)\n",
    "\n",
    "        # Cast\n",
    "        X = torch.LongTensor(X.astype(np.int32))\n",
    "        y = torch.FloatTensor(y.astype(np.int32))\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data splits:\n  Train dataset:<Dataset(N=1000)>\n  Val dataset: <Dataset(N=227)>\n  Test dataset: <Dataset(N=217)>\nSample point:\n  X: [26 11 22 17 16  3  4  6  7  6  5  8  4 10  2 40  7 10  6  2  5 16  7 14\n  3  2  7 17 14 16  3  4  6  7  6  5  8  4  2 11  5 22  9  7  9 20  2  7\n  4 13  2  3  7 10 20  2  6  8  2 17 10  3  2 21  9  7 18 18  3  9  2  7\n  9  8 17  4 13  2  8  6 15  3  9  2 11  5 22  9  7  9  5  3 10 25]\n  y: [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "filter_sizes = list(range(1, 11))\n",
    "train_dataset = CNNTextDataset(\n",
    "    X=X_train, y=y_train, max_filter_size=max(filter_sizes))\n",
    "val_dataset = CNNTextDataset(\n",
    "    X=X_val, y=y_val, max_filter_size=max(filter_sizes))\n",
    "test_dataset = CNNTextDataset(\n",
    "    X=X_test, y=y_test, max_filter_size=max(filter_sizes))\n",
    "print (\"Data splits:\\n\"\n",
    "    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n",
    "    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n",
    "    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  X: {train_dataset[0][0]}\\n\"\n",
    "    f\"  y: {train_dataset[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample batch:\n  X: [128, 226]\n  y: [128, 35]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create dataloaders\n",
    "batch_size = 128\n",
    "train_dataloader = train_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "val_dataloader = val_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "test_dataloader = test_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print (\"Sample batch:\\n\"\n",
    "    f\"  X: {list(batch_X.size())}\\n\"\n",
    "    f\"  y: {list(batch_y.size())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "embedding_dim = 128\n",
    "num_filters = 128\n",
    "hidden_dim = 128\n",
    "dropout_p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, num_filters, filter_sizes,\n",
    "                 hidden_dim, dropout_p, num_classes, padding_idx=0):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Initialize embeddings\n",
    "        self.embeddings = nn.Embedding(\n",
    "                embedding_dim=embedding_dim, num_embeddings=vocab_size,\n",
    "                padding_idx=padding_idx)\n",
    "\n",
    "        # Conv weights\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.conv = nn.ModuleList(\n",
    "            [nn.Conv1d(in_channels=embedding_dim,\n",
    "                       out_channels=num_filters,\n",
    "                       kernel_size=f) for f in filter_sizes])\n",
    "\n",
    "        # FC weights\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(num_filters*len(filter_sizes), hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs, channel_first=False):\n",
    "\n",
    "        # Embed\n",
    "        x_in, = inputs\n",
    "        x_in = self.embeddings(x_in)\n",
    "        if not channel_first:\n",
    "            x_in = x_in.transpose(1, 2)  # (N, channels, sequence length)\n",
    "\n",
    "        z = []\n",
    "        max_seq_len = x_in.shape[2]\n",
    "        for i, f in enumerate(self.filter_sizes):\n",
    "\n",
    "            # `SAME` padding\n",
    "            padding_left = int(\n",
    "                (self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2)\n",
    "            padding_right = int(math.ceil(\n",
    "                (self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2))\n",
    "\n",
    "            # Conv\n",
    "            _z = self.conv[i](F.pad(x_in, (padding_left, padding_right)))\n",
    "\n",
    "            # Pool\n",
    "            _z = F.max_pool1d(_z, _z.size(2)).squeeze(2)\n",
    "            z.append(_z)\n",
    "\n",
    "        # Concat outputs\n",
    "        z = torch.cat(z, 1)\n",
    "\n",
    "        # FC\n",
    "        z = self.fc1(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc2(z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method Module.named_parameters of CNN(\n  (embeddings): Embedding(136, 128, padding_idx=0)\n  (conv): ModuleList(\n    (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n    (1): Conv1d(128, 128, kernel_size=(2,), stride=(1,))\n    (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n    (3): Conv1d(128, 128, kernel_size=(4,), stride=(1,))\n    (4): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n    (5): Conv1d(128, 128, kernel_size=(6,), stride=(1,))\n    (6): Conv1d(128, 128, kernel_size=(7,), stride=(1,))\n    (7): Conv1d(128, 128, kernel_size=(8,), stride=(1,))\n    (8): Conv1d(128, 128, kernel_size=(9,), stride=(1,))\n    (9): Conv1d(128, 128, kernel_size=(10,), stride=(1,))\n  )\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=1280, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=35, bias=True)\n)>\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = CNN(\n",
    "    embedding_dim=embedding_dim, vocab_size=vocab_size,\n",
    "    num_filters=num_filters, filter_sizes=filter_sizes,\n",
    "    hidden_dim=hidden_dim, dropout_p=dropout_p,\n",
    "    num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "print (model.named_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "lr = 2e-4\n",
    "num_epochs = 200\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "class_weights_tensor = torch.Tensor(np.array(list(class_weights.values())))\n",
    "loss_fn = nn.BCEWithLogitsLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer & scheduler\n",
    "# Define optimizer & scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer module\n",
    "trainer = Trainer(\n",
    "    model=model, device=device, loss_fn=loss_fn, \n",
    "    optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 | train_loss: 0.00813, val_loss: 0.00326, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.00440, val_loss: 0.00280, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 3 | train_loss: 0.00414, val_loss: 0.00296, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 4 | train_loss: 0.00397, val_loss: 0.00283, lr: 2.00E-04, _patience: 8\n",
      "Epoch: 5 | train_loss: 0.00370, val_loss: 0.00266, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 6 | train_loss: 0.00347, val_loss: 0.00260, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 7 | train_loss: 0.00336, val_loss: 0.00258, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 8 | train_loss: 0.00324, val_loss: 0.00256, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 9 | train_loss: 0.00317, val_loss: 0.00254, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 10 | train_loss: 0.00300, val_loss: 0.00251, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 11 | train_loss: 0.00301, val_loss: 0.00248, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 12 | train_loss: 0.00292, val_loss: 0.00245, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 13 | train_loss: 0.00286, val_loss: 0.00242, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 14 | train_loss: 0.00280, val_loss: 0.00239, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 15 | train_loss: 0.00267, val_loss: 0.00235, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 16 | train_loss: 0.00266, val_loss: 0.00231, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 17 | train_loss: 0.00259, val_loss: 0.00227, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 18 | train_loss: 0.00250, val_loss: 0.00223, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 19 | train_loss: 0.00240, val_loss: 0.00219, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 20 | train_loss: 0.00241, val_loss: 0.00215, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 21 | train_loss: 0.00228, val_loss: 0.00212, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 22 | train_loss: 0.00222, val_loss: 0.00207, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 23 | train_loss: 0.00215, val_loss: 0.00204, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 24 | train_loss: 0.00210, val_loss: 0.00200, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 25 | train_loss: 0.00203, val_loss: 0.00196, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 26 | train_loss: 0.00198, val_loss: 0.00193, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 27 | train_loss: 0.00188, val_loss: 0.00190, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 28 | train_loss: 0.00186, val_loss: 0.00187, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 29 | train_loss: 0.00180, val_loss: 0.00184, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 30 | train_loss: 0.00179, val_loss: 0.00182, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 31 | train_loss: 0.00170, val_loss: 0.00179, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 32 | train_loss: 0.00167, val_loss: 0.00177, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 33 | train_loss: 0.00162, val_loss: 0.00175, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 34 | train_loss: 0.00157, val_loss: 0.00172, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 35 | train_loss: 0.00153, val_loss: 0.00172, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 36 | train_loss: 0.00148, val_loss: 0.00169, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 37 | train_loss: 0.00142, val_loss: 0.00167, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 38 | train_loss: 0.00139, val_loss: 0.00165, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 39 | train_loss: 0.00136, val_loss: 0.00164, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 40 | train_loss: 0.00134, val_loss: 0.00164, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 41 | train_loss: 0.00131, val_loss: 0.00162, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 42 | train_loss: 0.00125, val_loss: 0.00159, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 43 | train_loss: 0.00127, val_loss: 0.00161, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 44 | train_loss: 0.00122, val_loss: 0.00158, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 45 | train_loss: 0.00117, val_loss: 0.00158, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 46 | train_loss: 0.00113, val_loss: 0.00160, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 47 | train_loss: 0.00112, val_loss: 0.00156, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 48 | train_loss: 0.00109, val_loss: 0.00156, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 49 | train_loss: 0.00106, val_loss: 0.00156, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 50 | train_loss: 0.00106, val_loss: 0.00156, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 51 | train_loss: 0.00100, val_loss: 0.00156, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 52 | train_loss: 0.00097, val_loss: 0.00155, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 53 | train_loss: 0.00096, val_loss: 0.00154, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 54 | train_loss: 0.00092, val_loss: 0.00151, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 55 | train_loss: 0.00089, val_loss: 0.00154, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 56 | train_loss: 0.00087, val_loss: 0.00155, lr: 2.00E-04, _patience: 8\n",
      "Epoch: 57 | train_loss: 0.00087, val_loss: 0.00150, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 58 | train_loss: 0.00083, val_loss: 0.00151, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 59 | train_loss: 0.00083, val_loss: 0.00152, lr: 2.00E-04, _patience: 8\n",
      "Epoch: 60 | train_loss: 0.00081, val_loss: 0.00154, lr: 2.00E-04, _patience: 7\n",
      "Epoch: 61 | train_loss: 0.00077, val_loss: 0.00154, lr: 2.00E-04, _patience: 6\n",
      "Epoch: 62 | train_loss: 0.00076, val_loss: 0.00150, lr: 2.00E-04, _patience: 5\n",
      "Epoch: 63 | train_loss: 0.00076, val_loss: 0.00150, lr: 2.00E-05, _patience: 4\n",
      "Epoch: 64 | train_loss: 0.00073, val_loss: 0.00150, lr: 2.00E-05, _patience: 3\n",
      "Epoch: 65 | train_loss: 0.00070, val_loss: 0.00153, lr: 2.00E-05, _patience: 2\n",
      "Epoch: 66 | train_loss: 0.00072, val_loss: 0.00154, lr: 2.00E-05, _patience: 1\n",
      "Stopping early!\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "best_model = trainer.train(\n",
    "    num_epochs, patience, train_dataloader, val_dataloader)"
   ]
  },
  {
   "source": [
    "### Evaluation "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2938502ff08>"
      ]
     },
     "metadata": {},
     "execution_count": 124
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 385.78125 262.19625\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 385.78125 262.19625 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 378.58125 224.64 \r\nL 378.58125 7.2 \r\nL 43.78125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m3ce24b8c11\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.562349\" xlink:href=\"#m3ce24b8c11\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(44.610787 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"114.744323\" xlink:href=\"#m3ce24b8c11\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0.2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(106.792761 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"176.926298\" xlink:href=\"#m3ce24b8c11\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 0.4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(168.974735 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.108272\" xlink:href=\"#m3ce24b8c11\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0.6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(231.156709 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"301.290246\" xlink:href=\"#m3ce24b8c11\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0.8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(293.338683 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.47222\" xlink:href=\"#m3ce24b8c11\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1.0 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(355.520657 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Threshold -->\r\n     <defs>\r\n      <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n     </defs>\r\n     <g transform=\"translate(186.432031 252.916563)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-84\"/>\r\n      <use x=\"61.083984\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"124.462891\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"165.544922\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"227.068359\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"279.167969\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"342.546875\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"403.728516\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"431.511719\" xlink:href=\"#DejaVuSans-100\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m69017058a6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m69017058a6\" y=\"214.846174\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(20.878125 218.645393)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m69017058a6\" y=\"175.293667\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(20.878125 179.092885)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m69017058a6\" y=\"135.741159\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(20.878125 139.540378)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m69017058a6\" y=\"96.188651\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(20.878125 99.98787)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m69017058a6\" y=\"56.636144\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(20.878125 60.435363)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m69017058a6\" y=\"17.083636\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 20.882855)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- Performance -->\r\n     <defs>\r\n      <path d=\"M 19.671875 64.796875 \r\nL 19.671875 37.40625 \r\nL 32.078125 37.40625 \r\nQ 38.96875 37.40625 42.71875 40.96875 \r\nQ 46.484375 44.53125 46.484375 51.125 \r\nQ 46.484375 57.671875 42.71875 61.234375 \r\nQ 38.96875 64.796875 32.078125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.34375 72.90625 50.609375 67.359375 \r\nQ 56.890625 61.8125 56.890625 51.125 \r\nQ 56.890625 40.328125 50.609375 34.8125 \r\nQ 44.34375 29.296875 32.078125 29.296875 \r\nL 19.671875 29.296875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-80\"/>\r\n      <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n      <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 147.867656)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"60.255859\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"121.779297\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"162.892578\" xlink:href=\"#DejaVuSans-102\"/>\r\n      <use x=\"198.097656\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"259.279297\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"300.376953\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"397.789062\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"459.068359\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"522.447266\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"577.427734\" xlink:href=\"#DejaVuSans-101\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#pbfeca32b17)\" d=\"M 58.999432 159.215987 \r\nL 59.000231 159.234146 \r\nL 59.090207 158.768443 \r\nL 59.627189 156.545163 \r\nL 59.702089 156.333559 \r\nL 59.837563 155.817784 \r\nL 60.347885 153.651833 \r\nL 60.395197 153.453319 \r\nL 60.70364 152.27065 \r\nL 60.819475 151.899631 \r\nL 60.943239 151.394958 \r\nL 61.223807 150.389585 \r\nL 61.288077 150.236504 \r\nL 61.401847 149.782595 \r\nL 62.054258 147.204318 \r\nL 66.24488 133.320638 \r\nL 66.338746 133.075033 \r\nL 66.485713 132.594937 \r\nL 66.765959 131.830536 \r\nL 66.853529 131.607675 \r\nL 67.352732 129.786184 \r\nL 67.38899 129.808283 \r\nL 67.533843 129.507086 \r\nL 67.741372 128.761767 \r\nL 67.821174 128.556233 \r\nL 67.963281 128.147078 \r\nL 68.152751 127.60619 \r\nL 68.175686 127.610877 \r\nL 68.2541 127.452523 \r\nL 69.741243 123.313481 \r\nL 69.826561 123.099802 \r\nL 69.910443 122.728359 \r\nL 69.962721 122.590752 \r\nL 70.19865 121.896506 \r\nL 70.280493 121.738838 \r\nL 70.804233 120.343252 \r\nL 70.977597 119.78061 \r\nL 70.984844 119.803261 \r\nL 71.238646 119.21267 \r\nL 71.434037 118.528657 \r\nL 71.449299 118.551337 \r\nL 71.472016 118.421995 \r\nL 71.552227 118.183964 \r\nL 71.847624 117.175696 \r\nL 71.99347 116.865053 \r\nL 72.1797 116.349481 \r\nL 72.26208 116.123623 \r\nL 72.539497 115.140513 \r\nL 72.902247 114.0197 \r\nL 73.026806 113.759289 \r\nL 73.361018 112.773673 \r\nL 73.556617 112.262458 \r\nL 73.815884 111.795499 \r\nL 73.929048 111.57282 \r\nL 74.087851 110.946764 \r\nL 74.282259 110.692516 \r\nL 74.305283 110.715209 \r\nL 74.338142 110.614135 \r\nL 74.437886 110.360591 \r\nL 74.54673 110.207872 \r\nL 74.694962 109.872465 \r\nL 74.996132 108.914156 \r\nL 75.055827 108.730883 \r\nL 75.170976 108.441597 \r\nL 75.363258 107.987432 \r\nL 75.490633 107.52895 \r\nL 76.849619 104.336325 \r\nL 76.958907 104.072498 \r\nL 77.559312 102.692426 \r\nL 77.70985 102.367428 \r\nL 77.822573 101.951061 \r\nL 77.901585 101.771672 \r\nL 78.04297 101.41118 \r\nL 78.260423 100.644527 \r\nL 78.560998 99.928665 \r\nL 78.66181 99.680463 \r\nL 78.934207 99.118069 \r\nL 79.480831 97.719926 \r\nL 79.620296 97.526649 \r\nL 79.7268 97.170645 \r\nL 79.924519 96.823945 \r\nL 80.197507 95.956834 \r\nL 80.699935 95.388938 \r\nL 80.850489 95.006981 \r\nL 81.137449 94.622215 \r\nL 81.396038 93.947493 \r\nL 82.27374 91.911155 \r\nL 82.353396 91.732678 \r\nL 82.439964 91.50359 \r\nL 82.671846 90.839337 \r\nL 82.827295 90.809881 \r\nL 82.939555 90.481839 \r\nL 83.556009 89.21087 \r\nL 83.630311 89.061438 \r\nL 83.737489 88.64866 \r\nL 84.278589 87.534305 \r\nL 84.486871 87.28635 \r\nL 84.685685 86.861022 \r\nL 84.952523 86.453945 \r\nL 85.020242 86.219153 \r\nL 85.54691 84.753693 \r\nL 86.090921 84.251744 \r\nL 86.217604 84.029052 \r\nL 86.254903 84.049869 \r\nL 86.291974 83.968477 \r\nL 86.415765 83.662662 \r\nL 86.607082 83.437358 \r\nL 86.723141 83.396623 \r\nL 87.052722 82.796858 \r\nL 87.23755 82.630286 \r\nL 87.407008 82.420928 \r\nL 87.57203 82.147825 \r\nL 87.699487 81.979372 \r\nL 87.794473 81.915574 \r\nL 87.914306 81.619326 \r\nL 88.15408 81.279136 \r\nL 88.262261 81.022851 \r\nL 88.413758 80.722607 \r\nL 88.571115 80.441365 \r\nL 88.58957 80.46173 \r\nL 88.603533 80.41852 \r\nL 88.814744 80.158673 \r\nL 89.346915 79.409967 \r\nL 89.498771 79.145701 \r\nL 89.839425 78.654266 \r\nL 89.967907 78.33752 \r\nL 90.202465 77.933648 \r\nL 90.347324 77.592583 \r\nL 90.62829 77.18409 \r\nL 90.854265 76.904344 \r\nL 90.857814 76.924192 \r\nL 90.864928 76.944053 \r\nL 90.887819 76.898268 \r\nL 90.989672 76.714827 \r\nL 91.165493 76.392628 \r\nL 91.384192 76.061966 \r\nL 91.599058 75.802674 \r\nL 91.754122 75.616038 \r\nL 91.900114 75.119745 \r\nL 92.495824 74.058082 \r\nL 92.625124 73.770172 \r\nL 92.759446 73.548623 \r\nL 93.051167 72.937641 \r\nL 93.183423 72.927266 \r\nL 93.187333 72.878497 \r\nL 93.784025 72.032881 \r\nL 93.962207 71.804339 \r\nL 94.047499 71.456305 \r\nL 94.163397 71.256667 \r\nL 94.264098 71.15664 \r\nL 94.471634 70.874494 \r\nL 94.64447 70.723528 \r\nL 94.946781 70.065665 \r\nL 94.972374 70.084274 \r\nL 95.005828 69.982508 \r\nL 95.007959 69.931572 \r\nL 95.0147 69.950161 \r\nL 95.187258 69.885328 \r\nL 96.060197 68.841801 \r\nL 96.839583 68.339738 \r\nL 97.119689 67.781086 \r\nL 97.228202 67.746558 \r\nL 97.302921 67.535436 \r\nL 97.344173 67.571667 \r\nL 97.561401 67.555024 \r\nL 97.867924 67.166357 \r\nL 97.995909 66.952947 \r\nL 98.008992 66.97097 \r\nL 98.207077 66.935531 \r\nL 98.357684 66.685549 \r\nL 98.460646 66.524271 \r\nL 98.728174 66.308687 \r\nL 98.974348 65.730334 \r\nL 99.27793 65.329003 \r\nL 99.384519 65.199806 \r\nL 99.859704 64.814403 \r\nL 99.884671 64.831971 \r\nL 99.919686 64.776737 \r\nL 100.058854 64.610792 \r\nL 100.155492 64.572907 \r\nL 100.319317 63.995432 \r\nL 100.729988 63.620323 \r\nL 100.893942 63.615998 \r\nL 101.177218 63.220847 \r\nL 101.183315 63.164228 \r\nL 101.186729 63.181441 \r\nL 101.681006 62.931955 \r\nL 102.148439 62.239664 \r\nL 102.328836 62.199147 \r\nL 102.39447 62.026274 \r\nL 103.964423 60.745495 \r\nL 104.004882 60.644333 \r\nL 104.234684 60.57624 \r\nL 104.240999 60.517111 \r\nL 104.397225 60.339449 \r\nL 104.457856 60.161377 \r\nL 104.705852 59.982895 \r\nL 104.711129 59.923309 \r\nL 104.73238 59.939798 \r\nL 104.975376 59.896636 \r\nL 105.197363 59.673947 \r\nL 105.325505 59.690392 \r\nL 105.328882 59.630463 \r\nL 106.10629 58.928374 \r\nL 106.633808 58.656962 \r\nL 108.465122 56.447948 \r\nL 108.585786 56.259304 \r\nL 108.597044 56.274862 \r\nL 108.802158 56.022561 \r\nL 108.89849 55.784874 \r\nL 109.139295 55.46726 \r\nL 109.144817 55.482595 \r\nL 109.289156 55.306648 \r\nL 109.49697 55.002005 \r\nL 109.698604 54.824579 \r\nL 109.771637 54.726164 \r\nL 109.816604 54.741318 \r\nL 110.047088 54.722313 \r\nL 110.297798 54.39883 \r\nL 110.389835 54.008922 \r\nL 110.427881 54.023883 \r\nL 110.93071 53.762808 \r\nL 111.360751 53.282323 \r\nL 111.580204 53.150488 \r\nL 112.030681 52.56909 \r\nL 112.350857 52.383884 \r\nL 112.376528 52.317192 \r\nL 112.699808 52.279353 \r\nL 113.417086 51.434449 \r\nL 113.815322 51.1778 \r\nL 114.386946 50.456065 \r\nL 115.394494 49.809318 \r\nL 115.565551 49.767546 \r\nL 115.813806 49.711964 \r\nL 115.817897 49.642609 \r\nL 115.906557 49.65629 \r\nL 116.175776 49.405399 \r\nL 116.317747 49.349278 \r\nL 117.599925 48.105211 \r\nL 117.799743 48.060526 \r\nL 117.848662 47.98931 \r\nL 118.01649 47.931212 \r\nL 118.056045 47.716943 \r\nL 118.130075 47.730057 \r\nL 118.318408 47.684698 \r\nL 118.410677 47.397602 \r\nL 118.728236 47.037343 \r\nL 118.877281 46.820442 \r\nL 118.894567 46.833259 \r\nL 119.924562 46.242033 \r\nL 120.253851 46.232115 \r\nL 120.605883 45.950935 \r\nL 121.860751 45.471432 \r\nL 121.934516 45.508751 \r\nL 122.656885 45.385036 \r\nL 122.99222 45.347851 \r\nL 123.570316 44.83672 \r\nL 123.595162 44.686535 \r\nL 124.582599 44.067236 \r\nL 124.774764 43.710603 \r\nL 124.779373 43.72248 \r\nL 125.214349 43.657977 \r\nL 125.797577 43.36349 \r\nL 126.039573 43.144679 \r\nL 126.175035 42.925082 \r\nL 126.934051 42.561152 \r\nL 127.235765 42.261531 \r\nL 127.248339 42.272903 \r\nL 127.515602 42.050038 \r\nL 127.652385 41.916013 \r\nL 128.193801 41.488758 \r\nL 128.537518 41.273668 \r\nL 128.638066 41.137059 \r\nL 128.969922 40.749855 \r\nL 129.861457 40.483716 \r\nL 129.93046 40.414148 \r\nL 130.247996 40.172664 \r\nL 130.271716 40.011302 \r\nL 130.305884 40.032502 \r\nL 130.499558 39.972819 \r\nL 130.86227 39.577518 \r\nL 131.106684 39.445803 \r\nL 132.000235 39.252253 \r\nL 132.938373 39.211561 \r\nL 133.197195 39.170677 \r\nL 133.419826 39.119236 \r\nL 133.668363 38.829116 \r\nL 133.849788 38.756243 \r\nL 134.406572 38.275829 \r\nL 135.426797 38.242253 \r\nL 136.000194 37.989962 \r\nL 136.172567 37.831256 \r\nL 136.369438 37.577653 \r\nL 136.504738 37.323322 \r\nL 138.605293 36.39153 \r\nL 139.22641 36.342488 \r\nL 139.375952 36.26526 \r\nL 139.532655 36.187882 \r\nL 140.007261 35.596755 \r\nL 140.334984 35.421793 \r\nL 140.478595 35.343099 \r\nL 141.040995 35.31775 \r\nL 141.061111 35.229659 \r\nL 141.516341 35.168116 \r\nL 142.563177 34.831556 \r\nL 143.427985 34.573505 \r\nL 143.539796 34.395194 \r\nL 143.876192 34.322966 \r\nL 144.198671 34.152427 \r\nL 144.199746 34.160898 \r\nL 144.375259 34.096525 \r\nL 144.482743 34.0066 \r\nL 145.250659 33.933359 \r\nL 145.384642 33.851532 \r\nL 146.015544 33.687388 \r\nL 146.876498 33.629901 \r\nL 146.991894 33.539066 \r\nL 147.538292 33.472797 \r\nL 147.565403 33.381655 \r\nL 148.196981 33.314902 \r\nL 149.122796 33.264208 \r\nL 149.568566 33.080606 \r\nL 149.678255 32.996715 \r\nL 150.4882 32.736075 \r\nL 150.603421 32.551097 \r\nL 150.896268 32.474109 \r\nL 151.132991 32.210807 \r\nL 151.565168 32.031979 \r\nL 151.940342 31.961371 \r\nL 152.214305 31.875302 \r\nL 152.504965 31.695132 \r\nL 152.505187 31.601112 \r\nL 153.517573 31.551581 \r\nL 153.531796 31.457099 \r\nL 156.761783 31.40082 \r\nL 157.425606 31.135729 \r\nL 157.741506 31.046995 \r\nL 158.922826 30.489505 \r\nL 160.490893 30.427656 \r\nL 160.55467 30.239961 \r\nL 161.038531 30.044817 \r\nL 161.177778 29.856018 \r\nL 161.70834 29.784946 \r\nL 162.655726 29.417441 \r\nL 162.663815 29.318545 \r\nL 162.671357 29.325074 \r\nL 163.378794 29.265046 \r\nL 164.108414 29.178556 \r\nL 164.261384 29.098242 \r\nL 165.120281 28.823452 \r\nL 165.52631 28.729296 \r\nL 165.648248 28.628693 \r\nL 167.313245 28.565238 \r\nL 167.620917 28.375135 \r\nL 168.680057 28.328745 \r\nL 168.791609 28.232811 \r\nL 170.981339 28.203691 \r\nL 171.075535 28.100744 \r\nL 171.730574 28.112911 \r\nL 172.052618 27.906462 \r\nL 172.527696 27.808998 \r\nL 172.804754 27.601755 \r\nL 173.102511 27.503737 \r\nL 175.579807 27.468932 \r\nL 175.988013 27.364193 \r\nL 179.789369 27.333864 \r\nL 180.011999 27.128009 \r\nL 180.415628 27.044511 \r\nL 180.553189 26.831705 \r\nL 181.10155 26.736064 \r\nL 181.272329 26.629244 \r\nL 182.553896 26.570817 \r\nL 182.92679 26.366137 \r\nL 183.07484 26.376776 \r\nL 184.479438 26.093579 \r\nL 184.504262 25.984662 \r\nL 185.112638 25.885784 \r\nL 185.768307 25.786562 \r\nL 185.859918 25.682008 \r\nL 186.250584 25.582143 \r\nL 187.287107 25.366884 \r\nL 187.434184 25.261353 \r\nL 190.096092 25.347992 \r\nL 190.118997 25.236391 \r\nL 190.416866 25.017473 \r\nL 194.014456 24.989418 \r\nL 194.047341 24.876071 \r\nL 194.643051 24.890052 \r\nL 194.920285 24.785593 \r\nL 195.867438 24.708216 \r\nL 197.126054 24.607211 \r\nL 199.566851 24.564587 \r\nL 200.386498 24.246307 \r\nL 200.701193 24.250706 \r\nL 204.739903 23.724916 \r\nL 204.853919 23.605985 \r\nL 206.045969 23.638617 \r\nL 206.051565 23.518963 \r\nL 206.705437 23.535112 \r\nL 207.973188 23.193576 \r\nL 208.450462 23.205155 \r\nL 208.868963 23.084017 \r\nL 209.417037 23.091612 \r\nL 210.149279 22.973902 \r\nL 210.772944 22.985122 \r\nL 210.911672 22.863228 \r\nL 212.603447 22.78467 \r\nL 213.011237 22.788325 \r\nL 213.166273 22.545286 \r\nL 215.503191 22.580591 \r\nL 215.779665 22.459705 \r\nL 220.870575 22.40402 \r\nL 221.013083 22.277662 \r\nL 223.572585 22.332808 \r\nL 223.76487 22.076976 \r\nL 223.845649 21.948805 \r\nL 224.459844 21.955283 \r\nL 224.497668 21.826775 \r\nL 225.19444 21.848999 \r\nL 226.010667 21.469499 \r\nL 226.264718 21.475402 \r\nL 226.272909 21.345186 \r\nL 226.354022 21.350933 \r\nL 227.283627 21.371171 \r\nL 227.398838 21.242821 \r\nL 228.627006 21.268373 \r\nL 228.629452 21.136147 \r\nL 230.108206 21.166827 \r\nL 230.205738 21.036161 \r\nL 230.208425 21.038887 \r\nL 230.933032 21.047087 \r\nL 230.934218 20.913063 \r\nL 231.02834 20.918367 \r\nL 235.833547 20.835982 \r\nL 235.938826 20.702092 \r\nL 236.381158 20.570282 \r\nL 237.374882 20.595045 \r\nL 237.375438 20.456985 \r\nL 237.44054 20.459384 \r\nL 241.200597 20.513045 \r\nL 241.404446 20.236372 \r\nL 250.941901 20.237025 \r\nL 251.119045 20.089146 \r\nL 251.427116 20.093721 \r\nL 251.658446 19.947576 \r\nL 253.249446 19.985172 \r\nL 259.17934 19.929141 \r\nL 259.410875 19.773207 \r\nL 260.379377 19.625163 \r\nL 261.332721 19.645742 \r\nL 261.588124 19.489507 \r\nL 267.964632 19.555668 \r\nL 267.990298 19.39279 \r\nL 272.907214 19.445987 \r\nL 273.332238 19.280998 \r\nL 277.325406 19.328975 \r\nL 277.342214 19.158069 \r\nL 278.267185 19.167175 \r\nL 278.340589 18.995225 \r\nL 281.876859 19.03992 \r\nL 282.065733 18.868497 \r\nL 282.204702 18.870109 \r\nL 283.186473 18.700224 \r\nL 283.504996 18.704641 \r\nL 283.63305 18.525843 \r\nL 284.126863 18.531121 \r\nL 284.214536 18.351345 \r\nL 284.974038 18.358349 \r\nL 285.05467 18.178263 \r\nL 285.86354 18.185378 \r\nL 286.242161 18.003462 \r\nL 290.794114 18.033504 \r\nL 290.980895 17.846462 \r\nL 310.489668 18.01979 \r\nL 315.750328 18.081178 \r\nL 316.282446 18.087507 \r\nL 316.888525 17.841347 \r\nL 319.033955 17.862229 \r\nL 319.215677 17.605437 \r\nL 323.811698 17.64387 \r\nL 323.866533 17.364151 \r\nL 330.963515 17.413241 \r\nL 331.12033 17.083636 \r\nL 363.363068 17.083636 \r\nL 363.363068 17.083636 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#pbfeca32b17)\" d=\"M 58.999432 17.083636 \r\nL 59.111319 17.173447 \r\nL 62.573511 17.263257 \r\nL 62.676801 17.353068 \r\nL 64.758653 17.442878 \r\nL 64.869875 17.532688 \r\nL 66.58618 17.622499 \r\nL 66.70118 17.712309 \r\nL 67.352732 17.80212 \r\nL 67.49083 17.89193 \r\nL 68.152751 17.981741 \r\nL 68.275328 18.071551 \r\nL 68.537136 18.161361 \r\nL 68.624292 18.340982 \r\nL 69.566983 18.430793 \r\nL 69.654587 18.520603 \r\nL 70.294999 18.610413 \r\nL 70.469301 18.879845 \r\nL 70.709375 18.969655 \r\nL 70.80704 19.059466 \r\nL 71.434037 19.149276 \r\nL 71.556397 19.239086 \r\nL 73.216237 19.328897 \r\nL 73.310987 19.418707 \r\nL 73.990485 19.508518 \r\nL 74.098763 19.598328 \r\nL 74.282259 19.688138 \r\nL 74.364554 19.777949 \r\nL 75.345628 19.867759 \r\nL 75.468249 20.04738 \r\nL 75.646017 20.137191 \r\nL 75.753014 20.227001 \r\nL 76.062148 20.316811 \r\nL 76.191744 20.496432 \r\nL 76.775416 20.586243 \r\nL 76.861386 20.676053 \r\nL 77.305669 20.765863 \r\nL 77.398782 20.855674 \r\nL 78.131492 20.945484 \r\nL 78.244567 21.035295 \r\nL 79.777503 21.125105 \r\nL 79.885297 21.214916 \r\nL 79.950025 21.304726 \r\nL 80.050344 21.394536 \r\nL 80.37494 21.484347 \r\nL 80.488262 21.574157 \r\nL 80.659145 21.663968 \r\nL 81.102496 21.753778 \r\nL 81.244861 22.023209 \r\nL 82.111238 22.11302 \r\nL 82.266035 22.292641 \r\nL 82.546012 22.382451 \r\nL 82.671846 22.472261 \r\nL 82.881268 22.651882 \r\nL 83.753149 22.741693 \r\nL 83.866574 22.831503 \r\nL 84.169715 22.921313 \r\nL 84.278589 23.011124 \r\nL 84.726272 23.100934 \r\nL 84.845442 23.190745 \r\nL 85.344961 23.280555 \r\nL 85.450057 23.370366 \r\nL 85.868687 23.460176 \r\nL 86.090921 23.639797 \r\nL 86.217604 23.729607 \r\nL 86.40589 23.909228 \r\nL 86.607082 23.999038 \r\nL 87.052722 24.448091 \r\nL 87.327688 24.537901 \r\nL 87.407008 24.627711 \r\nL 87.699487 24.717522 \r\nL 87.838618 24.807332 \r\nL 88.571115 24.897143 \r\nL 88.668413 24.986953 \r\nL 88.881321 25.076763 \r\nL 88.977503 25.166574 \r\nL 89.20418 25.256384 \r\nL 89.311355 25.346195 \r\nL 89.721468 25.436005 \r\nL 89.843824 25.525816 \r\nL 90.033593 25.705436 \r\nL 90.662238 25.795247 \r\nL 90.743261 25.974868 \r\nL 90.857814 26.064678 \r\nL 90.967955 26.154488 \r\nL 91.214901 26.244299 \r\nL 91.32926 26.334109 \r\nL 91.830282 26.42392 \r\nL 91.921875 26.51373 \r\nL 92.102267 26.603541 \r\nL 92.206887 26.693351 \r\nL 92.411551 26.783161 \r\nL 92.495824 26.872972 \r\nL 92.793799 26.962782 \r\nL 92.901121 27.052593 \r\nL 93.118989 27.142403 \r\nL 93.217044 27.232213 \r\nL 93.739628 27.322024 \r\nL 93.863063 27.411834 \r\nL 94.400588 27.501645 \r\nL 94.592896 27.591455 \r\nL 95.007959 27.681266 \r\nL 95.0147 27.771076 \r\nL 95.352111 27.950697 \r\nL 95.808995 28.040507 \r\nL 95.981622 28.130318 \r\nL 96.879361 28.220128 \r\nL 96.962022 28.309938 \r\nL 97.302921 28.399749 \r\nL 97.561401 28.758991 \r\nL 97.850602 28.848801 \r\nL 97.95891 28.938611 \r\nL 98.008992 29.028422 \r\nL 98.207077 29.118232 \r\nL 98.863372 29.208043 \r\nL 98.974348 29.297853 \r\nL 99.234241 29.387663 \r\nL 99.384519 29.657095 \r\nL 100.058854 29.746905 \r\nL 100.365354 30.016336 \r\nL 100.729988 30.106147 \r\nL 100.985035 30.375578 \r\nL 101.390346 30.465388 \r\nL 101.503963 30.645009 \r\nL 101.765862 30.73482 \r\nL 101.915343 30.82463 \r\nL 102.148439 30.914441 \r\nL 102.345528 31.004251 \r\nL 102.651899 31.094061 \r\nL 102.813004 31.273682 \r\nL 103.239858 31.453303 \r\nL 103.773954 31.543113 \r\nL 103.799505 31.632924 \r\nL 104.004882 31.722734 \r\nL 104.457856 31.992166 \r\nL 104.73238 32.081976 \r\nL 105.411714 32.441218 \r\nL 105.503677 32.531028 \r\nL 105.849242 32.800459 \r\nL 106.39222 32.89027 \r\nL 106.448024 32.98008 \r\nL 107.729619 33.069891 \r\nL 107.849908 33.249511 \r\nL 108.339834 33.339322 \r\nL 108.477941 33.429132 \r\nL 108.852893 33.518943 \r\nL 108.986951 33.608753 \r\nL 109.192004 33.698563 \r\nL 109.49697 33.878184 \r\nL 109.705915 33.967995 \r\nL 109.816604 34.237426 \r\nL 110.346708 34.506857 \r\nL 110.976979 34.596668 \r\nL 111.064277 34.776288 \r\nL 111.854922 34.866099 \r\nL 111.935859 34.955909 \r\nL 112.376528 35.04572 \r\nL 113.07198 35.315151 \r\nL 113.771152 35.404961 \r\nL 113.861767 35.494772 \r\nL 114.281102 35.584582 \r\nL 114.386946 35.674393 \r\nL 114.929655 35.764203 \r\nL 115.017898 35.854013 \r\nL 115.517674 35.943824 \r\nL 115.565551 36.123445 \r\nL 116.008643 36.392876 \r\nL 116.022783 36.482686 \r\nL 116.491588 36.572497 \r\nL 116.536258 36.752118 \r\nL 116.855545 36.841928 \r\nL 117.239174 36.931738 \r\nL 118.337023 37.740032 \r\nL 118.998015 37.829843 \r\nL 119.071419 38.009463 \r\nL 119.335861 38.099274 \r\nL 119.831264 38.189084 \r\nL 120.069563 38.548326 \r\nL 120.192548 38.638136 \r\nL 120.435647 38.817757 \r\nL 120.856867 38.907568 \r\nL 121.186068 38.997378 \r\nL 121.860751 39.087188 \r\nL 121.934516 39.35662 \r\nL 122.570727 39.44643 \r\nL 122.99222 39.805672 \r\nL 123.95438 39.895482 \r\nL 124.051926 40.075103 \r\nL 124.492897 40.164913 \r\nL 124.582599 40.344534 \r\nL 124.667183 40.434345 \r\nL 124.779373 40.613966 \r\nL 125.214349 40.703776 \r\nL 125.974712 40.793586 \r\nL 126.126996 40.973207 \r\nL 126.724356 41.063018 \r\nL 127.077301 41.242638 \r\nL 127.429801 41.332449 \r\nL 127.887255 41.871311 \r\nL 128.399091 41.961122 \r\nL 128.839565 42.410174 \r\nL 129.497611 42.499984 \r\nL 129.743781 42.859226 \r\nL 130.271716 42.949036 \r\nL 130.499558 43.308278 \r\nL 130.97245 43.398088 \r\nL 131.106684 43.66752 \r\nL 131.562072 43.75733 \r\nL 131.887294 43.936951 \r\nL 132.253035 44.206382 \r\nL 132.755669 44.386003 \r\nL 133.116221 44.655434 \r\nL 133.197195 44.835055 \r\nL 133.462578 45.104486 \r\nL 133.594616 45.284107 \r\nL 133.635961 45.373918 \r\nL 133.766516 45.553538 \r\nL 134.301146 45.643349 \r\nL 134.406572 45.733159 \r\nL 134.71052 45.91278 \r\nL 134.818856 46.002591 \r\nL 135.41552 46.092401 \r\nL 135.426797 46.182211 \r\nL 136.688572 46.272022 \r\nL 136.76116 46.451643 \r\nL 137.740735 46.541453 \r\nL 138.125786 47.080316 \r\nL 139.290104 47.529368 \r\nL 139.607644 47.708988 \r\nL 139.90681 47.798799 \r\nL 140.007261 47.888609 \r\nL 140.478595 47.97842 \r\nL 140.935373 48.337661 \r\nL 141.061111 48.607093 \r\nL 142.110587 48.966334 \r\nL 144.198671 49.415386 \r\nL 144.308703 49.684818 \r\nL 144.482743 49.774628 \r\nL 145.250659 49.954249 \r\nL 145.606105 50.044059 \r\nL 145.631271 50.13387 \r\nL 146.015544 50.22368 \r\nL 146.375373 50.403301 \r\nL 146.392413 50.493111 \r\nL 147.204249 50.672732 \r\nL 147.213728 50.762543 \r\nL 147.565403 50.852353 \r\nL 147.843008 51.031974 \r\nL 148.885127 51.391216 \r\nL 148.906318 51.481026 \r\nL 149.122796 51.570836 \r\nL 150.040679 51.660647 \r\nL 151.102312 52.109699 \r\nL 151.132991 52.199509 \r\nL 151.755137 52.28932 \r\nL 151.787585 52.468941 \r\nL 152.214305 52.648561 \r\nL 152.505187 52.738372 \r\nL 152.848885 53.007803 \r\nL 152.929072 53.097613 \r\nL 154.540011 53.456855 \r\nL 154.932577 53.726286 \r\nL 155.759785 54.265149 \r\nL 157.741506 55.253063 \r\nL 158.317693 55.342874 \r\nL 158.922826 55.522495 \r\nL 159.858231 55.791926 \r\nL 160.55467 56.061357 \r\nL 161.177778 56.151168 \r\nL 161.47019 56.330788 \r\nL 161.70834 56.510409 \r\nL 162.390306 56.60022 \r\nL 162.518174 56.69003 \r\nL 162.812707 57.049272 \r\nL 163.019993 57.139082 \r\nL 163.055463 57.228893 \r\nL 163.351877 57.408513 \r\nL 163.378794 57.498324 \r\nL 164.010734 57.588134 \r\nL 164.520457 57.947376 \r\nL 164.949845 58.037186 \r\nL 165.120281 58.306618 \r\nL 165.52631 58.396428 \r\nL 166.156266 58.486238 \r\nL 166.675023 58.665859 \r\nL 167.189444 58.84548 \r\nL 167.975048 59.294532 \r\nL 168.301298 59.563963 \r\nL 168.336841 59.653774 \r\nL 168.573083 59.833395 \r\nL 168.68772 59.923205 \r\nL 168.791609 60.013016 \r\nL 170.297261 60.551878 \r\nL 170.406876 60.821309 \r\nL 172.052618 61.270361 \r\nL 174.474624 61.809224 \r\nL 174.490376 61.899034 \r\nL 175.357825 62.258276 \r\nL 175.50732 62.527707 \r\nL 176.510013 62.617518 \r\nL 177.600732 62.886949 \r\nL 177.66638 63.06657 \r\nL 178.941526 63.15638 \r\nL 178.980331 63.336001 \r\nL 180.399811 63.964674 \r\nL 180.443611 64.144295 \r\nL 180.985449 64.234105 \r\nL 181.272329 64.323916 \r\nL 181.820959 64.503536 \r\nL 181.848997 64.593347 \r\nL 182.158245 64.862778 \r\nL 183.07484 65.491451 \r\nL 183.750514 65.581261 \r\nL 184.281298 66.030313 \r\nL 184.285718 66.120124 \r\nL 185.112638 66.389555 \r\nL 185.789331 66.569176 \r\nL 186.082317 66.838607 \r\nL 187.392997 66.928418 \r\nL 187.563655 67.108038 \r\nL 188.648036 67.37747 \r\nL 188.679271 67.557091 \r\nL 189.100404 67.826522 \r\nL 189.159288 67.916332 \r\nL 189.661756 68.095953 \r\nL 190.08627 68.545005 \r\nL 190.118997 68.634816 \r\nL 190.416866 68.724626 \r\nL 191.544194 69.083868 \r\nL 191.952873 69.62273 \r\nL 192.924831 69.802351 \r\nL 194.047341 70.341213 \r\nL 194.172652 70.431024 \r\nL 194.853459 70.700455 \r\nL 195.407371 71.418938 \r\nL 195.867438 71.508749 \r\nL 196.919518 71.598559 \r\nL 197.126054 71.77818 \r\nL 197.563086 71.957801 \r\nL 198.112958 72.406853 \r\nL 198.407862 72.766095 \r\nL 199.202389 73.035526 \r\nL 200.032867 73.394768 \r\nL 200.298472 73.754009 \r\nL 200.386498 73.84382 \r\nL 200.701193 73.93363 \r\nL 201.763457 74.023441 \r\nL 202.261653 74.203061 \r\nL 202.934992 74.472493 \r\nL 203.931903 74.741924 \r\nL 204.04894 74.921545 \r\nL 205.386741 75.640028 \r\nL 205.459951 75.729839 \r\nL 206.377954 76.08908 \r\nL 206.705437 76.358511 \r\nL 207.284348 76.448322 \r\nL 207.350626 76.538132 \r\nL 207.665906 76.627943 \r\nL 207.973188 76.807564 \r\nL 208.450462 77.076995 \r\nL 209.286259 77.166805 \r\nL 209.417037 77.256616 \r\nL 210.149279 77.346426 \r\nL 210.772944 77.615857 \r\nL 211.120154 77.705668 \r\nL 211.519771 78.24453 \r\nL 212.087766 78.424151 \r\nL 212.603447 78.693582 \r\nL 214.385249 79.142634 \r\nL 214.600754 79.412066 \r\nL 215.705649 79.771307 \r\nL 215.779665 79.861118 \r\nL 216.794774 80.130549 \r\nL 216.796868 80.220359 \r\nL 217.337863 80.39998 \r\nL 218.299268 80.849032 \r\nL 218.367891 80.938843 \r\nL 218.916632 81.118464 \r\nL 218.976879 81.208274 \r\nL 220.822615 81.567516 \r\nL 220.870575 81.657326 \r\nL 221.242283 81.747136 \r\nL 221.826309 82.196189 \r\nL 222.141921 82.375809 \r\nL 223.312345 82.735051 \r\nL 223.446848 83.004482 \r\nL 223.762405 83.094293 \r\nL 224.382919 83.184103 \r\nL 224.497668 83.273914 \r\nL 224.826457 83.453534 \r\nL 224.833499 83.543345 \r\nL 225.133063 83.722966 \r\nL 225.19444 83.902586 \r\nL 225.605435 83.992397 \r\nL 225.678265 84.082207 \r\nL 226.010667 84.172018 \r\nL 226.057349 84.261828 \r\nL 226.310028 84.441449 \r\nL 226.354022 84.531259 \r\nL 226.747691 84.71088 \r\nL 227.088766 85.070122 \r\nL 228.505253 85.788605 \r\nL 228.629452 86.058036 \r\nL 229.723006 86.507089 \r\nL 229.859622 86.77652 \r\nL 230.197028 87.045951 \r\nL 230.208425 87.225572 \r\nL 231.764881 87.944055 \r\nL 232.617023 88.123676 \r\nL 232.688722 88.303297 \r\nL 234.902145 88.842159 \r\nL 235.506074 89.111591 \r\nL 235.547603 89.291211 \r\nL 235.938826 89.560643 \r\nL 236.320207 89.650453 \r\nL 236.528151 89.830074 \r\nL 236.857552 90.009695 \r\nL 237.019111 90.279126 \r\nL 237.249904 90.458747 \r\nL 237.44054 90.728178 \r\nL 239.020643 91.267041 \r\nL 239.067713 91.356851 \r\nL 239.471055 91.536472 \r\nL 239.642065 91.895714 \r\nL 240.362558 92.165145 \r\nL 240.434406 92.344766 \r\nL 240.900718 92.524386 \r\nL 240.915562 92.614197 \r\nL 241.377112 92.793818 \r\nL 241.404446 92.883628 \r\nL 242.703627 93.33268 \r\nL 242.791523 93.602111 \r\nL 243.008121 93.781732 \r\nL 243.124778 94.140974 \r\nL 243.472395 94.320595 \r\nL 243.871438 94.590026 \r\nL 245.430063 95.667751 \r\nL 245.539714 95.847372 \r\nL 246.237765 96.206614 \r\nL 246.290507 96.386234 \r\nL 247.077436 96.835286 \r\nL 247.157956 97.104718 \r\nL 247.248909 97.194528 \r\nL 249.724129 98.002822 \r\nL 249.729244 98.092632 \r\nL 250.559092 98.272253 \r\nL 250.941901 98.451874 \r\nL 251.352507 98.541684 \r\nL 251.530189 98.631495 \r\nL 251.68276 98.811116 \r\nL 251.810295 99.170357 \r\nL 251.902194 99.439789 \r\nL 252.73308 99.79903 \r\nL 252.840619 99.978651 \r\nL 253.592838 100.427703 \r\nL 253.992937 101.056376 \r\nL 254.842503 101.235997 \r\nL 254.853418 101.325807 \r\nL 255.660824 101.95448 \r\nL 255.75743 102.134101 \r\nL 256.140591 102.313722 \r\nL 256.35263 102.672964 \r\nL 256.631828 102.942395 \r\nL 257.764151 103.750689 \r\nL 257.778883 103.840499 \r\nL 258.406496 104.02012 \r\nL 260.039173 104.10993 \r\nL 260.041286 104.199741 \r\nL 260.938849 104.738603 \r\nL 261.048204 105.008034 \r\nL 261.332721 105.367276 \r\nL 261.588124 105.457086 \r\nL 262.168369 105.636707 \r\nL 262.436171 105.816328 \r\nL 262.705769 105.995949 \r\nL 262.73757 106.085759 \r\nL 263.033354 106.26538 \r\nL 263.260886 106.534811 \r\nL 264.491537 106.714432 \r\nL 264.516054 106.804243 \r\nL 265.616206 107.073674 \r\nL 266.689653 107.971778 \r\nL 267.593572 108.151399 \r\nL 267.646869 108.241209 \r\nL 269.409695 108.959693 \r\nL 269.913441 109.229124 \r\nL 270.094217 109.408745 \r\nL 270.128983 109.498555 \r\nL 271.961154 110.48647 \r\nL 272.881418 110.755901 \r\nL 272.907214 110.845711 \r\nL 273.332238 110.935522 \r\nL 274.30832 111.115143 \r\nL 274.371031 111.204953 \r\nL 274.780358 111.384574 \r\nL 274.921385 111.564195 \r\nL 275.464363 111.833626 \r\nL 276.186912 112.552109 \r\nL 276.285872 112.64192 \r\nL 276.722663 112.821541 \r\nL 276.841711 113.001161 \r\nL 277.342214 113.180782 \r\nL 277.498436 113.360403 \r\nL 277.516671 113.450214 \r\nL 278.267185 113.629834 \r\nL 278.473202 113.719645 \r\nL 278.723639 113.899266 \r\nL 278.768893 113.989076 \r\nL 279.465462 114.168697 \r\nL 280.960931 115.246422 \r\nL 281.319852 115.426043 \r\nL 281.333677 115.515853 \r\nL 281.612208 115.695474 \r\nL 281.639134 115.875095 \r\nL 282.041716 116.144526 \r\nL 282.204702 116.324147 \r\nL 282.894006 116.413957 \r\nL 283.186473 116.773199 \r\nL 284.961548 117.850924 \r\nL 285.05467 118.030545 \r\nL 285.229016 118.299976 \r\nL 285.388685 118.569407 \r\nL 286.338322 118.838839 \r\nL 286.362709 118.928649 \r\nL 288.016439 119.736943 \r\nL 288.064769 119.916564 \r\nL 288.676815 120.096184 \r\nL 288.763543 120.275805 \r\nL 290.794114 121.802582 \r\nL 290.932657 121.892393 \r\nL 290.980895 122.072014 \r\nL 291.61629 122.431255 \r\nL 291.980438 122.790497 \r\nL 292.307707 122.970118 \r\nL 292.308189 123.059928 \r\nL 292.805746 123.41917 \r\nL 292.915731 123.598791 \r\nL 292.940471 123.688601 \r\nL 293.112945 123.868222 \r\nL 293.127214 123.958032 \r\nL 294.09479 124.317274 \r\nL 294.170566 124.407084 \r\nL 294.437089 124.586705 \r\nL 294.520481 124.766326 \r\nL 294.764636 125.035757 \r\nL 294.793898 125.125568 \r\nL 295.737493 125.484809 \r\nL 295.873015 125.66443 \r\nL 296.141428 125.844051 \r\nL 296.257436 126.203293 \r\nL 297.004763 126.742155 \r\nL 297.81406 127.370828 \r\nL 297.846082 127.460639 \r\nL 298.256503 127.640259 \r\nL 298.42642 127.81988 \r\nL 298.690812 128.268932 \r\nL 298.846348 128.448553 \r\nL 298.871625 128.628174 \r\nL 299.445755 128.807795 \r\nL 299.701603 129.167036 \r\nL 299.910177 129.526278 \r\nL 300.453841 129.705899 \r\nL 300.507416 129.97533 \r\nL 301.651487 130.693814 \r\nL 301.654564 130.783624 \r\nL 302.04788 131.053055 \r\nL 302.153232 131.232676 \r\nL 303.165062 132.220591 \r\nL 303.172141 132.310401 \r\nL 303.469518 132.490022 \r\nL 303.817729 132.669643 \r\nL 304.061309 133.118695 \r\nL 304.597449 133.298316 \r\nL 304.793366 133.477936 \r\nL 305.054458 133.657557 \r\nL 305.980597 135.453766 \r\nL 306.368298 135.723197 \r\nL 306.380177 135.813007 \r\nL 306.755796 135.992628 \r\nL 307.147462 136.44168 \r\nL 307.412799 136.621301 \r\nL 307.68338 137.160164 \r\nL 307.687939 137.249974 \r\nL 308.167705 137.429595 \r\nL 308.438601 137.609216 \r\nL 308.887383 137.878647 \r\nL 309.102832 138.237889 \r\nL 309.105 138.327699 \r\nL 310.489668 139.315614 \r\nL 310.523933 139.405424 \r\nL 310.803465 139.585045 \r\nL 310.882447 139.854476 \r\nL 311.26709 140.213718 \r\nL 312.634987 141.381253 \r\nL 312.801068 141.650684 \r\nL 313.435148 142.009926 \r\nL 313.890526 142.548789 \r\nL 314.739073 142.728409 \r\nL 314.9755 143.087651 \r\nL 315.030947 143.177461 \r\nL 315.487308 143.357082 \r\nL 315.750328 143.985755 \r\nL 316.081989 144.165376 \r\nL 316.282446 144.434807 \r\nL 316.736787 144.524618 \r\nL 316.888525 144.794049 \r\nL 317.540525 145.153291 \r\nL 317.573678 145.332911 \r\nL 317.936361 145.602343 \r\nL 318.267837 146.141205 \r\nL 318.438996 146.320826 \r\nL 318.926842 146.500447 \r\nL 319.783524 147.667982 \r\nL 319.882965 147.937414 \r\nL 320.452128 148.206845 \r\nL 321.106908 148.925328 \r\nL 321.259442 149.28457 \r\nL 321.291669 149.37438 \r\nL 321.668399 149.643811 \r\nL 321.770286 149.913243 \r\nL 322.128856 150.092864 \r\nL 322.241658 150.362295 \r\nL 323.756511 151.44002 \r\nL 323.866533 151.619641 \r\nL 324.303195 152.068693 \r\nL 324.498389 152.338124 \r\nL 324.905882 152.697366 \r\nL 324.928769 152.787176 \r\nL 325.183375 152.966797 \r\nL 326.515858 154.763005 \r\nL 326.942217 155.212057 \r\nL 326.985951 155.391678 \r\nL 327.191523 155.661109 \r\nL 327.332364 155.930541 \r\nL 327.596125 156.110161 \r\nL 327.600443 156.199972 \r\nL 327.765004 156.379593 \r\nL 328.122332 157.098076 \r\nL 328.75252 157.99618 \r\nL 328.784487 158.175801 \r\nL 329.058367 158.355422 \r\nL 329.454129 158.804474 \r\nL 329.542433 158.984095 \r\nL 329.673192 159.163716 \r\nL 330.088246 159.882199 \r\nL 330.224194 160.15163 \r\nL 330.366369 160.421062 \r\nL 332.704714 163.115374 \r\nL 333.17414 163.294995 \r\nL 333.314795 163.744047 \r\nL 333.75192 164.37272 \r\nL 334.194772 164.911582 \r\nL 334.482254 165.540255 \r\nL 334.565683 166.079118 \r\nL 334.789676 166.348549 \r\nL 334.835579 166.52817 \r\nL 335.027233 166.707791 \r\nL 335.115166 167.067032 \r\nL 335.499364 167.336464 \r\nL 335.736032 167.605895 \r\nL 336.486491 168.324378 \r\nL 336.687282 168.77343 \r\nL 336.904491 169.132672 \r\nL 337.056136 169.581724 \r\nL 337.503769 169.851155 \r\nL 337.710842 170.479828 \r\nL 337.937058 170.659449 \r\nL 338.020506 170.83907 \r\nL 338.118557 171.108501 \r\nL 338.240569 171.916795 \r\nL 338.939584 172.725089 \r\nL 338.951759 172.814899 \r\nL 339.254511 172.99452 \r\nL 339.972558 173.982434 \r\nL 340.11464 174.431487 \r\nL 340.394895 174.611107 \r\nL 340.678652 175.14997 \r\nL 340.798811 175.329591 \r\nL 340.920268 175.688832 \r\nL 341.162699 176.048074 \r\nL 341.282933 176.227695 \r\nL 341.58044 176.407316 \r\nL 341.642947 176.676747 \r\nL 342.072753 176.856368 \r\nL 342.097307 176.946178 \r\nL 342.46542 177.125799 \r\nL 343.050428 178.203524 \r\nL 343.351178 178.562766 \r\nL 343.466316 178.832197 \r\nL 343.777333 179.730301 \r\nL 344.161995 179.909922 \r\nL 344.421809 180.269164 \r\nL 344.570581 180.718216 \r\nL 344.861232 181.077457 \r\nL 344.87665 181.167268 \r\nL 345.415273 181.61632 \r\nL 346.294601 182.873666 \r\nL 346.366485 183.053287 \r\nL 346.61153 183.322718 \r\nL 346.678429 183.502339 \r\nL 346.941801 183.86158 \r\nL 347.220017 184.400443 \r\nL 347.466229 184.939305 \r\nL 347.559536 185.298547 \r\nL 347.690147 185.567978 \r\nL 347.718501 185.837409 \r\nL 347.972218 186.106841 \r\nL 348.134185 186.825324 \r\nL 348.333956 187.094755 \r\nL 348.482951 187.723428 \r\nL 348.695416 188.08267 \r\nL 348.727198 188.262291 \r\nL 349.005043 188.531722 \r\nL 350.023952 190.32793 \r\nL 350.231488 190.507551 \r\nL 350.304095 190.687172 \r\nL 350.543283 190.956603 \r\nL 350.551104 191.046414 \r\nL 350.793405 191.315845 \r\nL 350.938861 191.764897 \r\nL 351.489418 192.842622 \r\nL 351.50063 192.932432 \r\nL 351.870967 193.291674 \r\nL 351.935698 193.471295 \r\nL 352.363538 194.099968 \r\nL 352.62628 194.369399 \r\nL 352.784912 194.998072 \r\nL 353.483463 195.985987 \r\nL 354.100067 197.063712 \r\nL 354.106127 197.153522 \r\nL 354.41859 197.422953 \r\nL 354.619992 197.872005 \r\nL 354.750826 198.051626 \r\nL 354.873524 198.231247 \r\nL 355.641365 199.668214 \r\nL 355.926456 199.847834 \r\nL 356.281171 200.476507 \r\nL 356.355835 201.01537 \r\nL 356.796203 202.003284 \r\nL 356.920365 202.542147 \r\nL 357.149732 202.901389 \r\nL 357.154383 203.17082 \r\nL 357.297133 203.350441 \r\nL 357.705441 204.607787 \r\nL 357.905787 204.967028 \r\nL 358.646998 206.314184 \r\nL 358.798439 206.763237 \r\nL 359.089609 207.751151 \r\nL 359.39855 208.020582 \r\nL 359.534425 208.379824 \r\nL 359.924035 209.008497 \r\nL 359.944438 209.098307 \r\nL 360.200082 209.277928 \r\nL 360.369425 209.457549 \r\nL 360.403745 209.72698 \r\nL 360.603294 209.996412 \r\nL 360.744117 210.265843 \r\nL 360.779883 210.355653 \r\nL 361.02161 210.535274 \r\nL 361.130539 210.894516 \r\nL 361.529674 211.163947 \r\nL 361.675815 211.612999 \r\nL 362.116813 212.241672 \r\nL 362.473288 212.421293 \r\nL 362.630678 212.690724 \r\nL 362.893364 213.319397 \r\nL 362.929557 213.588828 \r\nL 363.187091 213.858259 \r\nL 363.286421 214.397122 \r\nL 363.363068 214.756364 \r\nL 363.363068 214.756364 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 43.78125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 224.64 \r\nL 378.58125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 378.58125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 7.2 \r\nL 378.58125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 50.78125 219.64 \r\nL 127.790625 219.64 \r\nQ 129.790625 219.64 129.790625 217.64 \r\nL 129.790625 189.28375 \r\nQ 129.790625 187.28375 127.790625 187.28375 \r\nL 50.78125 187.28375 \r\nQ 48.78125 187.28375 48.78125 189.28375 \r\nL 48.78125 217.64 \r\nQ 48.78125 219.64 50.78125 219.64 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\">\r\n     <path d=\"M 52.78125 195.382188 \r\nL 72.78125 195.382188 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\"/>\r\n    <g id=\"text_15\">\r\n     <!-- Precision -->\r\n     <defs>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n     </defs>\r\n     <g transform=\"translate(80.78125 198.882188)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"60.287109\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"101.369141\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"162.892578\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"217.873047\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"245.65625\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"297.755859\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"325.539062\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"386.720703\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 52.78125 210.060313 \r\nL 72.78125 210.060313 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_16\">\r\n     <!-- Recall -->\r\n     <defs>\r\n      <path d=\"M 44.390625 34.1875 \r\nQ 47.5625 33.109375 50.5625 29.59375 \r\nQ 53.5625 26.078125 56.59375 19.921875 \r\nL 66.609375 0 \r\nL 56 0 \r\nL 46.6875 18.703125 \r\nQ 43.0625 26.03125 39.671875 28.421875 \r\nQ 36.28125 30.8125 30.421875 30.8125 \r\nL 19.671875 30.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nL 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.578125 72.90625 50.734375 67.671875 \r\nQ 56.890625 62.453125 56.890625 51.90625 \r\nQ 56.890625 45.015625 53.6875 40.46875 \r\nQ 50.484375 35.9375 44.390625 34.1875 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 38.921875 \r\nL 32.078125 38.921875 \r\nQ 39.203125 38.921875 42.84375 42.21875 \r\nQ 46.484375 45.515625 46.484375 51.90625 \r\nQ 46.484375 58.296875 42.84375 61.546875 \r\nQ 39.203125 64.796875 32.078125 64.796875 \r\nz\r\n\" id=\"DejaVuSans-82\"/>\r\n     </defs>\r\n     <g transform=\"translate(80.78125 213.560313)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"69.419922\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"130.943359\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"185.923828\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"247.203125\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"274.986328\" xlink:href=\"#DejaVuSans-108\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pbfeca32b17\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "#Threshold-PR curve\n",
    "train_loss, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader)\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_true.ravel(), y_prob.ravel())\n",
    "plt.plot(thresholds, precisions[:-1], \"r--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"b-\", label=\"Recall\")\n",
    "plt.ylabel(\"Performance\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the best threshold\n",
    "def find_best_threshold(y_true, y_prob):\n",
    "    \"\"\"Find the best threshold for maximum F1.\"\"\"\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    f1s = (2 * precisions * recalls) / (precisions + recalls)\n",
    "    return thresholds[np.argmax(f1s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.2521956"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "# Best threshold for f1\n",
    "threshold = find_best_threshold(y_true.ravel(), y_prob.ravel())\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine predictions using threshold\n",
    "test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
    "y_pred = np.array([np.where(prob >= threshold, 1, 0) for prob in y_prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"precision\": 0.8228961917603603,\n  \"recall\": 0.48522342497404075,\n  \"f1\": 0.5791385398227955,\n  \"num_samples\": 480.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "performance = get_performance(\n",
    "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "print (json.dumps(performance['overall'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='tag', index=3, options=('interpretability', 'segmentation', 'produ…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "041a943470204824a5626f828f1e7818"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(tag=list(sorted_tags_by_f1.keys()))\n",
    "def display_tag_analysis(tag='transformers'):\n",
    "    # Performance\n",
    "    print (json.dumps(performance[\"class\"][tag], indent=2))\n",
    "    \n",
    "    # TP, FP, FN samples\n",
    "    index = label_encoder.class_to_index[tag]\n",
    "    tp, fp, fn = [], [], []\n",
    "    for i in range(len(y_test)):\n",
    "        true = y_test[i][index]\n",
    "        pred = y_pred[i][index]\n",
    "        if true and pred:\n",
    "            tp.append(i)\n",
    "        elif not true and pred:\n",
    "            fp.append(i)\n",
    "        elif true and not pred:\n",
    "            fn.append(i)\n",
    "            \n",
    "    # Samples\n",
    "    num_samples = 3\n",
    "    if len(tp): \n",
    "        print (\"\\n=== True positives ===\\n\")\n",
    "        for i in tp[:num_samples]:        \n",
    "            print (f\"  {X_test_raw[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")\n",
    "    if len(fp): \n",
    "        print (\"=== False positives ===\\n\")\n",
    "        for i in fp[:num_samples]:        \n",
    "            print (f\"  {X_test_raw[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")\n",
    "    if len(fn): \n",
    "        print (\"=== False negatives ===\\n\")\n",
    "        for i in fn[:num_samples]:        \n",
    "            print (f\"  {X_test_raw[i]}\") \n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts\n",
    "dir = Path(\"cnn\")\n",
    "dir.mkdir(parents=True, exist_ok=True)\n",
    "tokenizer.save(fp=Path(dir, 'tokenzier.json'))\n",
    "label_encoder.save(fp=Path(dir, 'label_encoder.json'))\n",
    "torch.save(best_model.state_dict(), Path(dir, 'model.pt'))\n",
    "with open(Path(dir, 'performance.json'), \"w\") as fp:\n",
    "    json.dump(performance, indent=2, sort_keys=False, fp=fp)"
   ]
  },
  {
   "source": [
    "### Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embeddings): Embedding(136, 128, padding_idx=0)\n",
       "  (conv): ModuleList(\n",
       "    (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "    (1): Conv1d(128, 128, kernel_size=(2,), stride=(1,))\n",
       "    (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
       "    (3): Conv1d(128, 128, kernel_size=(4,), stride=(1,))\n",
       "    (4): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
       "    (5): Conv1d(128, 128, kernel_size=(6,), stride=(1,))\n",
       "    (6): Conv1d(128, 128, kernel_size=(7,), stride=(1,))\n",
       "    (7): Conv1d(128, 128, kernel_size=(8,), stride=(1,))\n",
       "    (8): Conv1d(128, 128, kernel_size=(9,), stride=(1,))\n",
       "    (9): Conv1d(128, 128, kernel_size=(10,), stride=(1,))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=35, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "# Load artifacts\n",
    "device = torch.device(\"cpu\")\n",
    "tokenizer = Tokenizer.load(fp=Path(dir, 'tokenzier.json'))\n",
    "label_encoder = LabelEncoder.load(fp=Path(dir, 'label_encoder.json'))\n",
    "model = CNN(\n",
    "    embedding_dim=embedding_dim, vocab_size=vocab_size,\n",
    "    num_filters=num_filters, filter_sizes=filter_sizes,\n",
    "    hidden_dim=hidden_dim, dropout_p=dropout_p,\n",
    "    num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(Path(dir, 'model.pt'), map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "text = \"Transfer learning with BERT for self-supervised learning\"\n",
    "X = np.array(tokenizer.texts_to_sequences([preprocess(text)]))\n",
    "y_filler = label_encoder.encode([np.array([label_encoder.classes[0]]*len(X))])\n",
    "dataset = CNNTextDataset(\n",
    "    X=X, y=y_filler, max_filter_size=max(filter_sizes))\n",
    "dataloader = dataset.create_dataloader(\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['self-supervised-learning', 'transfer-learning']]"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "# Inference\n",
    "y_prob = trainer.predict_step(dataloader)\n",
    "y_pred = np.array([np.where(prob >= threshold, 1, 0) for prob in y_prob])\n",
    "label_encoder.decode(y_pred)"
   ]
  },
  {
   "source": [
    "limitations: \n",
    "representation: embeddings are not contextual."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# RNN w/Embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data splits\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)\n",
    "X_test_raw = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "cuda = True\n",
    "device = torch.device('cuda' if (\n",
    "    torch.cuda.is_available() and cuda) else 'cpu')\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "if device.type == 'cuda':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X tokenizer:\n  <Tokenizer(num_tokens=136)>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "char_level = True\n",
    "tokenizer = Tokenizer(char_level=char_level)\n",
    "tokenizer.fit_on_texts(texts=X_train)\n",
    "vocab_size = len(tokenizer)\n",
    "print (\"X tokenizer:\\n\"\n",
    "    f\"  {tokenizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " ' ': 2,\n",
       " 'e': 3,\n",
       " 'n': 4,\n",
       " 'i': 5,\n",
       " 't': 6,\n",
       " 'a': 7,\n",
       " 'o': 8,\n",
       " 'r': 9,\n",
       " 's': 10,\n",
       " 'l': 11,\n",
       " 'c': 12,\n",
       " 'd': 13,\n",
       " 'g': 14,\n",
       " 'h': 15,\n",
       " 'm': 16,\n",
       " 'u': 17,\n",
       " 'p': 18,\n",
       " 'f': 19,\n",
       " 'y': 20,\n",
       " 'w': 21,\n",
       " 'b': 22,\n",
       " 'T': 23,\n",
       " 'v': 24,\n",
       " '.': 25,\n",
       " 'A': 26,\n",
       " '-': 27,\n",
       " 'L': 28,\n",
       " 'k': 29,\n",
       " 'P': 30,\n",
       " 'S': 31,\n",
       " 'N': 32,\n",
       " 'C': 33,\n",
       " 'M': 34,\n",
       " 'D': 35,\n",
       " 'I': 36,\n",
       " ',': 37,\n",
       " 'x': 38,\n",
       " 'R': 39,\n",
       " 'F': 40,\n",
       " 'G': 41,\n",
       " 'E': 42,\n",
       " 'B': 43,\n",
       " ':': 44,\n",
       " '2': 45,\n",
       " 'O': 46,\n",
       " 'z': 47,\n",
       " 'H': 48,\n",
       " 'W': 49,\n",
       " '0': 50,\n",
       " 'V': 51,\n",
       " '(': 52,\n",
       " ')': 53,\n",
       " 'j': 54,\n",
       " 'U': 55,\n",
       " 'K': 56,\n",
       " 'q': 57,\n",
       " '1': 58,\n",
       " '\"': 59,\n",
       " 'Q': 60,\n",
       " '\\r': 61,\n",
       " '\\n': 62,\n",
       " '/': 63,\n",
       " '&': 64,\n",
       " 'Y': 65,\n",
       " '3': 66,\n",
       " '+': 67,\n",
       " \"'\": 68,\n",
       " '?': 69,\n",
       " 'J': 70,\n",
       " '5': 71,\n",
       " '9': 72,\n",
       " 'X': 73,\n",
       " '8': 74,\n",
       " '6': 75,\n",
       " '4': 76,\n",
       " '’': 77,\n",
       " '!': 78,\n",
       " 'Z': 79,\n",
       " '—': 80,\n",
       " '7': 81,\n",
       " '“': 82,\n",
       " '”': 83,\n",
       " '|': 84,\n",
       " '🤗': 85,\n",
       " '️': 86,\n",
       " '%': 87,\n",
       " '=': 88,\n",
       " '–': 89,\n",
       " '💻': 90,\n",
       " '❤': 91,\n",
       " '_': 92,\n",
       " '🎨': 93,\n",
       " '>': 94,\n",
       " '🌊': 95,\n",
       " '^': 96,\n",
       " '🚗': 97,\n",
       " '🕶': 98,\n",
       " '<': 99,\n",
       " '\\u2060': 100,\n",
       " '⚡': 101,\n",
       " ';': 102,\n",
       " '@': 103,\n",
       " '#': 104,\n",
       " '🏥': 105,\n",
       " '💤': 106,\n",
       " '✨': 107,\n",
       " '😎': 108,\n",
       " '🔍': 109,\n",
       " '[': 110,\n",
       " ']': 111,\n",
       " '🦠': 112,\n",
       " '🍼': 113,\n",
       " '\\u2009': 114,\n",
       " '基': 115,\n",
       " '于': 116,\n",
       " '📈': 117,\n",
       " '⭐': 118,\n",
       " '🔥': 119,\n",
       " '*': 120,\n",
       " '…': 121,\n",
       " '🎐': 122,\n",
       " 'ç': 123,\n",
       " '🦄': 124,\n",
       " '✏': 125,\n",
       " '✂': 126,\n",
       " '🏡': 127,\n",
       " '𝘢': 128,\n",
       " '𝘯': 129,\n",
       " '𝘺': 130,\n",
       " '🐳': 131,\n",
       " '🤖': 132,\n",
       " '💬': 133,\n",
       " '🎥': 134,\n",
       " '👀': 135}"
      ]
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "source": [
    "tokenizer.token_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text to indices:\n  (preprocessed) → Albumentations Fast image augmentation library and easy to use wrapper around other libraries.\n  (tokenized) → [26 11 22 17 16  3  4  6  7  6  5  8  4 10  2 40  7 10  6  2  5 16  7 14\n  3  2  7 17 14 16  3  4  6  7  6  5  8  4  2 11  5 22  9  7  9 20  2  7\n  4 13  2  3  7 10 20  2  6  8  2 17 10  3  2 21  9  7 18 18  3  9  2  7\n  9  8 17  4 13  2  8  6 15  3  9  2 11  5 22  9  7  9  5  3 10 25]\n"
     ]
    }
   ],
   "source": [
    "# Convert texts to sequences of indices\n",
    "X_train = np.array(tokenizer.texts_to_sequences(X_train))\n",
    "X_val = np.array(tokenizer.texts_to_sequences(X_val))\n",
    "X_test = np.array(tokenizer.texts_to_sequences(X_test))\n",
    "preprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\n",
    "print (\"Text to indices:\\n\"\n",
    "    f\"  (preprocessed) → {preprocessed_text}\\n\"\n",
    "    f\"  (tokenized) → {X_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "class counts:\n  [120  41 388 106  41  75  34  73  51  78  64  51  55  93  51 429  33  69\n  30  51 258  32  49  59  57  60  48  40 213  40  34  46 196  39  39]\nclass weights:\n  {0: 0.008333333333333333, 1: 0.024390243902439025, 2: 0.002577319587628866, 3: 0.009433962264150943, 4: 0.024390243902439025, 5: 0.013333333333333334, 6: 0.029411764705882353, 7: 0.0136986301369863, 8: 0.0196078431372549, 9: 0.01282051282051282, 10: 0.015625, 11: 0.0196078431372549, 12: 0.01818181818181818, 13: 0.010752688172043012, 14: 0.0196078431372549, 15: 0.002331002331002331, 16: 0.030303030303030304, 17: 0.014492753623188406, 18: 0.03333333333333333, 19: 0.0196078431372549, 20: 0.003875968992248062, 21: 0.03125, 22: 0.02040816326530612, 23: 0.01694915254237288, 24: 0.017543859649122806, 25: 0.016666666666666666, 26: 0.020833333333333332, 27: 0.025, 28: 0.004694835680751174, 29: 0.025, 30: 0.029411764705882353, 31: 0.021739130434782608, 32: 0.00510204081632653, 33: 0.02564102564102564, 34: 0.02564102564102564}\n"
     ]
    }
   ],
   "source": [
    "# Class weights\n",
    "counts = np.bincount([label_encoder.class_to_index[class_] for class_ in all_tags])\n",
    "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
    "print (\"class counts:\\n\"\n",
    "    f\"  {counts}\\n\"\n",
    "    \"class weights:\\n\"\n",
    "    f\"  {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Dataset(N={len(self)})>\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        return [X, len(X), y]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Processing on a batch.\"\"\"\n",
    "        # Get inputs\n",
    "        batch = np.array(batch, dtype=object)\n",
    "        X = batch[:, 0]\n",
    "        seq_lens = batch[:, 1]\n",
    "        y = np.stack(batch[:, 2], axis=0)\n",
    "\n",
    "        # Pad inputs\n",
    "        X = pad_sequences(sequences=X)\n",
    "\n",
    "        # Cast\n",
    "        X = torch.LongTensor(X.astype(np.int32))\n",
    "        seq_lens = torch.LongTensor(seq_lens.astype(np.int32))\n",
    "        y = torch.FloatTensor(y.astype(np.int32))\n",
    "\n",
    "        return X, seq_lens, y\n",
    "\n",
    "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data splits:\n  Train dataset:<Dataset(N=1000)>\n  Val dataset: <Dataset(N=227)>\n  Test dataset: <Dataset(N=217)>\nSample point:\n  X: [26 11 22 17 16  3  4  6  7  6  5  8  4 10  2 40  7 10  6  2  5 16  7 14\n  3  2  7 17 14 16  3  4  6  7  6  5  8  4  2 11  5 22  9  7  9 20  2  7\n  4 13  2  3  7 10 20  2  6  8  2 17 10  3  2 21  9  7 18 18  3  9  2  7\n  9  8 17  4 13  2  8  6 15  3  9  2 11  5 22  9  7  9  5  3 10 25]\n  seq_len: 94\n  y: [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "batch_size = 128\n",
    "train_dataset = RNNTextDataset(\n",
    "    X=X_train, y=y_train)\n",
    "val_dataset = RNNTextDataset(\n",
    "    X=X_val, y=y_val)\n",
    "test_dataset = RNNTextDataset(\n",
    "    X=X_test, y=y_test)\n",
    "print (\"Data splits:\\n\"\n",
    "    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n",
    "    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n",
    "    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  X: {train_dataset[0][0]}\\n\"\n",
    "    f\"  seq_len: {train_dataset[0][1]}\\n\"\n",
    "    f\"  y: {train_dataset[0][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 226])\nSample batch:\n  X: [128, 226]\n  seq_lens: [128]\n  y: [128, 35]\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "train_dataloader = train_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "val_dataloader = val_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "test_dataloader = test_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "batch_X, batch_seq_lens, batch_y = next(iter(train_dataloader))\n",
    "print (batch_X.shape)\n",
    "print (\"Sample batch:\\n\"\n",
    "    f\"  X: {list(batch_X.size())}\\n\"\n",
    "    f\"  seq_lens: {list(batch_seq_lens.size())}\\n\"\n",
    "    f\"  y: {list(batch_y.size())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "embedding_dim = 128\n",
    "rnn_hidden_dim = 128\n",
    "hidden_dim = 128\n",
    "dropout_p =0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_last_relevant_hidden(hiddens, seq_lens):\n",
    "    \"\"\"Extract and collect the last relevant \n",
    "    hidden state based on the sequence length.\"\"\"\n",
    "    seq_lens = seq_lens.long().detach().cpu().numpy() - 1\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(seq_lens):\n",
    "        out.append(hiddens[batch_index, column_index])\n",
    "    return torch.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, rnn_hidden_dim,\n",
    "                 hidden_dim, dropout_p, num_classes, padding_idx=0):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                       num_embeddings=vocab_size,\n",
    "                                       padding_idx=padding_idx)\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.GRU(embedding_dim, rnn_hidden_dim, \n",
    "                          batch_first=True, bidirectional=True)\n",
    "     \n",
    "        # FC weights\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_dim*2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Inputs\n",
    "        x_in, seq_lens = inputs\n",
    "\n",
    "        # Embed\n",
    "        x_in = self.embeddings(x_in)\n",
    "            \n",
    "        # Rnn outputs\n",
    "        out, h_n = self.rnn(x_in)\n",
    "        z = gather_last_relevant_hidden(hiddens=out, seq_lens=seq_lens)\n",
    "\n",
    "        # FC layers\n",
    "        z = self.fc1(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc2(z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method Module.named_parameters of RNN(\n  (embeddings): Embedding(136, 128, padding_idx=0)\n  (rnn): GRU(128, 128, batch_first=True, bidirectional=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=256, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=35, bias=True)\n)>\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = RNN(\n",
    "    embedding_dim=embedding_dim, vocab_size=vocab_size,\n",
    "    rnn_hidden_dim=rnn_hidden_dim, hidden_dim=hidden_dim, \n",
    "    dropout_p=dropout_p, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "print (model.named_parameters)"
   ]
  },
  {
   "source": [
    "Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "lr = 2e-3\n",
    "num_epochs = 200\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "class_weights_tensor = torch.Tensor(np.array(list(class_weights.values())))\n",
    "loss_fn = nn.BCEWithLogitsLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer & scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer module\n",
    "trainer = Trainer(\n",
    "    model=model, device=device, loss_fn=loss_fn, \n",
    "    optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 | train_loss: 0.00776, val_loss: 0.00330, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.00406, val_loss: 0.00313, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 3 | train_loss: 0.00344, val_loss: 0.00276, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 4 | train_loss: 0.00311, val_loss: 0.00272, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 5 | train_loss: 0.00302, val_loss: 0.00267, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 6 | train_loss: 0.00296, val_loss: 0.00265, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 7 | train_loss: 0.00291, val_loss: 0.00262, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 8 | train_loss: 0.00287, val_loss: 0.00261, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 9 | train_loss: 0.00285, val_loss: 0.00260, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 10 | train_loss: 0.00282, val_loss: 0.00259, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 11 | train_loss: 0.00279, val_loss: 0.00258, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 12 | train_loss: 0.00279, val_loss: 0.00257, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 13 | train_loss: 0.00274, val_loss: 0.00256, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 14 | train_loss: 0.00271, val_loss: 0.00255, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 15 | train_loss: 0.00270, val_loss: 0.00255, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 16 | train_loss: 0.00267, val_loss: 0.00254, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 17 | train_loss: 0.00264, val_loss: 0.00254, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 18 | train_loss: 0.00263, val_loss: 0.00255, lr: 2.00E-03, _patience: 9\n",
      "Epoch: 19 | train_loss: 0.00259, val_loss: 0.00254, lr: 2.00E-03, _patience: 8\n",
      "Epoch: 20 | train_loss: 0.00255, val_loss: 0.00254, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 21 | train_loss: 0.00255, val_loss: 0.00253, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 22 | train_loss: 0.00253, val_loss: 0.00252, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 23 | train_loss: 0.00247, val_loss: 0.00253, lr: 2.00E-03, _patience: 9\n",
      "Epoch: 24 | train_loss: 0.00242, val_loss: 0.00252, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 25 | train_loss: 0.00239, val_loss: 0.00251, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 26 | train_loss: 0.00236, val_loss: 0.00250, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 27 | train_loss: 0.00232, val_loss: 0.00250, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 28 | train_loss: 0.00226, val_loss: 0.00252, lr: 2.00E-03, _patience: 9\n",
      "Epoch: 29 | train_loss: 0.00224, val_loss: 0.00252, lr: 2.00E-03, _patience: 8\n",
      "Epoch: 30 | train_loss: 0.00217, val_loss: 0.00251, lr: 2.00E-03, _patience: 7\n",
      "Epoch: 31 | train_loss: 0.00211, val_loss: 0.00250, lr: 2.00E-03, _patience: 6\n",
      "Epoch: 32 | train_loss: 0.00206, val_loss: 0.00249, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 33 | train_loss: 0.00201, val_loss: 0.00247, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 34 | train_loss: 0.00196, val_loss: 0.00249, lr: 2.00E-03, _patience: 9\n",
      "Epoch: 35 | train_loss: 0.00189, val_loss: 0.00250, lr: 2.00E-03, _patience: 8\n",
      "Epoch: 36 | train_loss: 0.00183, val_loss: 0.00250, lr: 2.00E-03, _patience: 7\n",
      "Epoch: 37 | train_loss: 0.00178, val_loss: 0.00256, lr: 2.00E-03, _patience: 6\n",
      "Epoch: 38 | train_loss: 0.00173, val_loss: 0.00256, lr: 2.00E-03, _patience: 5\n",
      "Epoch: 39 | train_loss: 0.00167, val_loss: 0.00255, lr: 2.00E-04, _patience: 4\n",
      "Epoch: 40 | train_loss: 0.00163, val_loss: 0.00257, lr: 2.00E-04, _patience: 3\n",
      "Epoch: 41 | train_loss: 0.00158, val_loss: 0.00256, lr: 2.00E-04, _patience: 2\n",
      "Epoch: 42 | train_loss: 0.00158, val_loss: 0.00255, lr: 2.00E-04, _patience: 1\n",
      "Stopping early!\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "best_model = trainer.train(\n",
    "    num_epochs, patience, train_dataloader, val_dataloader)"
   ]
  },
  {
   "source": [
    "Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x29385040248>"
      ]
     },
     "metadata": {},
     "execution_count": 154
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 385.78125 262.19625\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 385.78125 262.19625 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 378.58125 224.64 \r\nL 378.58125 7.2 \r\nL 43.78125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m0981855d89\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"57.250754\" xlink:href=\"#m0981855d89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(49.299191 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"118.480596\" xlink:href=\"#m0981855d89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0.2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(110.529034 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.710439\" xlink:href=\"#m0981855d89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 0.4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(171.758876 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"240.940281\" xlink:href=\"#m0981855d89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0.6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(232.988718 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"302.170123\" xlink:href=\"#m0981855d89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0.8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(294.218561 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.399965\" xlink:href=\"#m0981855d89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1.0 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(355.448403 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Threshold -->\r\n     <defs>\r\n      <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n     </defs>\r\n     <g transform=\"translate(186.432031 252.916563)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-84\"/>\r\n      <use x=\"61.083984\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"124.462891\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"165.544922\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"227.068359\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"279.167969\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"342.546875\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"403.728516\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"431.511719\" xlink:href=\"#DejaVuSans-100\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m0d86f6b7b0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m0d86f6b7b0\" y=\"214.846174\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(20.878125 218.645393)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m0d86f6b7b0\" y=\"175.293667\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(20.878125 179.092885)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m0d86f6b7b0\" y=\"135.741159\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(20.878125 139.540378)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m0d86f6b7b0\" y=\"96.188651\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(20.878125 99.98787)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m0d86f6b7b0\" y=\"56.636144\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(20.878125 60.435363)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m0d86f6b7b0\" y=\"17.083636\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 20.882855)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- Performance -->\r\n     <defs>\r\n      <path d=\"M 19.671875 64.796875 \r\nL 19.671875 37.40625 \r\nL 32.078125 37.40625 \r\nQ 38.96875 37.40625 42.71875 40.96875 \r\nQ 46.484375 44.53125 46.484375 51.125 \r\nQ 46.484375 57.671875 42.71875 61.234375 \r\nQ 38.96875 64.796875 32.078125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.34375 72.90625 50.609375 67.359375 \r\nQ 56.890625 61.8125 56.890625 51.125 \r\nQ 56.890625 40.328125 50.609375 34.8125 \r\nQ 44.34375 29.296875 32.078125 29.296875 \r\nL 19.671875 29.296875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-80\"/>\r\n      <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n      <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 147.867656)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"60.255859\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"121.779297\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"162.892578\" xlink:href=\"#DejaVuSans-102\"/>\r\n      <use x=\"198.097656\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"259.279297\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"300.376953\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"397.789062\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"459.068359\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"522.447266\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"577.427734\" xlink:href=\"#DejaVuSans-101\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p4002240743)\" d=\"M 58.999432 193.640822 \r\nL 58.999544 193.64942 \r\nL 59.106481 193.331972 \r\nL 59.275287 192.854788 \r\nL 59.33019 192.678875 \r\nL 59.468908 192.271713 \r\nL 59.731894 191.556943 \r\nL 59.845981 191.252434 \r\nL 60.34857 189.953146 \r\nL 60.417546 189.77945 \r\nL 60.705506 189.097431 \r\nL 60.808576 188.870898 \r\nL 60.948392 188.528239 \r\nL 61.074181 188.247386 \r\nL 61.253717 187.844976 \r\nL 61.482902 187.33837 \r\nL 61.600872 187.123991 \r\nL 61.727302 186.831839 \r\nL 61.848321 186.588851 \r\nL 62.926795 184.340622 \r\nL 63.001811 184.198804 \r\nL 65.186258 180.275654 \r\nL 65.188703 180.286041 \r\nL 65.26662 180.145081 \r\nL 65.46907 179.843955 \r\nL 65.768971 179.303902 \r\nL 65.885451 179.118124 \r\nL 66.114415 178.795014 \r\nL 66.340201 178.422018 \r\nL 66.547958 178.077371 \r\nL 66.654794 177.895489 \r\nL 66.884064 177.43963 \r\nL 67.323912 176.782125 \r\nL 67.457497 176.562635 \r\nL 68.39904 175.051242 \r\nL 68.400103 175.062387 \r\nL 68.563291 174.835326 \r\nL 69.230345 173.74236 \r\nL 69.358051 173.554881 \r\nL 69.541996 173.299587 \r\nL 69.692635 173.07923 \r\nL 71.22291 170.922206 \r\nL 71.466959 170.525195 \r\nL 72.436048 169.395064 \r\nL 72.564589 169.198546 \r\nL 72.718736 168.9441 \r\nL 72.827046 168.820391 \r\nL 72.842258 168.828608 \r\nL 73.120089 168.420417 \r\nL 73.292834 168.077702 \r\nL 73.640903 167.547682 \r\nL 73.776432 167.312404 \r\nL 74.101444 166.834057 \r\nL 74.114187 166.840369 \r\nL 74.118336 166.857967 \r\nL 74.202153 166.785847 \r\nL 74.202915 166.803484 \r\nL 74.328174 166.650763 \r\nL 74.759038 166.092873 \r\nL 74.972543 165.769381 \r\nL 75.241254 165.369425 \r\nL 75.611208 164.91868 \r\nL 75.632701 164.979222 \r\nL 75.712295 164.849615 \r\nL 76.201035 164.156317 \r\nL 76.300095 164.002924 \r\nL 76.797056 163.407683 \r\nL 77.694044 162.350915 \r\nL 77.697965 162.370054 \r\nL 77.781134 162.293891 \r\nL 77.7913 162.292248 \r\nL 78.089293 161.831677 \r\nL 78.59248 161.322303 \r\nL 78.816909 160.959998 \r\nL 79.371748 160.499845 \r\nL 79.381387 160.504659 \r\nL 79.483062 160.403763 \r\nL 79.759822 160.164141 \r\nL 80.078636 159.836237 \r\nL 80.187678 159.674205 \r\nL 80.207224 159.686582 \r\nL 80.391738 159.46478 \r\nL 80.620257 159.221915 \r\nL 80.621018 159.242305 \r\nL 81.033497 158.780509 \r\nL 81.141317 158.617598 \r\nL 81.41103 158.367367 \r\nL 81.676361 158.034602 \r\nL 82.125014 157.456956 \r\nL 82.229038 157.27049 \r\nL 82.235044 157.29152 \r\nL 82.328412 157.139393 \r\nL 82.639883 156.763752 \r\nL 82.724065 156.657714 \r\nL 82.752774 156.661245 \r\nL 82.753308 156.682493 \r\nL 82.859757 156.570657 \r\nL 83.031871 156.42597 \r\nL 84.007615 155.218809 \r\nL 84.009572 155.240539 \r\nL 84.099678 155.099561 \r\nL 84.750893 154.504869 \r\nL 84.87252 154.311156 \r\nL 85.225216 154.013357 \r\nL 85.463055 153.718987 \r\nL 85.476237 153.763457 \r\nL 85.54989 153.63396 \r\nL 85.744025 153.385569 \r\nL 85.876502 153.114569 \r\nL 85.751673 153.397834 \r\nL 85.883275 153.116639 \r\nL 85.971086 153.080049 \r\nL 85.983795 153.059661 \r\nL 86.100461 152.871179 \r\nL 86.327698 152.585765 \r\nL 86.467742 152.379166 \r\nL 86.478038 152.391361 \r\nL 86.523313 152.429757 \r\nL 86.582174 152.345906 \r\nL 87.01087 151.94768 \r\nL 87.186917 151.759182 \r\nL 87.392886 151.51334 \r\nL 87.5636 151.350055 \r\nL 87.687799 151.21022 \r\nL 87.718091 151.22238 \r\nL 87.99321 150.906618 \r\nL 88.175689 150.673404 \r\nL 88.739003 150.022064 \r\nL 88.847751 149.838457 \r\nL 88.866471 149.862036 \r\nL 89.642377 149.068169 \r\nL 89.72291 148.937262 \r\nL 89.743035 148.961155 \r\nL 89.806683 148.99702 \r\nL 89.855909 148.913358 \r\nL 89.944909 148.829443 \r\nL 89.976273 148.853401 \r\nL 90.112804 148.865236 \r\nL 90.123178 148.841169 \r\nL 90.584232 148.511803 \r\nL 90.683416 148.474816 \r\nL 90.691575 148.450289 \r\nL 90.891855 148.151523 \r\nL 91.027159 148.000513 \r\nL 92.186392 146.789473 \r\nL 92.355322 146.488047 \r\nL 92.356679 146.512971 \r\nL 92.403816 146.549688 \r\nL 92.444967 146.457431 \r\nL 92.496739 146.414907 \r\nL 92.536348 146.451703 \r\nL 92.764864 146.248014 \r\nL 92.788817 146.181298 \r\nL 92.862917 146.229966 \r\nL 93.103279 145.929513 \r\nL 93.194379 145.805964 \r\nL 93.21635 145.817687 \r\nL 93.401039 145.623436 \r\nL 93.403092 145.646849 \r\nL 93.504176 145.468853 \r\nL 93.539589 145.556824 \r\nL 93.608071 145.446863 \r\nL 93.735809 145.221467 \r\nL 93.750612 145.23313 \r\nL 93.754855 145.258678 \r\nL 93.834306 145.140715 \r\nL 93.843202 145.166336 \r\nL 94.172552 144.763726 \r\nL 94.323191 144.49143 \r\nL 94.440158 144.271303 \r\nL 94.697004 143.889802 \r\nL 94.844025 143.752411 \r\nL 94.851922 143.778437 \r\nL 94.860069 143.804473 \r\nL 94.931109 143.710329 \r\nL 95.113615 143.520638 \r\nL 95.218744 143.384037 \r\nL 95.561576 143.082845 \r\nL 95.734329 142.819238 \r\nL 95.949148 142.518485 \r\nL 96.420465 142.021884 \r\nL 96.628979 141.679565 \r\nL 96.796673 141.575516 \r\nL 96.830637 141.629235 \r\nL 96.830828 141.656111 \r\nL 96.931466 141.513687 \r\nL 96.938363 141.540597 \r\nL 97.168036 141.351017 \r\nL 97.184664 141.378014 \r\nL 97.237433 141.266118 \r\nL 97.401477 140.923543 \r\nL 97.785285 140.728898 \r\nL 98.293294 140.049383 \r\nL 98.391244 139.848091 \r\nL 98.431472 139.858808 \r\nL 98.586086 139.639448 \r\nL 98.71113 139.563348 \r\nL 99.027544 139.358108 \r\nL 99.042174 139.385929 \r\nL 99.046349 139.413763 \r\nL 99.130266 139.321265 \r\nL 99.130367 139.349141 \r\nL 99.514289 138.948365 \r\nL 99.616824 138.811779 \r\nL 99.632431 138.868056 \r\nL 100.021207 138.428476 \r\nL 100.055417 138.374912 \r\nL 100.097707 138.413751 \r\nL 100.284999 138.214934 \r\nL 100.461338 137.96447 \r\nL 100.293772 138.225403 \r\nL 100.487993 138.021597 \r\nL 100.494462 138.05018 \r\nL 100.579265 137.922914 \r\nL 100.776288 137.761061 \r\nL 100.912412 137.716433 \r\nL 101.022338 137.514137 \r\nL 101.169932 137.555321 \r\nL 101.196231 137.518375 \r\nL 101.274989 137.485565 \r\nL 101.240893 137.578168 \r\nL 101.300682 137.506218 \r\nL 101.309761 137.535117 \r\nL 101.390686 137.433957 \r\nL 101.408655 137.454625 \r\nL 101.484658 137.380049 \r\nL 102.296334 136.84441 \r\nL 102.418522 136.739911 \r\nL 102.518348 136.567194 \r\nL 102.686388 136.248868 \r\nL 102.864661 136.368977 \r\nL 102.885245 136.33003 \r\nL 103.000166 136.125164 \r\nL 103.241175 135.721353 \r\nL 103.396241 135.512647 \r\nL 103.819962 134.98921 \r\nL 103.928756 134.775482 \r\nL 103.829611 134.99886 \r\nL 103.94189 134.835376 \r\nL 104.48571 134.34782 \r\nL 104.596872 134.264939 \r\nL 104.602702 134.29511 \r\nL 104.612328 134.325295 \r\nL 104.675192 134.242263 \r\nL 104.709817 134.272474 \r\nL 104.923966 134.073529 \r\nL 104.931516 134.103798 \r\nL 105.006414 134.12259 \r\nL 105.025502 134.080753 \r\nL 105.330287 133.636095 \r\nL 105.434953 133.548394 \r\nL 105.448652 133.578859 \r\nL 105.453926 133.609339 \r\nL 105.508104 133.545524 \r\nL 105.69182 133.37175 \r\nL 105.912282 132.91598 \r\nL 106.005465 132.912238 \r\nL 106.011286 132.868806 \r\nL 106.051865 132.886746 \r\nL 106.262541 132.46712 \r\nL 106.648229 132.139682 \r\nL 106.759231 132.037103 \r\nL 107.303033 131.651376 \r\nL 107.410021 131.523807 \r\nL 107.428761 131.563533 \r\nL 107.530283 131.381296 \r\nL 107.657882 131.221182 \r\nL 108.949889 130.408862 \r\nL 109.062602 130.098781 \r\nL 109.125931 130.066818 \r\nL 109.104946 130.138726 \r\nL 109.155862 130.098813 \r\nL 109.181031 130.130874 \r\nL 109.19807 129.986432 \r\nL 109.314916 129.793075 \r\nL 109.422943 129.7682 \r\nL 109.353788 129.833088 \r\nL 109.423048 129.800377 \r\nL 109.434508 129.832572 \r\nL 109.482172 129.710985 \r\nL 109.519525 129.7511 \r\nL 109.693638 129.571377 \r\nL 109.702857 129.579199 \r\nL 109.828864 129.316444 \r\nL 109.856268 129.413671 \r\nL 109.927918 129.314877 \r\nL 110.165009 128.982404 \r\nL 110.228256 129.030125 \r\nL 110.55233 128.942891 \r\nL 110.552549 128.975598 \r\nL 110.651744 128.857423 \r\nL 110.661461 128.890182 \r\nL 110.797919 128.713474 \r\nL 110.833557 128.720967 \r\nL 110.90967 128.467135 \r\nL 110.987141 128.499972 \r\nL 111.023746 128.398028 \r\nL 111.128421 128.27026 \r\nL 111.130665 128.277539 \r\nL 111.238584 128.233561 \r\nL 111.23878 128.207906 \r\nL 111.384877 127.905951 \r\nL 111.410607 127.913059 \r\nL 112.482653 126.657214 \r\nL 112.540144 126.69714 \r\nL 112.585576 126.670258 \r\nL 113.049648 125.981291 \r\nL 113.147288 125.999885 \r\nL 113.156088 125.972387 \r\nL 113.225969 126.01232 \r\nL 113.25058 125.929684 \r\nL 113.323914 125.969652 \r\nL 113.37609 125.89299 \r\nL 113.530394 125.661569 \r\nL 113.594275 125.701573 \r\nL 113.616009 125.617895 \r\nL 114.084497 125.0053 \r\nL 114.119272 125.039452 \r\nL 114.14382 124.982576 \r\nL 114.301131 124.794151 \r\nL 114.327992 124.834027 \r\nL 114.441777 124.667466 \r\nL 114.453538 124.701783 \r\nL 114.481174 124.644238 \r\nL 114.500535 124.655333 \r\nL 114.790504 124.364086 \r\nL 114.790965 124.334964 \r\nL 114.839536 124.438622 \r\nL 114.880302 124.414933 \r\nL 115.525685 123.705692 \r\nL 115.534293 123.775586 \r\nL 115.620483 123.656031 \r\nL 115.762484 123.746474 \r\nL 116.175963 123.232202 \r\nL 116.279223 123.307737 \r\nL 116.310596 123.221036 \r\nL 116.649975 122.831199 \r\nL 116.736785 122.623577 \r\nL 116.836533 122.628063 \r\nL 116.8488 122.596982 \r\nL 116.966987 122.418772 \r\nL 117.081858 122.42314 \r\nL 117.08215 122.391853 \r\nL 117.201742 122.01886 \r\nL 117.205985 122.054565 \r\nL 117.291773 122.062864 \r\nL 117.303351 122.031241 \r\nL 118.256996 121.111397 \r\nL 118.363623 121.151106 \r\nL 118.385471 121.085972 \r\nL 118.767491 120.468957 \r\nL 118.802755 120.50838 \r\nL 118.860035 120.409007 \r\nL 119.035703 120.209632 \r\nL 119.199565 120.009414 \r\nL 119.243456 119.875464 \r\nL 119.353719 119.878228 \r\nL 119.361023 119.811065 \r\nL 119.507772 119.645349 \r\nL 119.539045 119.681719 \r\nL 119.566011 119.546542 \r\nL 119.73463 119.216184 \r\nL 119.824789 119.085978 \r\nL 119.805419 119.223171 \r\nL 119.827088 119.122578 \r\nL 119.857439 119.195855 \r\nL 119.882699 119.058278 \r\nL 120.475756 118.200733 \r\nL 120.517534 118.130188 \r\nL 120.553319 118.203999 \r\nL 120.58799 118.133354 \r\nL 120.699065 118.099554 \r\nL 120.982391 117.963608 \r\nL 121.026606 118.002194 \r\nL 121.076181 117.96655 \r\nL 122.211995 116.783952 \r\nL 122.252519 116.821504 \r\nL 122.337175 116.859084 \r\nL 122.364894 116.785187 \r\nL 122.519828 116.6014 \r\nL 122.587441 116.377692 \r\nL 122.724711 116.115409 \r\nL 123.036883 115.622755 \r\nL 123.148213 115.736542 \r\nL 123.180731 115.660186 \r\nL 123.316806 115.429999 \r\nL 123.47121 115.429171 \r\nL 123.512962 115.390682 \r\nL 123.542697 115.466859 \r\nL 123.627362 115.619032 \r\nL 123.657179 115.502843 \r\nL 123.887933 115.188757 \r\nL 123.904612 115.227275 \r\nL 123.922344 115.188147 \r\nL 124.095116 115.030594 \r\nL 124.26676 114.752837 \r\nL 124.268412 114.830138 \r\nL 124.458491 114.510191 \r\nL 124.636563 114.106681 \r\nL 124.68456 114.144033 \r\nL 124.693953 114.103704 \r\nL 124.889954 113.899915 \r\nL 125.301253 113.527832 \r\nL 125.369874 113.281614 \r\nL 125.429531 113.437717 \r\nL 125.532025 113.272892 \r\nL 125.685695 113.098196 \r\nL 125.696785 113.137418 \r\nL 125.707228 113.176672 \r\nL 125.757419 113.093506 \r\nL 125.78276 113.13279 \r\nL 125.988898 112.961123 \r\nL 126.021635 113.039905 \r\nL 126.035102 113.079344 \r\nL 126.067734 112.953706 \r\nL 126.22816 112.862107 \r\nL 126.229036 112.901669 \r\nL 126.342652 112.687978 \r\nL 126.405521 112.594887 \r\nL 126.622032 112.418705 \r\nL 126.719097 112.244456 \r\nL 126.872658 112.195238 \r\nL 126.87727 112.2351 \r\nL 126.989002 112.268797 \r\nL 126.998633 112.225698 \r\nL 127.032282 112.262578 \r\nL 127.118549 112.176191 \r\nL 127.272132 111.952817 \r\nL 127.302862 111.99293 \r\nL 127.317278 112.033078 \r\nL 127.342797 111.858819 \r\nL 127.378449 111.815161 \r\nL 127.428467 111.895545 \r\nL 127.443229 111.935787 \r\nL 127.479652 111.848353 \r\nL 127.515058 111.844852 \r\nL 127.609993 111.62542 \r\nL 127.689453 111.401154 \r\nL 127.720055 111.441478 \r\nL 127.733777 111.477998 \r\nL 127.872648 111.122781 \r\nL 128.13874 111.235986 \r\nL 128.157936 111.191327 \r\nL 128.349115 110.96745 \r\nL 128.46925 110.697518 \r\nL 128.5746 110.64317 \r\nL 128.580093 110.683795 \r\nL 129.9888 109.255105 \r\nL 129.990351 109.296271 \r\nL 130.247286 109.154657 \r\nL 130.31067 109.053849 \r\nL 130.575279 108.987665 \r\nL 130.590584 109.028989 \r\nL 130.617856 109.111748 \r\nL 130.692731 109.016492 \r\nL 130.736271 108.921064 \r\nL 130.762365 109.045448 \r\nL 131.348621 108.497075 \r\nL 131.353694 108.448669 \r\nL 131.395842 108.531917 \r\nL 131.602869 108.282174 \r\nL 131.685906 108.087002 \r\nL 131.709592 108.128689 \r\nL 131.957595 107.918016 \r\nL 132.092743 107.952427 \r\nL 132.224398 107.945042 \r\nL 132.248708 108.028862 \r\nL 132.315806 107.930231 \r\nL 132.962507 107.168295 \r\nL 133.051457 107.244159 \r\nL 132.979359 107.160093 \r\nL 133.073897 107.193924 \r\nL 133.215278 107.118853 \r\nL 133.314893 107.195051 \r\nL 133.345654 107.14451 \r\nL 133.447286 107.034894 \r\nL 133.543996 107.068851 \r\nL 133.555775 107.018061 \r\nL 133.664582 106.754688 \r\nL 133.698108 106.839563 \r\nL 133.73163 106.685999 \r\nL 133.832874 106.634714 \r\nL 133.837782 106.719718 \r\nL 133.896924 106.847526 \r\nL 133.941093 106.744768 \r\nL 133.955244 106.693316 \r\nL 134.038217 106.778699 \r\nL 134.256481 106.502583 \r\nL 134.29742 106.545387 \r\nL 134.433011 106.47534 \r\nL 134.494196 106.518255 \r\nL 134.522654 106.414144 \r\nL 134.702505 106.0386 \r\nL 134.742779 106.081553 \r\nL 134.810989 106.11503 \r\nL 134.817832 106.062427 \r\nL 135.206933 105.332511 \r\nL 135.211148 105.375664 \r\nL 135.220473 105.41886 \r\nL 135.25612 105.311683 \r\nL 135.268428 105.354911 \r\nL 135.97069 104.585712 \r\nL 136.067221 104.409909 \r\nL 136.302784 104.4304 \r\nL 136.549459 103.710177 \r\nL 136.570782 103.753664 \r\nL 136.782959 103.392716 \r\nL 136.893469 103.267311 \r\nL 136.936671 103.310904 \r\nL 137.109825 103.159053 \r\nL 137.245033 103.189807 \r\nL 137.250042 103.132985 \r\nL 137.601707 102.375069 \r\nL 137.644799 102.418831 \r\nL 137.799213 102.187618 \r\nL 137.902724 102.042994 \r\nL 138.183441 101.66326 \r\nL 138.345118 101.428075 \r\nL 138.538801 101.19191 \r\nL 139.094751 100.66433 \r\nL 139.329018 100.776575 \r\nL 139.497537 100.41291 \r\nL 139.936783 99.484694 \r\nL 140.111114 99.048898 \r\nL 140.230082 98.824057 \r\nL 140.348036 98.723793 \r\nL 140.441913 98.857047 \r\nL 140.463564 98.730697 \r\nL 140.503016 98.775191 \r\nL 140.544466 98.711903 \r\nL 141.09611 98.04609 \r\nL 141.219566 97.832107 \r\nL 141.30753 97.921679 \r\nL 141.341918 97.836673 \r\nL 141.391936 97.510741 \r\nL 141.426032 97.600453 \r\nL 141.506934 97.624916 \r\nL 141.515866 97.559392 \r\nL 141.637115 97.362382 \r\nL 141.760598 97.410779 \r\nL 141.94295 97.281755 \r\nL 141.957594 97.327014 \r\nL 141.97242 97.194224 \r\nL 142.629482 96.392297 \r\nL 142.718121 96.551935 \r\nL 142.834917 96.59758 \r\nL 142.854898 96.529621 \r\nL 143.036876 96.257003 \r\nL 143.098654 96.051713 \r\nL 143.156527 96.142952 \r\nL 143.333459 96.234403 \r\nL 143.355831 96.097042 \r\nL 143.97209 94.842381 \r\nL 144.134022 94.81756 \r\nL 144.145272 94.746996 \r\nL 144.778092 94.336459 \r\nL 145.106892 93.945375 \r\nL 145.213149 93.99129 \r\nL 145.220923 93.919051 \r\nL 145.338823 93.820238 \r\nL 145.556548 93.721068 \r\nL 145.576821 93.767069 \r\nL 145.639548 93.813125 \r\nL 146.278828 93.450955 \r\nL 146.281565 93.377203 \r\nL 146.371244 93.470018 \r\nL 146.709862 92.922848 \r\nL 146.737078 92.969261 \r\nL 146.799377 93.015731 \r\nL 146.823336 92.94108 \r\nL 147.001217 92.98759 \r\nL 147.009666 92.91283 \r\nL 147.452798 92.584113 \r\nL 147.466137 92.630632 \r\nL 147.524175 92.601842 \r\nL 147.624338 92.8355 \r\nL 147.781014 92.579577 \r\nL 147.920711 92.398305 \r\nL 148.020774 92.245722 \r\nL 148.209046 92.386475 \r\nL 148.222495 92.309986 \r\nL 148.740645 91.959767 \r\nL 148.742935 92.006889 \r\nL 148.822769 91.821655 \r\nL 148.955313 91.760572 \r\nL 148.966572 91.807836 \r\nL 148.993488 91.651894 \r\nL 149.040577 91.668365 \r\nL 149.147847 91.684899 \r\nL 149.209716 91.449409 \r\nL 149.293073 91.496836 \r\nL 150.191746 90.457521 \r\nL 150.216609 90.505229 \r\nL 150.312684 90.309922 \r\nL 151.309002 88.420824 \r\nL 151.79749 88.538469 \r\nL 151.804132 88.453584 \r\nL 152.419278 87.00923 \r\nL 152.450884 87.056993 \r\nL 152.45561 86.969646 \r\nL 152.57101 86.619059 \r\nL 152.759511 86.354855 \r\nL 152.837822 86.178113 \r\nL 152.933924 86.273285 \r\nL 152.986341 86.320969 \r\nL 153.042189 86.232331 \r\nL 153.193154 85.882523 \r\nL 153.207524 85.930234 \r\nL 153.300954 85.798903 \r\nL 153.340232 85.846688 \r\nL 153.357066 85.89454 \r\nL 153.402421 85.714817 \r\nL 153.445523 85.810603 \r\nL 153.533679 85.720495 \r\nL 153.545394 85.63026 \r\nL 153.622491 85.774266 \r\nL 153.64011 85.683816 \r\nL 153.648367 85.731923 \r\nL 153.71499 85.550582 \r\nL 153.902022 85.277615 \r\nL 154.170321 84.867969 \r\nL 154.281122 84.591811 \r\nL 154.370793 84.639656 \r\nL 154.410436 84.547311 \r\nL 154.529686 83.990478 \r\nL 154.558929 84.038166 \r\nL 154.597833 84.133747 \r\nL 154.645113 84.040381 \r\nL 154.788761 84.232234 \r\nL 154.820202 84.04484 \r\nL 154.91032 83.668433 \r\nL 154.940812 83.71637 \r\nL 155.162989 83.337656 \r\nL 155.311956 83.004482 \r\nL 155.413834 83.196113 \r\nL 155.441763 83.100367 \r\nL 155.614662 82.763895 \r\nL 155.672727 82.811872 \r\nL 155.707781 82.715356 \r\nL 155.756621 82.569839 \r\nL 156.071571 82.761769 \r\nL 156.273594 82.173753 \r\nL 156.49953 81.776449 \r\nL 156.663232 81.674241 \r\nL 156.691362 81.722551 \r\nL 156.721918 81.622908 \r\nL 156.782638 81.423173 \r\nL 156.794864 81.471439 \r\nL 157.365567 80.660852 \r\nL 157.513677 80.553707 \r\nL 157.537773 80.650166 \r\nL 157.541714 80.548116 \r\nL 157.621285 80.445911 \r\nL 157.875177 80.488419 \r\nL 157.945586 80.173526 \r\nL 158.083094 79.855721 \r\nL 158.103796 79.904119 \r\nL 158.209506 80.098459 \r\nL 158.335973 79.680312 \r\nL 158.496327 79.57537 \r\nL 158.534465 79.672558 \r\nL 158.574309 79.721265 \r\nL 158.582083 79.616028 \r\nL 158.60617 79.664767 \r\nL 158.708094 79.126785 \r\nL 158.759927 79.224261 \r\nL 159.020015 79.001403 \r\nL 159.11296 79.050272 \r\nL 159.136007 78.943008 \r\nL 159.151254 78.835575 \r\nL 159.192439 78.933361 \r\nL 159.436058 78.717798 \r\nL 159.554724 78.550371 \r\nL 159.751719 78.055154 \r\nL 159.808516 78.152708 \r\nL 159.907183 77.933648 \r\nL 159.924783 77.823854 \r\nL 160.013312 77.872603 \r\nL 160.623003 77.13299 \r\nL 160.709242 77.230394 \r\nL 160.746578 77.167462 \r\nL 160.991957 76.992158 \r\nL 161.117202 77.138872 \r\nL 161.117786 77.026366 \r\nL 161.322847 76.347451 \r\nL 161.662386 76.282291 \r\nL 161.733598 76.216916 \r\nL 161.771855 76.314495 \r\nL 161.775769 76.200101 \r\nL 161.837192 76.297895 \r\nL 162.1407 75.953248 \r\nL 162.178619 76.002102 \r\nL 162.35358 76.051038 \r\nL 162.394483 75.704339 \r\nL 162.459309 75.802121 \r\nL 162.616514 75.618702 \r\nL 162.675373 75.667685 \r\nL 162.682043 75.55112 \r\nL 163.008927 75.249089 \r\nL 163.066855 75.34701 \r\nL 163.079975 75.396095 \r\nL 163.506292 75.524894 \r\nL 163.521994 75.406825 \r\nL 163.556829 75.100707 \r\nL 163.810886 75.150084 \r\nL 164.568466 73.782395 \r\nL 164.718337 73.929665 \r\nL 164.732296 73.807553 \r\nL 165.075731 73.044389 \r\nL 165.087392 73.093306 \r\nL 165.286175 73.142308 \r\nL 165.561089 72.617307 \r\nL 165.739846 72.491995 \r\nL 165.760776 72.589803 \r\nL 165.846103 72.737165 \r\nL 165.85135 72.611293 \r\nL 165.879826 72.485198 \r\nL 165.89238 72.534357 \r\nL 166.365357 72.301901 \r\nL 166.498366 72.450071 \r\nL 166.887421 72.116299 \r\nL 167.045201 71.859591 \r\nL 167.124388 71.958286 \r\nL 167.287424 72.007768 \r\nL 167.397468 71.539987 \r\nL 167.512959 71.638819 \r\nL 167.831166 71.558226 \r\nL 167.855454 71.607794 \r\nL 167.907014 71.47734 \r\nL 168.293806 71.726167 \r\nL 168.322665 71.595105 \r\nL 168.406934 71.332259 \r\nL 168.667943 71.068447 \r\nL 168.672541 70.936177 \r\nL 169.084223 70.636708 \r\nL 169.281821 70.150835 \r\nL 169.391244 70.01636 \r\nL 169.435213 70.0656 \r\nL 169.478743 70.114931 \r\nL 169.480066 69.980047 \r\nL 169.657682 69.709522 \r\nL 169.807571 69.758705 \r\nL 169.856064 69.622982 \r\nL 170.130787 69.721463 \r\nL 170.554649 69.448928 \r\nL 170.803806 68.584297 \r\nL 170.848924 68.44566 \r\nL 170.904973 68.494391 \r\nL 171.207323 68.173863 \r\nL 171.854348 68.574365 \r\nL 171.85453 68.624066 \r\nL 171.90369 68.482789 \r\nL 172.085732 68.532498 \r\nL 172.243375 68.248944 \r\nL 172.266532 67.964289 \r\nL 172.392662 68.013736 \r\nL 172.638789 67.539181 \r\nL 172.639912 67.588551 \r\nL 172.708433 67.638017 \r\nL 172.983848 67.108215 \r\nL 173.200223 67.011851 \r\nL 173.23202 67.061138 \r\nL 173.59061 66.573368 \r\nL 173.726666 66.327296 \r\nL 173.746045 66.376393 \r\nL 174.041634 65.979879 \r\nL 174.042328 66.028874 \r\nL 174.091205 66.077966 \r\nL 174.477805 65.977691 \r\nL 174.50889 66.02688 \r\nL 174.824497 65.775926 \r\nL 174.867152 65.82511 \r\nL 174.93122 65.923777 \r\nL 175.020215 65.772893 \r\nL 175.077185 65.871755 \r\nL 175.09567 65.921336 \r\nL 175.17191 65.769836 \r\nL 175.21357 65.869094 \r\nL 175.268889 65.918875 \r\nL 175.371962 65.61432 \r\nL 175.556731 64.995179 \r\nL 175.705169 65.044623 \r\nL 175.855577 64.734796 \r\nL 175.902101 64.784124 \r\nL 175.964253 64.833555 \r\nL 176.174642 64.052239 \r\nL 176.215654 64.101216 \r\nL 176.308792 63.676904 \r\nL 176.483196 63.774687 \r\nL 176.66167 63.297619 \r\nL 177.111372 63.44402 \r\nL 177.308586 63.591352 \r\nL 177.535626 63.430954 \r\nL 177.964754 63.529461 \r\nL 177.986815 63.207168 \r\nL 178.069022 63.25634 \r\nL 178.23818 63.354999 \r\nL 178.446398 62.917014 \r\nL 178.546433 63.015581 \r\nL 178.904257 63.114572 \r\nL 179.033935 62.622469 \r\nL 179.058114 62.671754 \r\nL 179.521137 61.872307 \r\nL 179.560087 61.704757 \r\nL 179.873577 61.732667 \r\nL 180.180543 61.103869 \r\nL 180.190461 61.152672 \r\nL 180.664651 61.030867 \r\nL 180.77715 61.079752 \r\nL 181.052894 60.736906 \r\nL 181.078669 60.785626 \r\nL 181.173102 60.613491 \r\nL 181.186998 60.662182 \r\nL 181.254406 60.710982 \r\nL 181.261723 60.538185 \r\nL 181.348017 60.684826 \r\nL 181.445188 60.783137 \r\nL 181.670057 60.484012 \r\nL 181.91384 60.309197 \r\nL 181.955627 60.407325 \r\nL 182.011028 60.5059 \r\nL 182.016676 60.330114 \r\nL 182.15952 60.026359 \r\nL 182.172504 60.075492 \r\nL 182.227056 59.898206 \r\nL 182.420739 59.720514 \r\nL 182.429845 59.542413 \r\nL 182.486888 59.591216 \r\nL 182.782349 59.68916 \r\nL 183.027419 59.248534 \r\nL 183.425342 59.29745 \r\nL 183.467412 59.116789 \r\nL 183.482303 59.214654 \r\nL 183.594062 59.263758 \r\nL 183.748312 58.766607 \r\nL 183.800309 58.448774 \r\nL 183.997405 58.264777 \r\nL 184.024667 58.313282 \r\nL 184.397381 58.459486 \r\nL 184.47779 58.088902 \r\nL 184.69193 57.951429 \r\nL 184.720378 58.000023 \r\nL 185.101449 56.872985 \r\nL 185.240845 56.541066 \r\nL 185.303746 56.636144 \r\nL 185.673348 56.253994 \r\nL 185.6953 56.301358 \r\nL 185.738575 55.917007 \r\nL 185.795773 56.011378 \r\nL 186.18147 55.671449 \r\nL 186.211315 55.282268 \r\nL 186.245073 55.329023 \r\nL 186.336961 55.133536 \r\nL 186.352225 55.180223 \r\nL 186.422033 55.227025 \r\nL 186.654593 54.880422 \r\nL 186.672622 54.927085 \r\nL 186.860904 55.020757 \r\nL 186.875384 54.822708 \r\nL 187.616121 54.905722 \r\nL 187.620309 54.705546 \r\nL 188.014355 54.303687 \r\nL 188.01763 54.102001 \r\nL 188.20354 53.899807 \r\nL 188.21208 53.697104 \r\nL 188.238558 53.743274 \r\nL 188.378474 53.789562 \r\nL 188.658999 53.381824 \r\nL 188.795083 53.427829 \r\nL 189.040243 52.696047 \r\nL 189.239255 52.741588 \r\nL 189.548703 51.698443 \r\nL 189.760142 51.877563 \r\nL 189.974692 51.711318 \r\nL 190.039189 51.846582 \r\nL 190.418618 51.634067 \r\nL 190.432733 51.679172 \r\nL 191.095196 51.815196 \r\nL 191.39271 51.171653 \r\nL 191.782321 51.261595 \r\nL 191.997245 50.61159 \r\nL 192.050857 50.393772 \r\nL 192.083877 50.438009 \r\nL 192.227433 50.219381 \r\nL 192.339065 49.339024 \r\nL 192.381729 49.117461 \r\nL 192.452038 49.203458 \r\nL 192.71364 49.376844 \r\nL 192.758594 49.153237 \r\nL 192.79561 49.240147 \r\nL 193.00141 49.015444 \r\nL 193.161334 48.337661 \r\nL 193.189491 48.423055 \r\nL 193.340228 48.465927 \r\nL 193.552652 48.238009 \r\nL 193.574513 48.009465 \r\nL 194.144331 48.180091 \r\nL 194.881054 47.339379 \r\nL 194.883189 47.106083 \r\nL 194.988398 47.189944 \r\nL 195.021518 47.274276 \r\nL 195.602924 47.530129 \r\nL 195.890146 47.293472 \r\nL 196.400649 47.336263 \r\nL 196.43033 47.09866 \r\nL 196.517272 47.184051 \r\nL 196.558202 47.226929 \r\nL 196.779084 46.987815 \r\nL 196.816821 46.748017 \r\nL 197.385581 47.004481 \r\nL 197.414367 47.047657 \r\nL 197.490506 46.805174 \r\nL 197.92915 46.934585 \r\nL 198.061839 46.690528 \r\nL 198.06339 46.733624 \r\nL 198.390886 46.907269 \r\nL 198.539862 46.661025 \r\nL 198.669166 46.704457 \r\nL 198.821381 46.45719 \r\nL 198.863397 46.543837 \r\nL 199.126742 46.295237 \r\nL 199.328892 46.38179 \r\nL 199.379932 46.131843 \r\nL 199.459912 46.175005 \r\nL 199.872306 46.305265 \r\nL 199.948254 46.053335 \r\nL 200.573665 45.420652 \r\nL 200.574851 45.463457 \r\nL 200.578227 45.207205 \r\nL 200.632606 45.292557 \r\nL 200.825404 44.776412 \r\nL 201.103311 44.818626 \r\nL 201.318161 44.340341 \r\nL 201.942377 44.424079 \r\nL 202.107539 43.898896 \r\nL 202.73347 44.023425 \r\nL 202.751015 43.758583 \r\nL 203.264402 43.492919 \r\nL 203.853527 43.61652 \r\nL 203.917358 43.783131 \r\nL 203.93713 43.514164 \r\nL 204.623178 43.723598 \r\nL 204.695686 43.808304 \r\nL 205.023136 43.89355 \r\nL 205.02966 43.620463 \r\nL 205.083938 43.662921 \r\nL 205.487089 43.920556 \r\nL 205.605993 43.643977 \r\nL 205.947867 43.687025 \r\nL 206.068641 43.172831 \r\nL 206.745101 43.473695 \r\nL 206.812937 43.190902 \r\nL 206.847909 43.234055 \r\nL 207.325375 43.495995 \r\nL 207.404799 43.209457 \r\nL 207.459725 43.253218 \r\nL 207.805177 43.297127 \r\nL 207.915723 42.719521 \r\nL 208.144433 42.429257 \r\nL 208.203766 42.138012 \r\nL 208.236448 42.180405 \r\nL 209.024019 42.30845 \r\nL 209.114556 41.761697 \r\nL 209.315903 40.869509 \r\nL 209.756645 41.034117 \r\nL 210.001998 40.732883 \r\nL 210.363571 40.815141 \r\nL 210.400724 40.511951 \r\nL 210.466161 40.552838 \r\nL 211.147392 40.801189 \r\nL 211.204818 40.493689 \r\nL 211.64452 40.576704 \r\nL 211.740668 40.267167 \r\nL 211.747511 40.308418 \r\nL 212.455676 40.038217 \r\nL 213.126269 40.203357 \r\nL 213.161852 39.888686 \r\nL 213.526573 39.92985 \r\nL 213.549109 39.613546 \r\nL 214.038099 39.736436 \r\nL 214.057332 39.417475 \r\nL 214.075434 39.45823 \r\nL 214.2518 39.540188 \r\nL 214.521486 38.895681 \r\nL 214.565518 38.93585 \r\nL 214.741811 38.976168 \r\nL 215.219578 38.691025 \r\nL 215.346365 38.731113 \r\nL 215.658842 37.414365 \r\nL 215.868876 37.082095 \r\nL 216.021191 37.119616 \r\nL 216.15836 36.450213 \r\nL 216.161663 36.486753 \r\nL 216.678244 36.56025 \r\nL 216.932802 36.221946 \r\nL 217.324694 36.331517 \r\nL 217.52418 35.990189 \r\nL 217.74706 36.062767 \r\nL 217.949704 35.096891 \r\nL 218.486922 35.202231 \r\nL 218.52944 34.851364 \r\nL 218.567286 34.886135 \r\nL 218.640186 34.921042 \r\nL 219.192952 35.062049 \r\nL 219.289046 35.09765 \r\nL 219.347567 34.741006 \r\nL 219.428642 34.846739 \r\nL 219.684697 34.953745 \r\nL 219.781064 34.989701 \r\nL 219.86568 34.627087 \r\nL 220.047009 34.662529 \r\nL 220.150986 34.297784 \r\nL 220.21573 34.332702 \r\nL 220.335509 34.402962 \r\nL 220.374395 34.034711 \r\nL 221.268233 34.209464 \r\nL 221.5502 34.351883 \r\nL 221.629158 33.975853 \r\nL 221.756693 34.011119 \r\nL 221.883753 33.632803 \r\nL 222.214898 33.702337 \r\nL 222.940691 33.737324 \r\nL 223.039138 33.355238 \r\nL 223.557635 33.424185 \r\nL 223.649385 33.039 \r\nL 224.357988 33.244876 \r\nL 224.54888 32.853494 \r\nL 224.851594 32.921762 \r\nL 224.860225 32.527132 \r\nL 225.399251 32.594424 \r\nL 226.027307 31.395399 \r\nL 226.146576 31.426853 \r\nL 226.508395 30.617055 \r\nL 226.602025 30.67707 \r\nL 226.716732 30.267806 \r\nL 227.2349 30.297169 \r\nL 227.367033 29.885229 \r\nL 227.407853 29.942635 \r\nL 228.544388 30.029717 \r\nL 228.950623 29.219065 \r\nL 229.806396 29.41525 \r\nL 229.875847 28.986011 \r\nL 229.886413 29.013627 \r\nL 230.994007 29.239253 \r\nL 231.069133 28.799427 \r\nL 231.144972 28.855216 \r\nL 231.593487 28.939904 \r\nL 231.706059 28.493014 \r\nL 232.970329 28.802898 \r\nL 233.225873 27.960576 \r\nL 233.260325 27.987836 \r\nL 233.271894 27.518343 \r\nL 234.333868 27.6511 \r\nL 234.338685 27.678058 \r\nL 235.195662 27.814937 \r\nL 235.531989 26.843346 \r\nL 236.033661 26.868762 \r\nL 236.044062 26.377959 \r\nL 236.142309 26.426748 \r\nL 236.301723 25.930908 \r\nL 237.801446 26.00131 \r\nL 238.081643 25.499063 \r\nL 238.712455 25.544066 \r\nL 239.044895 25.036553 \r\nL 240.254002 25.188658 \r\nL 240.395624 25.233191 \r\nL 240.9185 25.255642 \r\nL 241.020013 24.731911 \r\nL 242.250452 24.860815 \r\nL 242.398297 24.325645 \r\nL 242.449299 24.346102 \r\nL 244.263963 24.600926 \r\nL 244.510693 24.043022 \r\nL 250.592286 24.522979 \r\nL 250.757831 23.924479 \r\nL 251.686522 24.033757 \r\nL 251.721065 23.422179 \r\nL 254.628943 23.764803 \r\nL 255.183333 23.11707 \r\nL 259.020541 23.328769 \r\nL 259.387635 22.654412 \r\nL 262.075242 22.96505 \r\nL 262.539177 22.268422 \r\nL 262.710762 21.544445 \r\nL 266.790084 21.773657 \r\nL 269.985677 22.048386 \r\nL 270.343701 20.421401 \r\nL 282.167492 20.815005 \r\nL 284.234634 21.078839 \r\nL 294.581494 21.527738 \r\nL 297.932286 21.907113 \r\nL 297.975698 21.936705 \r\nL 299.484234 22.058795 \r\nL 304.92319 22.220326 \r\nL 305.024046 22.253899 \r\nL 309.206614 22.501788 \r\nL 309.240811 22.539155 \r\nL 309.386721 21.203689 \r\nL 309.574547 21.232501 \r\nL 310.103627 19.869024 \r\nL 310.309427 19.908815 \r\nL 311.906393 19.94976 \r\nL 312.438813 18.527159 \r\nL 316.442095 18.628656 \r\nL 316.585177 17.083636 \r\nL 363.363068 17.083636 \r\nL 363.363068 17.083636 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p4002240743)\" d=\"M 58.999432 17.083636 \r\nL 59.110341 17.173447 \r\nL 59.649442 17.263257 \r\nL 59.7601 17.353068 \r\nL 60.529605 17.442878 \r\nL 60.640332 17.532688 \r\nL 60.740807 17.622499 \r\nL 60.851537 17.80212 \r\nL 61.376169 17.89193 \r\nL 61.487255 17.981741 \r\nL 61.776729 18.071551 \r\nL 61.887148 18.251172 \r\nL 62.26567 18.340982 \r\nL 62.376415 18.520603 \r\nL 62.939719 18.610413 \r\nL 63.052302 18.790034 \r\nL 63.17419 18.879845 \r\nL 63.283938 19.059466 \r\nL 63.305895 19.149276 \r\nL 63.414985 19.328897 \r\nL 63.470845 19.418707 \r\nL 63.581948 19.598328 \r\nL 64.04993 19.688138 \r\nL 64.159677 19.777949 \r\nL 64.388473 19.867759 \r\nL 64.499366 19.95757 \r\nL 64.861578 20.04738 \r\nL 64.973516 20.227001 \r\nL 65.100377 20.316811 \r\nL 65.211642 20.496432 \r\nL 65.406042 20.586243 \r\nL 65.53287 20.855674 \r\nL 65.793111 20.945484 \r\nL 65.900004 21.035295 \r\nL 65.943608 21.125105 \r\nL 66.054813 21.394536 \r\nL 66.696719 21.484347 \r\nL 66.804956 21.574157 \r\nL 66.98655 21.663968 \r\nL 67.102755 22.023209 \r\nL 67.230548 22.11302 \r\nL 67.343931 22.382451 \r\nL 67.471595 22.472261 \r\nL 67.583055 22.562072 \r\nL 67.688048 22.651882 \r\nL 67.801075 22.921313 \r\nL 68.268731 23.011124 \r\nL 68.377661 23.190745 \r\nL 68.488314 23.280555 \r\nL 68.59883 23.549986 \r\nL 68.748629 23.639797 \r\nL 68.860564 23.819418 \r\nL 68.907386 23.909228 \r\nL 69.016821 23.999038 \r\nL 69.321057 24.088849 \r\nL 69.437746 24.26847 \r\nL 69.562274 24.35828 \r\nL 69.672532 24.448091 \r\nL 69.778191 24.537901 \r\nL 69.895877 24.807332 \r\nL 70.003168 24.897143 \r\nL 70.115865 25.076763 \r\nL 70.257999 25.166574 \r\nL 70.365484 25.346195 \r\nL 70.518115 25.436005 \r\nL 70.634066 25.795247 \r\nL 70.800756 25.885057 \r\nL 70.909586 26.064678 \r\nL 71.010233 26.154488 \r\nL 71.12613 26.334109 \r\nL 71.239047 26.42392 \r\nL 71.354707 26.51373 \r\nL 71.416752 26.603541 \r\nL 71.531143 26.693351 \r\nL 71.567226 26.783161 \r\nL 71.689893 27.052593 \r\nL 71.768605 27.142403 \r\nL 71.888712 27.501645 \r\nL 72.135347 27.591455 \r\nL 72.246451 27.771076 \r\nL 72.754185 27.860886 \r\nL 72.873695 28.399749 \r\nL 73.13866 28.489559 \r\nL 73.249933 28.57937 \r\nL 73.375888 28.66918 \r\nL 73.497038 28.848801 \r\nL 73.659088 28.938611 \r\nL 73.771017 29.028422 \r\nL 74.025645 29.118232 \r\nL 74.137411 29.387663 \r\nL 74.150409 29.477474 \r\nL 74.264039 29.746905 \r\nL 74.359328 29.836716 \r\nL 74.469757 30.016336 \r\nL 75.082533 30.106147 \r\nL 75.205666 30.285768 \r\nL 75.379064 30.375578 \r\nL 75.49177 30.645009 \r\nL 75.616261 30.73482 \r\nL 75.753674 31.094061 \r\nL 75.886624 31.183872 \r\nL 75.996144 31.363493 \r\nL 76.12636 31.722734 \r\nL 76.462912 31.812545 \r\nL 76.578592 31.992166 \r\nL 76.739082 32.081976 \r\nL 76.865613 32.351407 \r\nL 77.07082 32.441218 \r\nL 77.177714 32.620838 \r\nL 77.318479 33.159701 \r\nL 77.332429 33.249511 \r\nL 77.461196 33.429132 \r\nL 77.500488 33.518943 \r\nL 77.632386 33.878184 \r\nL 77.781134 33.967995 \r\nL 77.890599 34.057805 \r\nL 77.973358 34.147616 \r\nL 78.109405 34.327236 \r\nL 78.167201 34.417047 \r\nL 78.286049 34.686478 \r\nL 78.355916 34.776288 \r\nL 78.473125 35.04572 \r\nL 78.487294 35.13553 \r\nL 78.604184 35.315151 \r\nL 78.789667 35.404961 \r\nL 78.903807 35.674393 \r\nL 78.994579 35.764203 \r\nL 79.107792 36.123445 \r\nL 79.138414 36.213255 \r\nL 79.247623 36.392876 \r\nL 79.289007 36.482686 \r\nL 79.397801 36.841928 \r\nL 79.547103 36.931738 \r\nL 79.652848 37.29098 \r\nL 79.833352 37.380791 \r\nL 79.963886 37.560411 \r\nL 80.09804 37.650222 \r\nL 80.235673 38.009463 \r\nL 80.322831 38.099274 \r\nL 80.444926 38.458516 \r\nL 80.521715 38.548326 \r\nL 80.63326 38.907568 \r\nL 80.918555 38.997378 \r\nL 81.035777 39.087188 \r\nL 81.211181 39.176999 \r\nL 81.322783 39.35662 \r\nL 81.559648 39.44643 \r\nL 81.668084 39.715861 \r\nL 81.740991 39.805672 \r\nL 81.892624 40.075103 \r\nL 82.06012 40.164913 \r\nL 82.170732 40.344534 \r\nL 82.200296 40.434345 \r\nL 82.307561 40.703776 \r\nL 82.442589 40.793586 \r\nL 82.567882 41.063018 \r\nL 82.724065 41.152828 \r\nL 82.830759 41.332449 \r\nL 82.944035 41.422259 \r\nL 82.99439 41.51207 \r\nL 83.111352 41.691691 \r\nL 83.284705 41.781501 \r\nL 83.444943 42.230553 \r\nL 83.480905 42.320363 \r\nL 83.59802 42.410174 \r\nL 83.780877 42.499984 \r\nL 83.882737 42.589795 \r\nL 84.007615 42.679605 \r\nL 84.121981 42.859226 \r\nL 84.300145 42.949036 \r\nL 84.407178 43.218468 \r\nL 84.54333 43.308278 \r\nL 84.65779 43.487899 \r\nL 84.899053 43.577709 \r\nL 85.042732 43.75733 \r\nL 85.259116 43.847141 \r\nL 85.372189 44.026761 \r\nL 85.430158 44.116572 \r\nL 85.541726 44.386003 \r\nL 85.745854 44.475813 \r\nL 85.85744 44.565624 \r\nL 85.883275 44.655434 \r\nL 86.131417 45.104486 \r\nL 86.231372 45.194297 \r\nL 86.358594 45.284107 \r\nL 86.467742 45.373918 \r\nL 86.604729 45.91278 \r\nL 86.634046 46.002591 \r\nL 86.742941 46.182211 \r\nL 86.785397 46.272022 \r\nL 86.906941 46.451643 \r\nL 86.958467 46.541453 \r\nL 87.084234 46.810884 \r\nL 87.309287 46.900695 \r\nL 87.392886 47.080316 \r\nL 87.672994 47.170126 \r\nL 87.793656 47.349747 \r\nL 87.919242 47.619178 \r\nL 88.050547 47.708988 \r\nL 88.197617 47.888609 \r\nL 88.320185 47.97842 \r\nL 88.475799 48.247851 \r\nL 88.575444 48.337661 \r\nL 88.686948 48.517282 \r\nL 88.847751 48.607093 \r\nL 88.973725 48.696903 \r\nL 89.000671 48.786713 \r\nL 89.133741 48.966334 \r\nL 89.195962 49.056145 \r\nL 89.320502 49.325576 \r\nL 89.64643 49.415386 \r\nL 89.775826 49.774628 \r\nL 89.847835 49.864438 \r\nL 89.944909 49.954249 \r\nL 90.275399 50.582922 \r\nL 90.33869 50.672732 \r\nL 90.584232 51.570836 \r\nL 90.781966 52.019888 \r\nL 90.894038 52.28932 \r\nL 90.981603 52.37913 \r\nL 91.092041 52.648561 \r\nL 91.120584 52.738372 \r\nL 91.237233 53.097613 \r\nL 91.339114 53.187424 \r\nL 91.479981 53.277234 \r\nL 91.639771 53.367045 \r\nL 91.743248 53.456855 \r\nL 91.961801 53.546666 \r\nL 92.110556 53.636476 \r\nL 92.356679 53.726286 \r\nL 92.571418 54.265149 \r\nL 92.618929 54.354959 \r\nL 92.731359 54.53458 \r\nL 92.800523 54.624391 \r\nL 92.974049 54.983632 \r\nL 93.117259 55.073443 \r\nL 93.194379 55.163253 \r\nL 93.32154 55.342874 \r\nL 93.40163 55.432684 \r\nL 93.504176 55.612305 \r\nL 93.676231 56.151168 \r\nL 93.750612 56.240978 \r\nL 93.873701 56.779841 \r\nL 93.940489 56.869651 \r\nL 94.047132 56.959461 \r\nL 94.290732 57.049272 \r\nL 94.401118 57.318703 \r\nL 94.543217 57.408513 \r\nL 94.655529 57.498324 \r\nL 94.79398 57.588134 \r\nL 94.887003 57.947376 \r\nL 95.073408 58.216807 \r\nL 95.090995 58.306618 \r\nL 95.201376 58.486238 \r\nL 95.265399 58.576049 \r\nL 95.365861 58.665859 \r\nL 95.401088 58.75567 \r\nL 95.535705 58.935291 \r\nL 95.674535 59.025101 \r\nL 95.758193 59.114911 \r\nL 95.882927 59.204722 \r\nL 95.986141 59.384343 \r\nL 96.09135 59.474153 \r\nL 96.245116 60.013016 \r\nL 96.479633 60.102826 \r\nL 96.628979 60.372257 \r\nL 96.786162 60.462068 \r\nL 96.909878 60.821309 \r\nL 96.948664 60.91112 \r\nL 97.051076 61.00093 \r\nL 97.068927 61.090741 \r\nL 97.168036 61.180551 \r\nL 97.325858 61.449982 \r\nL 97.367326 61.539793 \r\nL 97.47987 61.719413 \r\nL 97.529441 61.809224 \r\nL 97.649325 62.168466 \r\nL 97.698895 62.258276 \r\nL 97.815723 62.437897 \r\nL 97.983225 62.527707 \r\nL 98.099547 62.886949 \r\nL 98.391244 62.976759 \r\nL 98.53689 63.06657 \r\nL 98.610716 63.15638 \r\nL 98.840252 63.605432 \r\nL 98.914106 63.695243 \r\nL 99.164071 64.503536 \r\nL 99.202802 64.593347 \r\nL 99.315893 64.862778 \r\nL 99.457607 65.22202 \r\nL 99.570862 65.31183 \r\nL 99.754363 65.760882 \r\nL 99.77684 65.850693 \r\nL 99.887605 66.030313 \r\nL 99.969812 66.120124 \r\nL 100.084641 66.389555 \r\nL 100.142309 66.479366 \r\nL 100.244593 66.748797 \r\nL 100.293233 66.838607 \r\nL 100.395102 67.018228 \r\nL 100.476246 67.108038 \r\nL 100.610989 67.287659 \r\nL 100.678155 67.37747 \r\nL 100.776288 67.46728 \r\nL 100.92733 67.557091 \r\nL 101.141168 67.646901 \r\nL 101.282247 68.455195 \r\nL 101.359108 68.545005 \r\nL 101.484658 68.814436 \r\nL 101.647671 68.904247 \r\nL 101.842664 69.353299 \r\nL 101.903087 69.443109 \r\nL 102.160355 69.892161 \r\nL 102.199739 69.981972 \r\nL 102.304984 70.071782 \r\nL 102.562608 70.161593 \r\nL 102.683733 70.251403 \r\nL 102.709609 70.341213 \r\nL 102.864661 71.149507 \r\nL 103.170838 71.239318 \r\nL 103.276521 71.329128 \r\nL 103.428763 71.418938 \r\nL 103.582443 71.77818 \r\nL 103.928756 71.867991 \r\nL 104.070679 72.227232 \r\nL 104.179852 72.406853 \r\nL 104.326661 72.496664 \r\nL 104.436924 72.676284 \r\nL 104.602702 72.766095 \r\nL 104.732714 72.945716 \r\nL 104.923966 73.035526 \r\nL 105.052764 73.394768 \r\nL 105.337815 73.484578 \r\nL 105.448652 73.754009 \r\nL 105.508104 73.84382 \r\nL 105.593336 73.93363 \r\nL 105.710834 74.023441 \r\nL 105.780208 74.113251 \r\nL 105.893304 74.203061 \r\nL 106.002942 74.292872 \r\nL 106.118871 74.652114 \r\nL 106.334187 74.741924 \r\nL 106.461987 75.011355 \r\nL 106.846169 75.101166 \r\nL 107.533281 76.268701 \r\nL 107.875005 76.358511 \r\nL 108.027918 76.627943 \r\nL 108.116694 76.717753 \r\nL 108.300961 77.076995 \r\nL 108.88057 78.513961 \r\nL 109.081082 78.603772 \r\nL 109.272093 79.232445 \r\nL 109.341818 79.322255 \r\nL 109.519525 79.861118 \r\nL 109.663113 79.950928 \r\nL 109.812974 80.220359 \r\nL 109.849457 80.31017 \r\nL 109.943848 80.489791 \r\nL 110.089864 80.579601 \r\nL 110.345189 81.118464 \r\nL 110.491838 81.208274 \r\nL 110.595919 81.477705 \r\nL 110.858854 81.747136 \r\nL 111.128421 81.836947 \r\nL 111.23878 82.016568 \r\nL 111.384877 82.106378 \r\nL 111.497791 82.196189 \r\nL 111.561421 82.285999 \r\nL 111.6709 82.375809 \r\nL 111.89747 82.46562 \r\nL 112.108599 82.824861 \r\nL 112.173776 82.914672 \r\nL 112.265243 83.184103 \r\nL 112.459187 83.273914 \r\nL 112.540144 83.543345 \r\nL 112.681693 83.633155 \r\nL 112.799209 83.812776 \r\nL 112.88716 83.902586 \r\nL 113.018932 84.172018 \r\nL 113.099269 84.261828 \r\nL 113.294115 84.71088 \r\nL 113.616009 85.339553 \r\nL 113.676035 85.429364 \r\nL 113.787128 85.698795 \r\nL 114.024835 85.788605 \r\nL 114.14382 85.968226 \r\nL 114.215284 86.058036 \r\nL 114.770828 87.135761 \r\nL 114.828916 87.225572 \r\nL 114.941902 87.584814 \r\nL 115.100294 87.674624 \r\nL 115.295346 88.213486 \r\nL 115.375377 88.303297 \r\nL 115.47824 88.482918 \r\nL 115.528522 88.572728 \r\nL 115.64957 88.842159 \r\nL 115.677672 88.93197 \r\nL 116.836533 90.548557 \r\nL 116.922992 90.638368 \r\nL 117.014728 90.817989 \r\nL 117.201742 90.907799 \r\nL 117.303351 91.17723 \r\nL 117.400736 91.267041 \r\nL 117.493809 91.356851 \r\nL 117.688405 91.446661 \r\nL 117.793873 91.626282 \r\nL 118.004267 91.716093 \r\nL 118.041165 91.895714 \r\nL 118.201404 91.985524 \r\nL 118.317826 92.254955 \r\nL 118.422847 92.344766 \r\nL 118.5342 92.434576 \r\nL 118.767491 92.524386 \r\nL 118.880605 92.704007 \r\nL 119.420607 92.793818 \r\nL 120.185774 94.410405 \r\nL 120.517534 94.500216 \r\nL 120.58799 94.679836 \r\nL 121.513975 95.847372 \r\nL 121.611269 96.026993 \r\nL 121.864705 96.116803 \r\nL 121.982541 96.296424 \r\nL 122.100719 96.386234 \r\nL 122.211995 96.565855 \r\nL 122.406828 96.745476 \r\nL 122.473638 96.835286 \r\nL 122.591661 97.014907 \r\nL 122.792575 97.104718 \r\nL 122.899886 97.284339 \r\nL 122.980241 97.374149 \r\nL 123.193436 97.913011 \r\nL 123.316806 98.002822 \r\nL 123.560105 98.541684 \r\nL 123.749021 99.170357 \r\nL 123.807866 99.260168 \r\nL 123.922344 99.619409 \r\nL 124.186784 99.70922 \r\nL 124.466301 100.248082 \r\nL 124.592422 100.337893 \r\nL 124.693953 100.607324 \r\nL 125.058355 100.697134 \r\nL 125.186656 100.876755 \r\nL 125.333784 100.966566 \r\nL 125.391466 101.056376 \r\nL 125.532025 101.325807 \r\nL 125.56655 101.415618 \r\nL 125.965878 102.134101 \r\nL 126.004614 102.223911 \r\nL 126.095945 102.403532 \r\nL 126.171117 102.493343 \r\nL 126.287881 102.762774 \r\nL 126.372583 102.852584 \r\nL 126.405521 103.122016 \r\nL 126.578105 103.211826 \r\nL 126.719097 103.391447 \r\nL 126.849479 103.481257 \r\nL 127.118549 104.10993 \r\nL 127.262999 104.199741 \r\nL 127.384311 104.558982 \r\nL 127.639983 104.828414 \r\nL 127.689453 104.918224 \r\nL 127.872648 105.187655 \r\nL 127.995644 105.457086 \r\nL 128.241348 105.636707 \r\nL 128.545764 105.726518 \r\nL 128.651967 105.995949 \r\nL 128.696501 106.085759 \r\nL 128.783356 106.17557 \r\nL 128.874413 106.26538 \r\nL 128.967437 106.445001 \r\nL 129.140582 106.534811 \r\nL 129.250143 106.714432 \r\nL 129.474491 106.804243 \r\nL 129.623705 106.983864 \r\nL 129.824017 107.253295 \r\nL 129.919736 107.343105 \r\nL 129.990351 107.522726 \r\nL 130.402663 107.612536 \r\nL 130.523783 107.792157 \r\nL 130.842022 108.510641 \r\nL 130.932609 108.690261 \r\nL 131.372507 108.780072 \r\nL 131.51902 108.959693 \r\nL 131.886861 109.049503 \r\nL 132.015769 109.318934 \r\nL 132.092743 109.408745 \r\nL 132.224398 109.498555 \r\nL 132.315806 109.678176 \r\nL 132.796096 109.767986 \r\nL 133.073897 110.127228 \r\nL 133.116844 110.217039 \r\nL 133.348574 110.666091 \r\nL 133.447286 110.755901 \r\nL 133.619455 111.025332 \r\nL 133.68627 111.115143 \r\nL 133.73163 111.204953 \r\nL 133.834708 111.294764 \r\nL 133.955244 111.654005 \r\nL 134.634714 112.462299 \r\nL 134.742779 112.552109 \r\nL 134.98877 112.911351 \r\nL 135.066469 113.001161 \r\nL 135.25612 113.270593 \r\nL 135.338938 113.360403 \r\nL 135.432449 113.450214 \r\nL 135.554501 113.719645 \r\nL 135.651005 113.809455 \r\nL 136.021355 113.899266 \r\nL 136.302784 114.258507 \r\nL 136.586548 114.348318 \r\nL 136.721811 114.527939 \r\nL 136.893469 114.617749 \r\nL 137.273035 115.156611 \r\nL 137.601707 115.246422 \r\nL 137.753037 115.336232 \r\nL 137.819496 115.426043 \r\nL 137.960534 115.515853 \r\nL 138.032093 115.605664 \r\nL 138.183441 115.695474 \r\nL 138.718653 115.785284 \r\nL 139.497537 117.04263 \r\nL 139.538376 117.132441 \r\nL 139.718428 117.401872 \r\nL 140.1597 117.491682 \r\nL 140.312727 117.671303 \r\nL 140.348036 117.761114 \r\nL 140.463564 118.030545 \r\nL 140.762465 118.479597 \r\nL 140.957243 118.569407 \r\nL 141.034623 118.838839 \r\nL 141.219566 118.928649 \r\nL 142.37548 120.814668 \r\nL 142.645184 120.904478 \r\nL 142.718121 121.26372 \r\nL 143.012615 121.35353 \r\nL 143.155496 121.443341 \r\nL 143.156527 121.533151 \r\nL 143.879865 121.892393 \r\nL 143.97209 121.982203 \r\nL 144.145272 122.072014 \r\nL 144.385424 122.161824 \r\nL 144.411756 122.251634 \r\nL 144.683485 122.341445 \r\nL 144.778092 122.521066 \r\nL 144.860134 122.610876 \r\nL 144.969476 122.880307 \r\nL 145.238021 122.970118 \r\nL 145.639548 123.329359 \r\nL 145.857064 123.41917 \r\nL 145.990164 123.778411 \r\nL 146.095144 123.868222 \r\nL 146.12195 123.958032 \r\nL 146.281565 124.047843 \r\nL 146.662663 124.317274 \r\nL 146.737078 124.407084 \r\nL 146.823336 124.496895 \r\nL 147.201324 124.586705 \r\nL 147.452798 124.676516 \r\nL 147.624338 125.305189 \r\nL 147.797821 125.394999 \r\nL 147.920711 125.484809 \r\nL 148.113364 125.57462 \r\nL 148.533951 125.933861 \r\nL 148.649013 126.023672 \r\nL 148.906172 126.382914 \r\nL 149.209716 126.921776 \r\nL 149.721771 127.370828 \r\nL 149.869169 127.460639 \r\nL 149.951376 127.73007 \r\nL 150.312684 128.089311 \r\nL 150.528767 128.179122 \r\nL 150.65913 128.358743 \r\nL 150.782513 128.448553 \r\nL 150.895112 128.538364 \r\nL 151.087628 128.628174 \r\nL 151.492293 129.077226 \r\nL 151.586799 129.346657 \r\nL 152.15139 129.436468 \r\nL 152.474761 129.88552 \r\nL 152.879254 129.97533 \r\nL 153.098356 130.154951 \r\nL 153.144606 130.244761 \r\nL 153.248217 130.424382 \r\nL 153.445523 130.873434 \r\nL 153.617072 130.963245 \r\nL 153.71499 131.232676 \r\nL 154.281122 131.322486 \r\nL 154.50975 131.412297 \r\nL 154.558929 131.502107 \r\nL 154.645113 131.681728 \r\nL 155.034761 132.13078 \r\nL 155.311956 132.220591 \r\nL 155.614662 132.669643 \r\nL 156.273594 133.567747 \r\nL 156.467323 133.657557 \r\nL 156.587622 133.837178 \r\nL 156.794864 134.106609 \r\nL 157.297941 134.19642 \r\nL 157.437975 134.465851 \r\nL 157.528092 134.555661 \r\nL 157.621285 134.645472 \r\nL 157.933205 134.914903 \r\nL 158.075156 135.274145 \r\nL 158.103796 135.363955 \r\nL 158.224853 135.723197 \r\nL 158.5064 135.813007 \r\nL 158.759927 136.44168 \r\nL 158.895171 136.531491 \r\nL 159.177239 136.800922 \r\nL 159.192439 136.890732 \r\nL 159.596612 136.980543 \r\nL 159.717267 137.070353 \r\nL 159.763955 137.160164 \r\nL 159.924783 137.249974 \r\nL 160.206667 137.429595 \r\nL 160.563788 137.519405 \r\nL 160.673613 137.699026 \r\nL 160.889787 137.968457 \r\nL 161.047851 138.058268 \r\nL 161.146864 138.237889 \r\nL 161.662386 138.327699 \r\nL 161.837192 138.776751 \r\nL 162.178619 138.866561 \r\nL 162.395477 139.046182 \r\nL 162.682043 139.315614 \r\nL 163.008927 139.405424 \r\nL 163.079975 139.674855 \r\nL 163.385336 139.854476 \r\nL 163.556829 140.213718 \r\nL 164.568466 140.66277 \r\nL 165.561089 141.471064 \r\nL 165.759344 141.560874 \r\nL 165.85135 141.920116 \r\nL 166.058737 142.009926 \r\nL 166.498366 142.548789 \r\nL 167.045201 142.638599 \r\nL 167.124388 142.81822 \r\nL 167.471062 143.087651 \r\nL 167.512959 143.177461 \r\nL 167.831166 143.267272 \r\nL 167.992998 143.446893 \r\nL 168.293806 143.806134 \r\nL 169.003805 143.895945 \r\nL 169.190454 144.075566 \r\nL 169.435213 144.165376 \r\nL 169.545385 144.255186 \r\nL 169.856064 144.344997 \r\nL 170.554649 144.524618 \r\nL 170.649073 144.614428 \r\nL 170.768041 144.704239 \r\nL 171.051047 144.794049 \r\nL 171.107023 144.97367 \r\nL 171.307102 145.06348 \r\nL 171.737643 145.692153 \r\nL 171.85453 146.051395 \r\nL 172.266532 146.141205 \r\nL 172.5412 146.231016 \r\nL 172.633899 146.320826 \r\nL 172.708433 146.590257 \r\nL 173.153755 146.680068 \r\nL 173.370795 146.949499 \r\nL 173.618027 147.039309 \r\nL 173.726666 147.12912 \r\nL 173.819575 147.21893 \r\nL 174.091205 147.488361 \r\nL 174.477805 147.578172 \r\nL 174.50889 147.667982 \r\nL 174.824497 147.757793 \r\nL 175.281909 148.566086 \r\nL 175.463767 148.655897 \r\nL 175.556731 148.835518 \r\nL 176.66167 149.464191 \r\nL 177.00922 149.643811 \r\nL 177.535626 150.003053 \r\nL 178.069022 150.272484 \r\nL 178.546433 150.721536 \r\nL 179.330327 151.170589 \r\nL 179.560087 151.350209 \r\nL 179.825402 151.52983 \r\nL 179.95363 151.709451 \r\nL 180.037105 151.799261 \r\nL 180.644779 151.889072 \r\nL 180.77715 152.068693 \r\nL 181.173102 152.158503 \r\nL 181.348017 152.607555 \r\nL 181.670057 152.876986 \r\nL 181.92227 152.966797 \r\nL 182.172504 153.415849 \r\nL 182.486888 153.505659 \r\nL 183.425342 153.954711 \r\nL 183.482084 154.044522 \r\nL 183.482303 154.134332 \r\nL 183.800309 154.403764 \r\nL 184.024667 154.493574 \r\nL 184.975867 154.942626 \r\nL 185.240845 155.032436 \r\nL 185.303746 155.212057 \r\nL 185.738575 155.301868 \r\nL 185.795773 155.481489 \r\nL 186.211315 155.571299 \r\nL 186.422033 155.84073 \r\nL 186.654593 155.930541 \r\nL 186.672622 156.020351 \r\nL 187.163126 156.379593 \r\nL 187.225288 156.469403 \r\nL 188.01763 156.738834 \r\nL 188.238558 156.828645 \r\nL 188.460297 156.918455 \r\nL 188.844343 157.008266 \r\nL 189.010071 157.187886 \r\nL 189.548703 157.277697 \r\nL 189.760142 157.636939 \r\nL 189.974692 157.726749 \r\nL 190.039189 157.99618 \r\nL 190.432733 158.085991 \r\nL 192.050857 158.535043 \r\nL 192.083877 158.624853 \r\nL 192.442348 158.714664 \r\nL 192.527101 158.894284 \r\nL 192.791695 159.253526 \r\nL 192.79561 159.343336 \r\nL 193.179455 159.433147 \r\nL 193.189491 159.522957 \r\nL 193.552652 159.612768 \r\nL 193.764081 159.702578 \r\nL 194.409638 159.972009 \r\nL 194.797926 160.06182 \r\nL 195.007074 160.421062 \r\nL 195.021518 160.510872 \r\nL 195.514549 160.870114 \r\nL 195.602924 161.049734 \r\nL 196.43033 161.139545 \r\nL 196.779084 161.408976 \r\nL 196.971453 161.498787 \r\nL 197.333328 161.768218 \r\nL 197.414367 162.037649 \r\nL 197.611335 162.127459 \r\nL 197.92915 162.30708 \r\nL 198.06339 162.396891 \r\nL 198.184301 162.576512 \r\nL 198.188717 162.666322 \r\nL 199.78832 163.474616 \r\nL 199.948254 163.564426 \r\nL 200.327081 163.654237 \r\nL 200.578227 163.923668 \r\nL 200.632606 164.103289 \r\nL 201.208391 164.193099 \r\nL 201.489993 164.37272 \r\nL 203.830014 164.911582 \r\nL 203.93713 165.360634 \r\nL 204.598133 165.719876 \r\nL 204.695686 165.989307 \r\nL 205.083938 166.258739 \r\nL 205.487089 166.797601 \r\nL 205.970741 166.887412 \r\nL 206.068641 166.977222 \r\nL 206.925463 167.785516 \r\nL 207.023491 168.054947 \r\nL 207.459725 168.324378 \r\nL 209.035214 168.77343 \r\nL 209.114556 168.863241 \r\nL 209.545134 168.953051 \r\nL 209.733744 169.132672 \r\nL 209.756645 169.222482 \r\nL 210.343133 169.312293 \r\nL 210.466161 169.491914 \r\nL 211.147392 170.030776 \r\nL 211.433045 170.120587 \r\nL 211.747511 170.300207 \r\nL 212.455676 170.390018 \r\nL 213.161852 170.749259 \r\nL 213.830091 171.018691 \r\nL 214.192312 171.288122 \r\nL 214.357638 171.377932 \r\nL 214.565518 171.467743 \r\nL 214.741811 171.557553 \r\nL 215.219578 171.647364 \r\nL 215.530431 171.737174 \r\nL 216.15836 171.826984 \r\nL 216.161663 171.916795 \r\nL 217.045866 172.186226 \r\nL 218.093661 172.814899 \r\nL 218.141343 172.904709 \r\nL 218.567286 173.08433 \r\nL 218.640186 173.174141 \r\nL 219.155672 173.353762 \r\nL 219.192952 173.533382 \r\nL 219.428642 173.892624 \r\nL 219.46981 173.982434 \r\nL 220.21573 174.431487 \r\nL 220.374395 174.611107 \r\nL 220.881925 174.790728 \r\nL 220.889041 174.880539 \r\nL 221.268233 175.060159 \r\nL 221.629158 175.419401 \r\nL 222.03939 175.599022 \r\nL 222.214898 175.688832 \r\nL 223.649385 175.958264 \r\nL 223.765789 176.048074 \r\nL 224.16664 176.227695 \r\nL 224.277734 176.407316 \r\nL 224.357988 176.497126 \r\nL 224.706359 176.586937 \r\nL 224.996118 176.766557 \r\nL 225.753132 176.856368 \r\nL 226.508395 176.946178 \r\nL 226.716732 177.125799 \r\nL 227.402032 177.30542 \r\nL 227.407853 177.39523 \r\nL 228.784604 177.664662 \r\nL 228.950623 177.754472 \r\nL 229.429923 178.023903 \r\nL 230.357045 178.742387 \r\nL 230.881928 178.922007 \r\nL 231.069133 179.191439 \r\nL 231.391428 179.55068 \r\nL 232.865932 180.269164 \r\nL 232.970329 180.628405 \r\nL 233.156147 180.718216 \r\nL 233.271894 180.987647 \r\nL 234.193833 181.167268 \r\nL 234.239453 181.346889 \r\nL 234.338685 181.526509 \r\nL 234.798843 181.70613 \r\nL 235.195662 181.975562 \r\nL 236.044062 182.065372 \r\nL 236.301723 182.244993 \r\nL 237.801446 182.514424 \r\nL 238.40193 182.604234 \r\nL 239.044895 182.694045 \r\nL 239.754502 182.873666 \r\nL 239.85191 183.053287 \r\nL 240.241813 183.232907 \r\nL 240.265553 183.412528 \r\nL 240.395624 183.502339 \r\nL 242.069816 183.86158 \r\nL 242.242295 184.041201 \r\nL 242.250452 184.131012 \r\nL 242.449299 184.220822 \r\nL 243.012813 184.669874 \r\nL 243.063634 184.759684 \r\nL 243.475818 184.939305 \r\nL 243.98946 185.208737 \r\nL 245.33924 185.747599 \r\nL 246.748253 186.286462 \r\nL 247.750465 186.466082 \r\nL 248.049329 186.735514 \r\nL 250.145631 187.004945 \r\nL 250.554421 187.184566 \r\nL 250.592286 187.274376 \r\nL 251.032553 187.364187 \r\nL 253.099586 188.621532 \r\nL 253.106064 188.711343 \r\nL 254.440734 189.070584 \r\nL 254.628943 189.160395 \r\nL 255.678308 189.250205 \r\nL 256.844149 189.519637 \r\nL 256.895553 189.609447 \r\nL 257.176097 189.699257 \r\nL 258.292541 189.878878 \r\nL 258.464564 189.968689 \r\nL 260.172496 190.32793 \r\nL 260.791866 190.597362 \r\nL 261.60326 191.136224 \r\nL 261.635559 191.315845 \r\nL 262.936982 191.675087 \r\nL 262.96377 191.764897 \r\nL 264.597323 191.944518 \r\nL 264.690953 192.034328 \r\nL 266.172848 192.48338 \r\nL 266.17949 192.573191 \r\nL 267.402265 192.842622 \r\nL 267.464363 192.932432 \r\nL 267.919137 193.112053 \r\nL 267.97472 193.201864 \r\nL 270.343701 193.920347 \r\nL 272.638522 194.369399 \r\nL 274.087188 194.818451 \r\nL 277.34632 195.177693 \r\nL 277.973665 195.267503 \r\nL 280.915868 195.447124 \r\nL 280.924864 195.536934 \r\nL 281.346099 195.716555 \r\nL 281.70717 196.075797 \r\nL 282.284443 196.255418 \r\nL 282.654712 196.435039 \r\nL 283.018812 196.70447 \r\nL 283.637106 196.884091 \r\nL 283.653146 196.973901 \r\nL 284.015185 197.153522 \r\nL 284.234634 197.422953 \r\nL 287.458402 197.961816 \r\nL 287.547306 198.051626 \r\nL 290.475312 198.590489 \r\nL 290.670729 198.680299 \r\nL 294.581494 199.219162 \r\nL 297.59032 200.207076 \r\nL 297.762197 200.386697 \r\nL 298.341897 200.656128 \r\nL 299.484234 200.925559 \r\nL 302.288118 201.10518 \r\nL 304.92319 201.374612 \r\nL 305.024046 201.464422 \r\nL 306.194102 201.644043 \r\nL 306.413004 201.733853 \r\nL 310.243497 202.362526 \r\nL 310.309427 202.452337 \r\nL 312.681583 202.721768 \r\nL 312.719648 202.811578 \r\nL 313.137051 202.991199 \r\nL 314.310665 203.17082 \r\nL 314.40477 203.26063 \r\nL 316.583115 203.440251 \r\nL 316.585177 203.530062 \r\nL 317.446351 203.799493 \r\nL 317.67746 203.889303 \r\nL 320.468552 204.158734 \r\nL 320.790409 204.428166 \r\nL 323.751096 204.787407 \r\nL 323.808577 204.877218 \r\nL 327.695073 205.685512 \r\nL 327.864122 205.775322 \r\nL 329.073338 205.954943 \r\nL 329.189851 206.134564 \r\nL 331.810562 206.493805 \r\nL 332.213366 206.673426 \r\nL 333.945166 206.853047 \r\nL 334.153795 207.032668 \r\nL 334.33403 207.122478 \r\nL 340.473449 208.200203 \r\nL 342.25961 208.379824 \r\nL 343.16779 208.739066 \r\nL 343.260709 208.918687 \r\nL 344.847639 209.188118 \r\nL 345.37796 209.367739 \r\nL 345.421736 209.547359 \r\nL 348.672037 210.086222 \r\nL 348.913292 210.176032 \r\nL 351.488347 210.535274 \r\nL 351.649841 210.625084 \r\nL 353.380711 210.894516 \r\nL 353.549358 211.074137 \r\nL 353.593627 211.163947 \r\nL 354.435549 211.343568 \r\nL 355.038716 211.612999 \r\nL 355.167784 211.702809 \r\nL 355.891058 211.88243 \r\nL 357.781852 212.331482 \r\nL 358.45197 212.511103 \r\nL 358.618391 212.690724 \r\nL 359.800509 212.870345 \r\nL 360.222383 213.319397 \r\nL 361.568422 213.588828 \r\nL 361.867341 213.768449 \r\nL 362.332006 213.94807 \r\nL 362.644155 214.217501 \r\nL 363.33024 214.576743 \r\nL 363.363068 214.756364 \r\nL 363.363068 214.756364 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 43.78125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 224.64 \r\nL 378.58125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 378.58125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 7.2 \r\nL 378.58125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 294.571875 132.098125 \r\nL 371.58125 132.098125 \r\nQ 373.58125 132.098125 373.58125 130.098125 \r\nL 373.58125 101.741875 \r\nQ 373.58125 99.741875 371.58125 99.741875 \r\nL 294.571875 99.741875 \r\nQ 292.571875 99.741875 292.571875 101.741875 \r\nL 292.571875 130.098125 \r\nQ 292.571875 132.098125 294.571875 132.098125 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\">\r\n     <path d=\"M 296.571875 107.840313 \r\nL 316.571875 107.840313 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\"/>\r\n    <g id=\"text_15\">\r\n     <!-- Precision -->\r\n     <defs>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n     </defs>\r\n     <g transform=\"translate(324.571875 111.340313)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"60.287109\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"101.369141\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"162.892578\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"217.873047\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"245.65625\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"297.755859\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"325.539062\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"386.720703\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 296.571875 122.518438 \r\nL 316.571875 122.518438 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_16\">\r\n     <!-- Recall -->\r\n     <defs>\r\n      <path d=\"M 44.390625 34.1875 \r\nQ 47.5625 33.109375 50.5625 29.59375 \r\nQ 53.5625 26.078125 56.59375 19.921875 \r\nL 66.609375 0 \r\nL 56 0 \r\nL 46.6875 18.703125 \r\nQ 43.0625 26.03125 39.671875 28.421875 \r\nQ 36.28125 30.8125 30.421875 30.8125 \r\nL 19.671875 30.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nL 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.578125 72.90625 50.734375 67.671875 \r\nQ 56.890625 62.453125 56.890625 51.90625 \r\nQ 56.890625 45.015625 53.6875 40.46875 \r\nQ 50.484375 35.9375 44.390625 34.1875 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 38.921875 \r\nL 32.078125 38.921875 \r\nQ 39.203125 38.921875 42.84375 42.21875 \r\nQ 46.484375 45.515625 46.484375 51.90625 \r\nQ 46.484375 58.296875 42.84375 61.546875 \r\nQ 39.203125 64.796875 32.078125 64.796875 \r\nz\r\n\" id=\"DejaVuSans-82\"/>\r\n     </defs>\r\n     <g transform=\"translate(324.571875 126.018438)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"69.419922\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"130.943359\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"185.923828\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"247.203125\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"274.986328\" xlink:href=\"#DejaVuSans-108\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p4002240743\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Threshold-PR curve\n",
    "train_loss, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader)\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_true.ravel(), y_prob.ravel())\n",
    "plt.plot(thresholds, precisions[:-1], \"r--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"b-\", label=\"Recall\")\n",
    "plt.ylabel(\"Performance\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.21466002"
      ]
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "# Best threshold for f1\n",
    "threshold = find_best_threshold(y_true.ravel(), y_prob.ravel())\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine predictions using threshold\n",
    "test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
    "y_pred = np.array([np.where(prob >= threshold, 1, 0) for prob in y_prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"precision\": 0.34976369495166487,\n  \"recall\": 0.2389525693244905,\n  \"f1\": 0.25651611949847763,\n  \"num_samples\": 480.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "performance = get_performance(\n",
    "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "print (json.dumps(performance['overall'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='tag', index=3, options=('interpretability', 'segmentation', 'produ…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb9b7d07cd074e82bdaf2d1ecb5b9d1b"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(tag=list(sorted_tags_by_f1.keys()))\n",
    "def display_tag_analysis(tag='transformers'):\n",
    "    # Performance\n",
    "    print (json.dumps(performance[\"class\"][tag], indent=2))\n",
    "    \n",
    "    # TP, FP, FN samples\n",
    "    index = label_encoder.class_to_index[tag]\n",
    "    tp, fp, fn = [], [], []\n",
    "    for i in range(len(y_test)):\n",
    "        true = y_test[i][index]\n",
    "        pred = y_pred[i][index]\n",
    "        if true and pred:\n",
    "            tp.append(i)\n",
    "        elif not true and pred:\n",
    "            fp.append(i)\n",
    "        elif true and not pred:\n",
    "            fn.append(i)\n",
    "            \n",
    "    # Samples\n",
    "    num_samples = 3\n",
    "    if len(tp): \n",
    "        print (\"\\n=== True positives ===\")\n",
    "        for i in tp[:num_samples]:        \n",
    "            print (f\"  {X_test_raw[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")\n",
    "    if len(fp): \n",
    "        print (\"=== False positives === \")\n",
    "        for i in fp[:num_samples]:        \n",
    "            print (f\"  {X_test_raw[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")\n",
    "    if len(fn): \n",
    "        print (\"=== False negatives ===\")\n",
    "        for i in fn[:num_samples]:        \n",
    "            print (f\"  {X_test_raw[i]}\") \n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts\n",
    "dir = Path(\"rnn\")\n",
    "dir.mkdir(parents=True, exist_ok=True)\n",
    "tokenizer.save(fp=Path(dir, 'tokenzier.json'))\n",
    "label_encoder.save(fp=Path(dir, 'label_encoder.json'))\n",
    "torch.save(best_model.state_dict(), Path(dir, 'model.pt'))\n",
    "with open(Path(dir, 'performance.json'), \"w\") as fp:\n",
    "    json.dump(performance, indent=2, sort_keys=False, fp=fp)"
   ]
  },
  {
   "source": [
    "Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model:\n<bound method Module.named_parameters of RNN(\n  (embeddings): Embedding(136, 128, padding_idx=0)\n  (rnn): GRU(128, 128, batch_first=True, bidirectional=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=256, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=35, bias=True)\n)>\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = RNN(\n",
    "    embedding_dim=embedding_dim, vocab_size=vocab_size,\n",
    "    rnn_hidden_dim=rnn_hidden_dim, hidden_dim=hidden_dim, \n",
    "    dropout_p=dropout_p, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "print (f\"Model:\\n{model.named_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embeddings): Embedding(136, 128, padding_idx=0)\n",
       "  (rnn): GRU(128, 128, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=35, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "# Load artifacts\n",
    "device = torch.device(\"cpu\")\n",
    "tokenizer = Tokenizer.load(fp=Path(dir, 'tokenzier.json'))\n",
    "label_encoder = LabelEncoder.load(fp=Path(dir, 'label_encoder.json'))\n",
    "model = RNN(\n",
    "    embedding_dim=embedding_dim, vocab_size=vocab_size,\n",
    "    rnn_hidden_dim=rnn_hidden_dim, hidden_dim=hidden_dim, \n",
    "    dropout_p=dropout_p, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(Path(dir, 'model.pt'), map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "text = \"Transfer learning with BERT for self-supervised learning\"\n",
    "X = np.array(tokenizer.texts_to_sequences([preprocess(text)]))\n",
    "y_filler = label_encoder.encode([np.array([label_encoder.classes[0]]*len(X))])\n",
    "dataset = RNNTextDataset(X=X, y=y_filler)\n",
    "dataloader = dataset.create_dataloader(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "metadata": {},
     "execution_count": 164
    }
   ],
   "source": [
    "# Inference\n",
    "y_prob = trainer.predict_step(dataloader)\n",
    "y_pred = np.array([np.where(prob >= threshold, 1, 0) for prob in y_prob])\n",
    "label_encoder.decode(y_pred)"
   ]
  },
  {
   "source": [
    "limitation: since we're using character embeddings our encoded sequences are quite long (>100), the RNNs may potentially be suffering from memory issues. We also can't process our tokens in parallel because we're restricted by sequential processin"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Transformers w/ Contextual Embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data splits\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)\n",
    "X_test_raw = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "cuda = True\n",
    "device = torch.device('cuda' if (\n",
    "    torch.cuda.is_available() and cuda) else 'cpu')\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "if device.type == 'cuda':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "31090\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "vocab_size = len(tokenizer)\n",
    "print (vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1000, 60]) torch.Size([1000, 60])\n",
      "torch.Size([227, 61]) torch.Size([227, 61])\n",
      "torch.Size([217, 58]) torch.Size([217, 58])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize inputs\n",
    "encoded_input = tokenizer(X_train.tolist(), return_tensors='pt', padding=True)\n",
    "X_train_ids = encoded_input['input_ids']\n",
    "X_train_masks = encoded_input['attention_mask']\n",
    "print (X_train_ids.shape, X_train_masks.shape)\n",
    "encoded_input = tokenizer(X_val.tolist(), return_tensors='pt', padding=True)\n",
    "X_val_ids = encoded_input['input_ids']\n",
    "X_val_masks = encoded_input['attention_mask']\n",
    "print (X_val_ids.shape, X_val_masks.shape)\n",
    "encoded_input = tokenizer(X_test.tolist(), return_tensors='pt', padding=True)\n",
    "X_test_ids = encoded_input['input_ids']\n",
    "X_test_masks = encoded_input['attention_mask']\n",
    "print (X_test_ids.shape, X_test_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([  102,  6160,  1923,   288,  3254,  1572, 18205,  5560,   137,  4578,\n          147,   626, 23474,   291,  2715,   494, 10558,   205,   103,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n[CLS] albumentations fast image augmentation library and easy to use wrapper around other libraries. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# Decode\n",
    "print (f\"{X_train_ids[0]}\\n{tokenizer.decode(X_train_ids[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['[CLS]', 'alb', '##ument', '##ations', 'fast', 'image', 'augmentation', 'library', 'and', 'easy', 'to', 'use', 'wrap', '##per', 'around', 'other', 'libraries', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# Sub-word tokens\n",
    "print (tokenizer.convert_ids_to_tokens(ids=X_train_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "class counts:\n  [120  41 388 106  41  75  34  73  51  78  64  51  55  93  51 429  33  69\n  30  51 258  32  49  59  57  60  48  40 213  40  34  46 196  39  39]\n\nclass weights:\n  {0: 0.008333333333333333, 1: 0.024390243902439025, 2: 0.002577319587628866, 3: 0.009433962264150943, 4: 0.024390243902439025, 5: 0.013333333333333334, 6: 0.029411764705882353, 7: 0.0136986301369863, 8: 0.0196078431372549, 9: 0.01282051282051282, 10: 0.015625, 11: 0.0196078431372549, 12: 0.01818181818181818, 13: 0.010752688172043012, 14: 0.0196078431372549, 15: 0.002331002331002331, 16: 0.030303030303030304, 17: 0.014492753623188406, 18: 0.03333333333333333, 19: 0.0196078431372549, 20: 0.003875968992248062, 21: 0.03125, 22: 0.02040816326530612, 23: 0.01694915254237288, 24: 0.017543859649122806, 25: 0.016666666666666666, 26: 0.020833333333333332, 27: 0.025, 28: 0.004694835680751174, 29: 0.025, 30: 0.029411764705882353, 31: 0.021739130434782608, 32: 0.00510204081632653, 33: 0.02564102564102564, 34: 0.02564102564102564}\n"
     ]
    }
   ],
   "source": [
    "# Class weights\n",
    "counts = np.bincount([label_encoder.class_to_index[class_] for class_ in all_tags])\n",
    "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
    "print (\"class counts:\\n\"\n",
    "    f\"  {counts}\\n\\n\"\n",
    "    \"class weights:\\n\"\n",
    "    f\"  {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids, masks, targets):\n",
    "        self.ids = ids\n",
    "        self.masks = masks\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Dataset(N={len(self)})>\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ids = torch.tensor(self.ids[index], dtype=torch.long)\n",
    "        masks = torch.tensor(self.masks[index], dtype=torch.long)\n",
    "        targets = torch.FloatTensor(self.targets[index])\n",
    "        return ids, masks, targets\n",
    "\n",
    "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data splits:\n  Train dataset:<Dataset(N=1000)>\n  Val dataset: <Dataset(N=227)>\n  Test dataset: <Dataset(N=217)>\nSample point:\n  ids: tensor([  102,  6160,  1923,   288,  3254,  1572, 18205,  5560,   137,  4578,\n          147,   626, 23474,   291,  2715,   494, 10558,   205,   103,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n  masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n  targets: tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = TransformerTextDataset(ids=X_train_ids, masks=X_train_masks, targets=y_train)\n",
    "val_dataset = TransformerTextDataset(ids=X_val_ids, masks=X_val_masks, targets=y_val)\n",
    "test_dataset = TransformerTextDataset(ids=X_test_ids, masks=X_test_masks, targets=y_test)\n",
    "print (\"Data splits:\\n\"\n",
    "    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n",
    "    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n",
    "    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  ids: {train_dataset[0][0]}\\n\"\n",
    "    f\"  masks: {train_dataset[0][1]}\\n\"\n",
    "    f\"  targets: {train_dataset[0][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample batch:\n  ids: torch.Size([128, 60])\n  masks: torch.Size([128, 60])\n  targets: torch.Size([128, 35])\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 128\n",
    "train_dataloader = train_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "val_dataloader = val_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "test_dataloader = test_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "batch = next(iter(train_dataloader))\n",
    "print (\"Sample batch:\\n\"\n",
    "    f\"  ids: {batch[0].size()}\\n\"\n",
    "    f\"  masks: {batch[1].size()}\\n\"\n",
    "    f\"  targets: {batch[2].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "embedding_dim = transformer.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, transformer, dropout_p, embedding_dim, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "        self.fc1 = torch.nn.Linear(embedding_dim, num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ids, masks = inputs\n",
    "        seq, pool = self.transformer(input_ids=ids, attention_mask=masks)\n",
    "        z = self.dropout(pool)\n",
    "        z = self.fc1(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method Module.named_parameters of Transformer(\n  (transformer): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=768, out_features=35, bias=True)\n)>\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "dropout_p = 0.5\n",
    "model = Transformer(\n",
    "    transformer=transformer, dropout_p=dropout_p,\n",
    "    embedding_dim=embedding_dim, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "print (model.named_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "lr = 1e-4\n",
    "num_epochs = 200\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "class_weights_tensor = torch.Tensor(np.array(list(class_weights.values())))\n",
    "loss_fn = nn.BCEWithLogitsLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer & scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer module\n",
    "trainer = Trainer(\n",
    "    model=model, device=device, loss_fn=loss_fn, \n",
    "    optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 4.00 GiB total capacity; 2.19 GiB already allocated; 6.45 MiB free; 2.27 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-185-8d0f0dee99db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m best_model = trainer.train(\n\u001b[1;32m----> 3\u001b[1;33m     num_epochs, patience, train_dataloader, val_dataloader)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-57-92b71a82bb31>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, num_epochs, patience, train_dataloader, val_dataloader)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;31m# Steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-92b71a82bb31>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Reset gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Define loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mJ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-179-84a08ca4bd6c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    851\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m         )\n\u001b[0;32m    855\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    486\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m                 )\n\u001b[0;32m    490\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         )\n\u001b[0;32m    430\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   1698\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1700\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         return F.layer_norm(\n\u001b[1;32m--> 170\u001b[1;33m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2093\u001b[0m                 layer_norm, (input,), input, normalized_shape, weight=weight, bias=bias, eps=eps)\n\u001b[0;32m   2094\u001b[0m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[1;32m-> 2095\u001b[1;33m                             torch.backends.cudnn.enabled)\n\u001b[0m\u001b[0;32m   2096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 4.00 GiB total capacity; 2.19 GiB already allocated; 6.45 MiB free; 2.27 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "best_model = trainer.train(\n",
    "    num_epochs, patience, train_dataloader, val_dataloader)"
   ]
  },
  {
   "source": [
    "# Tracking Experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import mlflow\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify arguments\n",
    "args = Namespace(\n",
    "    char_level=True,\n",
    "    filter_sizes=list(range(1, 11)),\n",
    "    batch_size=64,\n",
    "    embedding_dim=128,\n",
    "    num_filters=128,\n",
    "    hidden_dim=128,\n",
    "    dropout_p=0.5,\n",
    "    lr=2e-4,\n",
    "    num_epochs=200,\n",
    "    patience=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tracking URI\n",
    "EXPERIMENTS_DIR = Path(\"experiments\")\n",
    "Path(EXPERIMENTS_DIR).mkdir(exist_ok=True) # create experiments dir\n",
    "mlflow.set_tracking_uri(\"file:\\\\\" + str(EXPERIMENTS_DIR.absolute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer (modified for experiment tracking)\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, device, loss_fn=None,\n",
    "                optimizer=None, scheduler=None):\n",
    "\n",
    "        # Set params\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train_step(self, dataloader):\n",
    "        \"\"\"Train step.\"\"\"\n",
    "        # Set model to train mode\n",
    "        self.model.train()\n",
    "        loss = 0.0\n",
    "\n",
    "        # Iterate over train batches\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Step\n",
    "            batch = [item.to(self.device) for item in batch]\n",
    "            inputs, targets = batch[:-1], batch[-1]\n",
    "            self.optimizer.zero_grad()  # Reset gradients\n",
    "            z = self.model(inputs)  # Forward pass\n",
    "            J = self.loss_fn(z, targets)  # Define loss\n",
    "            J.backward()  # Backward pass\n",
    "            self.optimizer.step()  # Update weights\n",
    "\n",
    "            # Cumulative Metrics\n",
    "            loss += (J.detach().item() - loss) / (i + 1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def eval_step(self, dataloader):\n",
    "        \"\"\"Validation or test step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        loss = 0.0\n",
    "        y_trues, y_probs = [], []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Step\n",
    "                batch = [item.to(self.device) for item in batch]  # Set device\n",
    "                inputs, y_true = batch[:-1], batch[-1]\n",
    "                z = self.model(inputs)  # Forward pass\n",
    "                J = self.loss_fn(z, y_true).item()\n",
    "\n",
    "                # Cumulative Metrics\n",
    "                loss += (J - loss) / (i + 1)\n",
    "\n",
    "                # Store outputs\n",
    "                y_prob = torch.sigmoid(z).cpu().numpy()\n",
    "                y_probs.extend(y_prob)\n",
    "                y_trues.extend(y_true.cpu().numpy())\n",
    "\n",
    "        return loss, np.vstack(y_trues), np.vstack(y_probs)\n",
    "\n",
    "    def predict_step(self, dataloader):\n",
    "        \"\"\"Prediction step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        y_probs = []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Forward pass w/ inputs\n",
    "                inputs, targets = batch[:-1], batch[-1]\n",
    "                y_prob = self.model(inputs)\n",
    "\n",
    "                # Store outputs\n",
    "                y_probs.extend(y_prob)\n",
    "\n",
    "        return np.vstack(y_probs)\n",
    "\n",
    "    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n",
    "        best_val_loss = np.inf\n",
    "        for epoch in range(num_epochs):\n",
    "            # Steps\n",
    "            train_loss = self.train_step(dataloader=train_dataloader)\n",
    "            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = self.model\n",
    "                _patience = patience  # reset _patience\n",
    "            else:\n",
    "                _patience -= 1\n",
    "            if not _patience:  # 0\n",
    "                print(\"Stopping early!\")\n",
    "                break\n",
    "\n",
    "            # Tracking\n",
    "            mlflow.log_metrics(\n",
    "                {\"train_loss\": train_loss, \"val_loss\": val_loss}, step=epoch\n",
    "            )\n",
    "\n",
    "            # Logging\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1} | \"\n",
    "                f\"train_loss: {train_loss:.5f}, \"\n",
    "                f\"val_loss: {val_loss:.5f}, \"\n",
    "                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n",
    "                f\"_patience: {_patience}\"\n",
    "            )\n",
    "\n",
    "        return best_model, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the cnn function\n",
    "def train_cnn(args, df):\n",
    "    \"\"\"Train a CNN using specific arguments.\"\"\"\n",
    "\n",
    "    # Set seeds\n",
    "    set_seed()\n",
    "\n",
    "    # Get data splits\n",
    "    preprocessed_df = df.copy()\n",
    "    preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)\n",
    "    X_test_raw = X_test\n",
    "    num_classes = len(label_encoder)\n",
    "\n",
    "    # Set device\n",
    "    cuda = True\n",
    "    device = torch.device('cuda' if (\n",
    "        torch.cuda.is_available() and cuda) else 'cpu')\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    if device.type == 'cuda':\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "    # Tokenize\n",
    "    tokenizer = Tokenizer(char_level=args.char_level)\n",
    "    tokenizer.fit_on_texts(texts=X_train)\n",
    "    vocab_size = len(tokenizer)\n",
    "\n",
    "    # Convert texts to sequences of indices\n",
    "    X_train = np.array(tokenizer.texts_to_sequences(X_train))\n",
    "    X_val = np.array(tokenizer.texts_to_sequences(X_val))\n",
    "    X_test = np.array(tokenizer.texts_to_sequences(X_test))\n",
    "\n",
    "    # Class weights\n",
    "    counts = np.bincount([label_encoder.class_to_index[class_] for class_ in all_tags])\n",
    "    class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CNNTextDataset(\n",
    "        X=X_train, y=y_train, max_filter_size=max(args.filter_sizes))\n",
    "    val_dataset = CNNTextDataset(\n",
    "        X=X_val, y=y_val, max_filter_size=max(args.filter_sizes))\n",
    "    test_dataset = CNNTextDataset(\n",
    "        X=X_test, y=y_test, max_filter_size=max(args.filter_sizes))\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = train_dataset.create_dataloader(\n",
    "        batch_size=args.batch_size)\n",
    "    val_dataloader = val_dataset.create_dataloader(\n",
    "        batch_size=args.batch_size)\n",
    "    test_dataloader = test_dataset.create_dataloader(\n",
    "        batch_size=args.batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    model = CNN(\n",
    "        embedding_dim=args.embedding_dim, vocab_size=vocab_size,\n",
    "        num_filters=args.num_filters, filter_sizes=args.filter_sizes,\n",
    "        hidden_dim=args.hidden_dim, dropout_p=args.dropout_p,\n",
    "        num_classes=num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define loss\n",
    "    class_weights_tensor = torch.Tensor(np.array(list(class_weights.values())))\n",
    "    loss_fn = nn.BCEWithLogitsLoss(weight=class_weights_tensor)\n",
    "\n",
    "    # Define optimizer & scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "    # Trainer module\n",
    "    trainer = Trainer(\n",
    "        model=model, device=device, loss_fn=loss_fn,\n",
    "        optimizer=optimizer, scheduler=scheduler)\n",
    "\n",
    "    # Train\n",
    "    best_model, best_val_loss = trainer.train(\n",
    "        args.num_epochs, args.patience, train_dataloader, val_dataloader)\n",
    "\n",
    "    # Best threshold for f1\n",
    "    train_loss, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader)\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true.ravel(), y_prob.ravel())\n",
    "    threshold = find_best_threshold(y_true.ravel(), y_prob.ravel())\n",
    "\n",
    "    # Determine predictions using threshold\n",
    "    test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
    "    y_pred = np.array([np.where(prob >= threshold, 1, 0) for prob in y_prob])\n",
    "\n",
    "    # Evaluate\n",
    "    performance = get_performance(\n",
    "        y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "\n",
    "    return {\n",
    "        \"args\": args,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"model\": best_model,\n",
    "        \"performance\": performance,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }"
   ]
  },
  {
   "source": [
    "### Tracking"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Experiment \n",
    "mlflow.set_experiment(experiment_name=\"baselines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(d,filepath):\n",
    "    \"\"\"Save dict to a json file. \"\"\"\n",
    "    with open(filepath,\"w\") as fp:\n",
    "        json.dump(d,indent=2,sort_keys=False,fp=fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.21 GiB already allocated; 6.45 MiB free; 2.27 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-cd61798805fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cnn\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#Train and Evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0martifacts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#Log Key Metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-190-ed21bd50aae8>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[1;34m(args, df)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     best_model, best_val_loss = trainer.train(\n\u001b[1;32m---> 77\u001b[1;33m         args.num_epochs, args.patience, train_dataloader, val_dataloader)\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# Best threshold for f1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-189-190dbd1cfe47>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, num_epochs, patience, train_dataloader, val_dataloader)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;31m# Steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-189-190dbd1cfe47>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Reset gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Define loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mJ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-116-56fd90a422f3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, channel_first)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;31m# Conv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0m_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpadding_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_right\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;31m# Pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    257\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m    258\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 259\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.21 GiB already allocated; 6.45 MiB free; 2.27 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"cnn\") as run:\n",
    "    #Train and Evaluate \n",
    "    artifacts=train_cnn(args=args,df=df)\n",
    "\n",
    "    #Log Key Metrics\n",
    "    mlflow.log_metrics({\"precision\": artifacts[\"performance\"][\"overall\"][\"precision\"]})\n",
    "    mlflow.log_metrics({\"recall\": artifacts[\"performance\"][\"overall\"][\"recall\"]})\n",
    "    mlflow.log_metrics({\"f1\": artifacts[\"performance\"][\"overall\"][\"f1\"]})\n",
    "\n",
    "    #Log artifacts\n",
    "    with tempfile.TemporaryDirectory() as fp:\n",
    "        artifacts[\"tokenizer\"].save(Path(fp, \"tokenizer.json\"))\n",
    "        artifacts[\"label_encoder\"].save(Path(fp, \"label_encoder.json\"))\n",
    "        torch.save(artifacts[\"model\"].state_dict(), Path(fp, \"model.pt\"))\n",
    "        save_dict(artifacts[\"performance\"], Path(fp, \"performance.json\"))\n",
    "        mlflow.log_artifacts(fp)\n",
    "\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params(vars(artifacts[\"args\"]))\n",
    "    \n"
   ]
  },
  {
   "source": [
    "### Viewing "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(filepath):\n",
    "\n",
    "    with open(filepath,\"r\") as fp:\n",
    "        d=json.load(fp)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                             run_id experiment_id    status  \\\n0  f813618727c644a09d3a54858635c7c7             0  FINISHED   \n1  8326e19252de49608e165b5a7f3d6bae             0    FAILED   \n2  255e0160d6dd4e4b9bc49c5ea74c3e1d             0    FAILED   \n3  fcc76648a52e496f89f5b216b97f4768             0    FAILED   \n\n                                        artifact_uri  \\\n0  file:\\c:\\Users\\KATTUBOINA\\Desktop\\Kattu\\AI_tag...   \n1  file:\\c:\\Users\\KATTUBOINA\\Desktop\\Kattu\\AI_tag...   \n2  file:\\c:\\Users\\KATTUBOINA\\Desktop\\Kattu\\AI_tag...   \n3  file:\\c:\\Users\\KATTUBOINA\\Desktop\\Kattu\\AI_tag...   \n\n                        start_time                         end_time  \\\n0 2021-02-21 11:52:30.093000+00:00 2021-02-21 11:53:28.125000+00:00   \n1 2021-02-21 12:12:39.128000+00:00 2021-02-21 12:12:39.576000+00:00   \n2 2021-02-21 11:52:06.561000+00:00 2021-02-21 11:52:06.615000+00:00   \n3 2021-02-21 11:51:57.845000+00:00 2021-02-21 11:51:57.937000+00:00   \n\n   metrics.precision  metrics.f1  metrics.train_loss  metrics.val_loss  ...  \\\n0           0.848144     0.60784            0.000609          0.001517  ...   \n1                NaN         NaN                 NaN               NaN  ...   \n2                NaN         NaN                 NaN               NaN  ...   \n3                NaN         NaN                 NaN               NaN  ...   \n\n   params.char_level params.dropout_p params.lr params.num_filters  \\\n0               True              0.5    0.0002                128   \n1               None             None      None               None   \n2               None             None      None               None   \n3               None             None      None               None   \n\n               params.filter_sizes params.batch_size  \\\n0  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]                64   \n1                             None              None   \n2                             None              None   \n3                             None              None   \n\n                             tags.mlflow.source.name tags.mlflow.runName  \\\n0  C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipy...                 cnn   \n1  C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipy...                 cnn   \n2  C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipy...                 cnn   \n3  C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipy...                 cnn   \n\n  tags.mlflow.source.type tags.mlflow.user  \n0                   LOCAL       KATTUBOINA  \n1                   LOCAL       KATTUBOINA  \n2                   LOCAL       KATTUBOINA  \n3                   LOCAL       KATTUBOINA  \n\n[4 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load components\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment_id = mlflow.get_experiment_by_name(\"baselines\").experiment_id\n",
    "all_runs = mlflow.search_runs(experiment_ids=experiment_id, order_by=[\"metrics.f1 DESC\"])\n",
    "print (all_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best run\n",
    "device = torch.device(\"cpu\")\n",
    "best_run_id = all_runs.iloc[0].run_id\n",
    "best_run = mlflow.get_run(run_id=best_run_id)\n",
    "with tempfile.TemporaryDirectory() as fp:\n",
    "    client.download_artifacts(run_id=best_run_id, path=\"\", dst_path=fp)\n",
    "    tokenizer = Tokenizer.load(fp=Path(fp, \"tokenizer.json\"))\n",
    "    label_encoder = LabelEncoder.load(fp=Path(fp, \"label_encoder.json\"))\n",
    "    model_state = torch.load(Path(fp, \"model.pt\"), map_location=device)\n",
    "    performance = load_dict(filepath=Path(fp, \"performance.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (json.dumps(performance[\"overall\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Wrong shape for input_ids (shape torch.Size([1, 47])) or attention_mask (shape torch.Size([1]))",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-206-f7f8e375199d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_prob\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-189-190dbd1cfe47>\u001b[0m in \u001b[0;36mpredict_step\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;31m# Forward pass w/ inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                 \u001b[0my_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[1;31m# Store outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-179-84a08ca4bd6c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[1;31m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m         \u001b[1;31m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m         \u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_extended_attention_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m         \u001b[1;31m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mget_extended_attention_mask\u001b[1;34m(self, attention_mask, input_shape, device)\u001b[0m\n\u001b[0;32m    260\u001b[0m             raise ValueError(\n\u001b[0;32m    261\u001b[0m                 \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n\u001b[1;32m--> 262\u001b[1;33m                     \u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m                 )\n\u001b[0;32m    264\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong shape for input_ids (shape torch.Size([1, 47])) or attention_mask (shape torch.Size([1]))"
     ]
    }
   ],
   "source": [
    "# Load artifacts\n",
    "device = torch.device(\"cpu\")\n",
    "model = CNN(\n",
    "    embedding_dim=args.embedding_dim, vocab_size=len(tokenizer),\n",
    "    num_filters=args.num_filters, filter_sizes=args.filter_sizes,\n",
    "    hidden_dim=args.hidden_dim, dropout_p=args.dropout_p,\n",
    "    num_classes=len(label_encoder))\n",
    "model.load_state_dict(model_state)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "y_prob = trainer.predict_step(dataloader)\n",
    "y_pred = np.array([np.where(prob >= threshold, 1, 0) for prob in y_prob])\n",
    "label_encoder.decode(y_pred)"
   ]
  }
 ]
}