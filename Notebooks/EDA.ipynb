{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the nesseasary libraries\n",
    "from collections import Counter,OrderedDict\n",
    "import ipywidgets as widgets\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"id\": 324,\n  \"title\": \"AdverTorch\",\n  \"description\": \"A Toolbox for Adversarial Robustness Research\",\n  \"tags\": [\n    \"code\",\n    \"library\",\n    \"security\",\n    \"adversarial-learning\",\n    \"adversarial-attacks\",\n    \"adversarial-perturbations\"\n  ]\n}\n"
     ]
    }
   ],
   "source": [
    "#Load projects\n",
    "url=\"https://raw.githubusercontent.com/GokuMohandas/applied-ml/main/datasets/projects.json\"\n",
    "projects=json.loads(urlopen(url).read())\n",
    "print(json.dumps(projects[-305],indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2032 projects\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     id                                              title  \\\n",
       "0  2438  How to Deal with Files in Google Colab: What Y...   \n",
       "1  2437                                             Rasoee   \n",
       "2  2436    Machine Learning Methods Explained (+ Examples)   \n",
       "3  2435  Top “Applied Data Science” Papers from ECML-PK...   \n",
       "4  2434                          OpenMMLab Computer Vision   \n",
       "\n",
       "                                         description  \\\n",
       "0  How to supercharge your Google Colab experienc...   \n",
       "1  A powerful web and mobile application that ide...   \n",
       "2  Most common techniques used in data science pr...   \n",
       "3  Explore the innovative world of Machine Learni...   \n",
       "4  MMCV is a python library for CV research and s...   \n",
       "\n",
       "                                                tags  \n",
       "0        [article, google-colab, colab, file-system]  \n",
       "1  [api, article, code, dataset, paper, research,...  \n",
       "2  [article, deep-learning, machine-learning, dim...  \n",
       "3  [article, deep-learning, machine-learning, adv...  \n",
       "4  [article, code, pytorch, library, 3d, computer...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>description</th>\n      <th>tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2438</td>\n      <td>How to Deal with Files in Google Colab: What Y...</td>\n      <td>How to supercharge your Google Colab experienc...</td>\n      <td>[article, google-colab, colab, file-system]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2437</td>\n      <td>Rasoee</td>\n      <td>A powerful web and mobile application that ide...</td>\n      <td>[api, article, code, dataset, paper, research,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2436</td>\n      <td>Machine Learning Methods Explained (+ Examples)</td>\n      <td>Most common techniques used in data science pr...</td>\n      <td>[article, deep-learning, machine-learning, dim...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2435</td>\n      <td>Top “Applied Data Science” Papers from ECML-PK...</td>\n      <td>Explore the innovative world of Machine Learni...</td>\n      <td>[article, deep-learning, machine-learning, adv...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2434</td>\n      <td>OpenMMLab Computer Vision</td>\n      <td>MMCV is a python library for CV research and s...</td>\n      <td>[article, code, pytorch, library, 3d, computer...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Creating a pandas dataframe \n",
    "df=pd.DataFrame(projects)\n",
    "print(f\"{len(df)} projects\")\n",
    "df.head(5)"
   ]
  },
  {
   "source": [
    "#Loading the tags\n",
    "url1=\"https://raw.githubusercontent.com/GokuMohandas/applied-ml/main/datasets/tags.json\"\n",
    "tags_dict=OrderedDict(json.loads(urlopen(url1).read()))\n",
    "print(f\"{len(tags_dict)} tags\")\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "400 tags\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='tag', index=283, options=('3d', 'action-localization', 'action-rec…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "813603682a0643598d24dee5774d3da8"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(tag=list(tags_dict.keys()))\n",
    "def display_tag_details(tag='question-answering'):\n",
    "    print(json.dumps(tags_dict[tag],indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     id                                              title  \\\n",
       "0  2438  How to Deal with Files in Google Colab: What Y...   \n",
       "1  2437                                             Rasoee   \n",
       "2  2436    Machine Learning Methods Explained (+ Examples)   \n",
       "3  2435  Top “Applied Data Science” Papers from ECML-PK...   \n",
       "4  2434                          OpenMMLab Computer Vision   \n",
       "\n",
       "                                         description  \\\n",
       "0  How to supercharge your Google Colab experienc...   \n",
       "1  A powerful web and mobile application that ide...   \n",
       "2  Most common techniques used in data science pr...   \n",
       "3  Explore the innovative world of Machine Learni...   \n",
       "4  MMCV is a python library for CV research and s...   \n",
       "\n",
       "                                                tags  \\\n",
       "0        [article, google-colab, colab, file-system]   \n",
       "1  [api, article, code, dataset, paper, research,...   \n",
       "2  [article, deep-learning, machine-learning, dim...   \n",
       "3  [article, deep-learning, machine-learning, adv...   \n",
       "4  [article, code, pytorch, library, 3d, computer...   \n",
       "\n",
       "                                                text  \n",
       "0  How to Deal with Files in Google Colab: What Y...  \n",
       "1  Rasoee A powerful web and mobile application t...  \n",
       "2  Machine Learning Methods Explained (+ Examples...  \n",
       "3  Top “Applied Data Science” Papers from ECML-PK...  \n",
       "4  OpenMMLab Computer Vision MMCV is a python lib...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>description</th>\n      <th>tags</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2438</td>\n      <td>How to Deal with Files in Google Colab: What Y...</td>\n      <td>How to supercharge your Google Colab experienc...</td>\n      <td>[article, google-colab, colab, file-system]</td>\n      <td>How to Deal with Files in Google Colab: What Y...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2437</td>\n      <td>Rasoee</td>\n      <td>A powerful web and mobile application that ide...</td>\n      <td>[api, article, code, dataset, paper, research,...</td>\n      <td>Rasoee A powerful web and mobile application t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2436</td>\n      <td>Machine Learning Methods Explained (+ Examples)</td>\n      <td>Most common techniques used in data science pr...</td>\n      <td>[article, deep-learning, machine-learning, dim...</td>\n      <td>Machine Learning Methods Explained (+ Examples...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2435</td>\n      <td>Top “Applied Data Science” Papers from ECML-PK...</td>\n      <td>Explore the innovative world of Machine Learni...</td>\n      <td>[article, deep-learning, machine-learning, adv...</td>\n      <td>Top “Applied Data Science” Papers from ECML-PK...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2434</td>\n      <td>OpenMMLab Computer Vision</td>\n      <td>MMCV is a python library for CV research and s...</td>\n      <td>[article, code, pytorch, library, 3d, computer...</td>\n      <td>OpenMMLab Computer Vision MMCV is a python lib...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#Combining the projects title and descrption separately as feature but we'll combine them to create on input feature\n",
    "\n",
    "df['text']= df.title+\" \"+ df.description\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constrains\n",
    "def filter(l,include=[],exclude=[]):\n",
    "    \"\"\"\n",
    "    Filtering a list using inclution and exclution list of items \n",
    "    \"\"\"\n",
    "    filtered=[item for item in l if item in include and item not in exclude]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inclusion/exclusion criteria for tags\n",
    "include = list(tags_dict.keys())\n",
    "exclude = ['machine-learning', 'deep-learning',  'data-science',\n",
    "           'neural-networks', 'python', 'r', 'visualization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter tags for each project\n",
    "df.tags=df.tags.apply(filter,include=include,exclude=exclude)\n",
    "tags=Counter(itertools.chain.from_iterable(df.tags.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=30, description='min_tag_freq', max=429), Output()), _dom_classes=('widg…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1591b971dbf14f748be2d4c568a04714"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(min_tag_freq=(0,tags.most_common()[0][1]))\n",
    "def separate_tag_by_freq(min_tag_freq=30):\n",
    "    tags_above_freq = Counter(tag for tag in tags.elements()\n",
    "                                    if tags[tag] >= min_tag_freq)\n",
    "    tags_below_freq = Counter(tag for tag in tags.elements()\n",
    "                                    if tags[tag] < min_tag_freq)\n",
    "    print (\"Most popular tags:\\n\", tags_above_freq.most_common(5))\n",
    "    print (\"\\nTags that just made the cut:\\n\", tags_above_freq.most_common()[-5:])\n",
    "    print (\"\\nTags that just missed the cut:\\n\", tags_below_freq.most_common(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tag_freq = 30\n",
    "tags_above_freq = Counter(tag for tag in tags.elements()\n",
    "                          if tags[tag] >= min_tag_freq)\n",
    "df.tags = df.tags.apply(filter, include=list(tags_above_freq.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1444 projects\n"
     ]
    }
   ],
   "source": [
    "df = df[df.tags.map(len) > 0]\n",
    "print (f\"{len(df)} projects\")"
   ]
  },
  {
   "source": [
    "# Exploratory data analysis\n",
    "NOTE: Revisit and re-do EDA when your data grows"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n  File \"c:\\python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\python39\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Python39\\Scripts\\pip.exe\\__main__.py\", line 4, in <module>\nModuleNotFoundError: No module named 'pip'\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "source": [
    "## Tags Per Project\n",
    "Q. How many tags do the projects have ?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 720x216 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"240.646875pt\" version=\"1.1\" viewBox=\"0 0 617.9725 240.646875\" width=\"617.9725pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 240.646875 \r\nL 617.9725 240.646875 \r\nL 617.9725 -0 \r\nL 0 -0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 52.7725 191.476875 \r\nL 610.7725 191.476875 \r\nL 610.7725 28.396875 \r\nL 52.7725 28.396875 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 59.7475 191.476875 \r\nL 115.5475 191.476875 \r\nL 115.5475 36.162589 \r\nL 59.7475 36.162589 \r\nz\r\n\" style=\"fill:#3274a1;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 129.4975 191.476875 \r\nL 185.2975 191.476875 \r\nL 185.2975 81.04325 \r\nL 129.4975 81.04325 \r\nz\r\n\" style=\"fill:#e1812c;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 199.2475 191.476875 \r\nL 255.0475 191.476875 \r\nL 255.0475 130.547978 \r\nL 199.2475 130.547978 \r\nz\r\n\" style=\"fill:#3a923a;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 268.9975 191.476875 \r\nL 324.7975 191.476875 \r\nL 324.7975 148.228239 \r\nL 268.9975 148.228239 \r\nz\r\n\" style=\"fill:#c03d3e;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 338.7475 191.476875 \r\nL 394.5475 191.476875 \r\nL 394.5475 174.884631 \r\nL 338.7475 174.884631 \r\nz\r\n\" style=\"fill:#9372b2;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 408.4975 191.476875 \r\nL 464.2975 191.476875 \r\nL 464.2975 186.852807 \r\nL 408.4975 186.852807 \r\nz\r\n\" style=\"fill:#845b53;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 478.2475 191.476875 \r\nL 534.0475 191.476875 \r\nL 534.0475 190.660863 \r\nL 478.2475 190.660863 \r\nz\r\n\" style=\"fill:#d684bd;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 547.9975 191.476875 \r\nL 603.7975 191.476875 \r\nL 603.7975 190.660863 \r\nL 547.9975 190.660863 \r\nz\r\n\" style=\"fill:#7f7f7f;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m2efa0be6f1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"87.6475\" xlink:href=\"#m2efa0be6f1\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 1 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(82.5575 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"157.3975\" xlink:href=\"#m2efa0be6f1\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(152.3075 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"227.1475\" xlink:href=\"#m2efa0be6f1\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 3 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(222.0575 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"296.8975\" xlink:href=\"#m2efa0be6f1\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(291.8075 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"366.6475\" xlink:href=\"#m2efa0be6f1\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(361.5575 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"436.3975\" xlink:href=\"#m2efa0be6f1\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(431.3075 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"506.1475\" xlink:href=\"#m2efa0be6f1\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 7 -->\r\n      <defs>\r\n       <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n      </defs>\r\n      <g transform=\"translate(501.0575 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"575.8975\" xlink:href=\"#m2efa0be6f1\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(570.8075 210.634375)scale(0.16 -0.16)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_9\">\r\n     <!-- Number of tags -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 23.09375 72.90625 \r\nL 55.421875 11.921875 \r\nL 55.421875 72.90625 \r\nL 64.984375 72.90625 \r\nL 64.984375 0 \r\nL 51.703125 0 \r\nL 19.390625 60.984375 \r\nL 19.390625 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-78\"/>\r\n      <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n      <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(269.55375 230.119375)scale(0.16 -0.16)\">\r\n      <use xlink:href=\"#DejaVuSans-78\"/>\r\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"138.183594\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"235.595703\" xlink:href=\"#DejaVuSans-98\"/>\r\n      <use x=\"299.072266\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"360.595703\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"401.708984\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"433.496094\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"494.677734\" xlink:href=\"#DejaVuSans-102\"/>\r\n      <use x=\"529.882812\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"561.669922\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"600.878906\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"662.158203\" xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"725.634766\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_9\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m01c0fe611e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m01c0fe611e\" y=\"191.476875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(39.41 195.276094)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m01c0fe611e\" y=\"164.276475\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(26.685 168.075693)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m01c0fe611e\" y=\"137.076074\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(26.685 140.875293)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m01c0fe611e\" y=\"109.875674\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(26.685 113.674893)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m01c0fe611e\" y=\"82.675274\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(26.685 86.474493)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m01c0fe611e\" y=\"55.474873\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 500 -->\r\n      <g transform=\"translate(26.685 59.274092)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- Number of projects -->\r\n     <defs>\r\n      <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 -0.984375 \r\nQ 18.40625 -11.421875 14.421875 -16.109375 \r\nQ 10.453125 -20.796875 1.609375 -20.796875 \r\nL -1.8125 -20.796875 \r\nL -1.8125 -13.1875 \r\nL 0.59375 -13.1875 \r\nQ 5.71875 -13.1875 7.5625 -10.8125 \r\nQ 9.421875 -8.453125 9.421875 -0.984375 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-106\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n     </defs>\r\n     <g transform=\"translate(19.3575 186.978125)rotate(-90)scale(0.16 -0.16)\">\r\n      <use xlink:href=\"#DejaVuSans-78\"/>\r\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"138.183594\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"235.595703\" xlink:href=\"#DejaVuSans-98\"/>\r\n      <use x=\"299.072266\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"360.595703\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"401.708984\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"433.496094\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"494.677734\" xlink:href=\"#DejaVuSans-102\"/>\r\n      <use x=\"529.882812\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"561.669922\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"625.146484\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"666.228516\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"727.410156\" xlink:href=\"#DejaVuSans-106\"/>\r\n      <use x=\"755.193359\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"816.716797\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"871.697266\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"910.90625\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_19\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_20\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_21\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_22\">\r\n    <path clip-path=\"url(#pe1e6c3ecea)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path d=\"M 52.7725 191.476875 \r\nL 52.7725 28.396875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path d=\"M 610.7725 191.476875 \r\nL 610.7725 28.396875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path d=\"M 52.7725 191.476875 \r\nL 610.7725 191.476875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path d=\"M 52.7725 28.396875 \r\nL 610.7725 28.396875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_17\">\r\n    <!-- Tags per project -->\r\n    <defs>\r\n     <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n    </defs>\r\n    <g transform=\"translate(250.11 22.396875)scale(0.2 -0.2)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n     <use x=\"60.833984\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"122.113281\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"185.589844\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"237.689453\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"269.476562\" xlink:href=\"#DejaVuSans-112\"/>\r\n     <use x=\"332.953125\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"394.476562\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"435.589844\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"467.376953\" xlink:href=\"#DejaVuSans-112\"/>\r\n     <use x=\"530.853516\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"571.935547\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"633.117188\" xlink:href=\"#DejaVuSans-106\"/>\r\n     <use x=\"660.900391\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"722.423828\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"777.404297\" xlink:href=\"#DejaVuSans-116\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pe1e6c3ecea\">\r\n   <rect height=\"163.08\" width=\"558\" x=\"52.7725\" y=\"28.396875\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "num_tags_per_project=[len(tags) for tags in df.tags]\n",
    "num_tags,num_projects=zip(*Counter(num_tags_per_project).items())\n",
    "plt.figure(figsize=(10,3))\n",
    "ax=sns.barplot(list(num_tags),list(num_projects))\n",
    "plt.title(\"Tags per project\",fontsize=20)\n",
    "plt.xlabel(\"Number of tags\", fontsize=16)\n",
    "ax.set_xticklabels(range(1, len(num_tags)+1), rotation=0, fontsize=16)\n",
    "plt.ylabel(\"Number of projects\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Tag distrbution\n",
    "What are the most popular tags?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1800x360 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"561.84125pt\" version=\"1.1\" viewBox=\"0 0 1454.9725 561.84125\" width=\"1454.9725pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 561.84125 \r\nL 1454.9725 561.84125 \r\nL 1454.9725 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 52.7725 300.196875 \r\nL 1447.7725 300.196875 \r\nL 1447.7725 28.396875 \r\nL 52.7725 28.396875 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 56.758214 300.196875 \r\nL 88.643929 300.196875 \r\nL 88.643929 41.339732 \r\nL 56.758214 41.339732 \r\nz\r\n\" style=\"fill:#ea96a3;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 96.615357 300.196875 \r\nL 128.501071 300.196875 \r\nL 128.501071 66.078993 \r\nL 96.615357 66.078993 \r\nz\r\n\" style=\"fill:#ea9794;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 136.4725 300.196875 \r\nL 168.358214 300.196875 \r\nL 168.358214 144.520551 \r\nL 136.4725 144.520551 \r\nz\r\n\" style=\"fill:#e6957c;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 176.329643 300.196875 \r\nL 208.215357 300.196875 \r\nL 208.215357 171.673398 \r\nL 176.329643 171.673398 \r\nz\r\n\" style=\"fill:#e0914f;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 216.186786 300.196875 \r\nL 248.0725 300.196875 \r\nL 248.0725 181.931141 \r\nL 216.186786 181.931141 \r\nz\r\n\" style=\"fill:#cf964d;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 256.043929 300.196875 \r\nL 287.929643 300.196875 \r\nL 287.929643 227.789283 \r\nL 256.043929 227.789283 \r\nz\r\n\" style=\"fill:#c29a4b;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 295.901071 300.196875 \r\nL 327.786786 300.196875 \r\nL 327.786786 236.236835 \r\nL 295.901071 236.236835 \r\nz\r\n\" style=\"fill:#b79c49;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 335.758214 300.196875 \r\nL 367.643929 300.196875 \r\nL 367.643929 244.080991 \r\nL 335.758214 244.080991 \r\nz\r\n\" style=\"fill:#ab9e47;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 375.615357 300.196875 \r\nL 407.501071 300.196875 \r\nL 407.501071 253.13194 \r\nL 375.615357 253.13194 \r\nz\r\n\" style=\"fill:#a09f45;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 415.4725 300.196875 \r\nL 447.358214 300.196875 \r\nL 447.358214 254.94213 \r\nL 415.4725 254.94213 \r\nz\r\n\" style=\"fill:#95a346;\"/>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 455.329643 300.196875 \r\nL 487.215357 300.196875 \r\nL 487.215357 256.148923 \r\nL 455.329643 256.148923 \r\nz\r\n\" style=\"fill:#88a746;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 495.186786 300.196875 \r\nL 527.0725 300.196875 \r\nL 527.0725 258.562509 \r\nL 495.186786 258.562509 \r\nz\r\n\" style=\"fill:#75ab47;\"/>\r\n   </g>\r\n   <g id=\"patch_15\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 535.043929 300.196875 \r\nL 566.929643 300.196875 \r\nL 566.929643 261.579492 \r\nL 535.043929 261.579492 \r\nz\r\n\" style=\"fill:#4fb047;\"/>\r\n   </g>\r\n   <g id=\"patch_16\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 574.901071 300.196875 \r\nL 606.786786 300.196875 \r\nL 606.786786 263.993079 \r\nL 574.901071 263.993079 \r\nz\r\n\" style=\"fill:#48af6f;\"/>\r\n   </g>\r\n   <g id=\"patch_17\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 614.758214 300.196875 \r\nL 646.643929 300.196875 \r\nL 646.643929 264.596475 \r\nL 614.758214 264.596475 \r\nz\r\n\" style=\"fill:#49ae83;\"/>\r\n   </g>\r\n   <g id=\"patch_18\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 654.615357 300.196875 \r\nL 686.501071 300.196875 \r\nL 686.501071 265.803269 \r\nL 654.615357 265.803269 \r\nz\r\n\" style=\"fill:#4aad8f;\"/>\r\n   </g>\r\n   <g id=\"patch_19\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 694.4725 300.196875 \r\nL 726.358214 300.196875 \r\nL 726.358214 267.010062 \r\nL 694.4725 267.010062 \r\nz\r\n\" style=\"fill:#4aac99;\"/>\r\n   </g>\r\n   <g id=\"patch_20\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 734.329643 300.196875 \r\nL 766.215357 300.196875 \r\nL 766.215357 269.423648 \r\nL 734.329643 269.423648 \r\nz\r\n\" style=\"fill:#4baca1;\"/>\r\n   </g>\r\n   <g id=\"patch_21\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 774.186786 300.196875 \r\nL 806.0725 300.196875 \r\nL 806.0725 269.423648 \r\nL 774.186786 269.423648 \r\nz\r\n\" style=\"fill:#4baba8;\"/>\r\n   </g>\r\n   <g id=\"patch_22\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 814.043929 300.196875 \r\nL 845.929643 300.196875 \r\nL 845.929643 269.423648 \r\nL 814.043929 269.423648 \r\nz\r\n\" style=\"fill:#4cabb0;\"/>\r\n   </g>\r\n   <g id=\"patch_23\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 853.901071 300.196875 \r\nL 885.786786 300.196875 \r\nL 885.786786 269.423648 \r\nL 853.901071 269.423648 \r\nz\r\n\" style=\"fill:#4eabb8;\"/>\r\n   </g>\r\n   <g id=\"patch_24\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 893.758214 300.196875 \r\nL 925.643929 300.196875 \r\nL 925.643929 270.630441 \r\nL 893.758214 270.630441 \r\nz\r\n\" style=\"fill:#50acc3;\"/>\r\n   </g>\r\n   <g id=\"patch_25\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 933.615357 300.196875 \r\nL 965.501071 300.196875 \r\nL 965.501071 271.233838 \r\nL 933.615357 271.233838 \r\nz\r\n\" style=\"fill:#53accf;\"/>\r\n   </g>\r\n   <g id=\"patch_26\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 973.4725 300.196875 \r\nL 1005.358214 300.196875 \r\nL 1005.358214 272.440631 \r\nL 973.4725 272.440631 \r\nz\r\n\" style=\"fill:#5aade0;\"/>\r\n   </g>\r\n   <g id=\"patch_27\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 1013.329643 300.196875 \r\nL 1045.215357 300.196875 \r\nL 1045.215357 275.457614 \r\nL 1013.329643 275.457614 \r\nz\r\n\" style=\"fill:#86aee6;\"/>\r\n   </g>\r\n   <g id=\"patch_28\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 1053.186786 300.196875 \r\nL 1085.0725 300.196875 \r\nL 1085.0725 275.457614 \r\nL 1053.186786 275.457614 \r\nz\r\n\" style=\"fill:#a0adea;\"/>\r\n   </g>\r\n   <g id=\"patch_29\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 1093.043929 300.196875 \r\nL 1124.929643 300.196875 \r\nL 1124.929643 276.061011 \r\nL 1093.043929 276.061011 \r\nz\r\n\" style=\"fill:#b2a9eb;\"/>\r\n   </g>\r\n   <g id=\"patch_30\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 1132.901071 300.196875 \r\nL 1164.786786 300.196875 \r\nL 1164.786786 276.061011 \r\nL 1132.901071 276.061011 \r\nz\r\n\" style=\"fill:#c1a3ea;\"/>\r\n   </g>\r\n   <g id=\"patch_31\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 1172.758214 300.196875 \r\nL 1204.643929 300.196875 \r\nL 1204.643929 276.664407 \r\nL 1172.758214 276.664407 \r\nz\r\n\" style=\"fill:#ce9be9;\"/>\r\n   </g>\r\n   <g id=\"patch_32\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 1212.615357 300.196875 \r\nL 1244.501071 300.196875 \r\nL 1244.501071 276.664407 \r\nL 1212.615357 276.664407 \r\nz\r\n\" style=\"fill:#dc91e7;\"/>\r\n   </g>\r\n   <g id=\"patch_33\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 1252.4725 300.196875 \r\nL 1284.358214 300.196875 \r\nL 1284.358214 279.68139 \r\nL 1252.4725 279.68139 \r\nz\r\n\" style=\"fill:#e78ae0;\"/>\r\n   </g>\r\n   <g id=\"patch_34\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 1292.329643 300.196875 \r\nL 1324.215357 300.196875 \r\nL 1324.215357 279.68139 \r\nL 1292.329643 279.68139 \r\nz\r\n\" style=\"fill:#e78dd2;\"/>\r\n   </g>\r\n   <g id=\"patch_35\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 1332.186786 300.196875 \r\nL 1364.0725 300.196875 \r\nL 1364.0725 280.284787 \r\nL 1332.186786 280.284787 \r\nz\r\n\" style=\"fill:#e890c7;\"/>\r\n   </g>\r\n   <g id=\"patch_36\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 1372.043929 300.196875 \r\nL 1403.929643 300.196875 \r\nL 1403.929643 280.888184 \r\nL 1372.043929 280.888184 \r\nz\r\n\" style=\"fill:#e992bc;\"/>\r\n   </g>\r\n   <g id=\"patch_37\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 1411.901071 300.196875 \r\nL 1443.786786 300.196875 \r\nL 1443.786786 282.094977 \r\nL 1411.901071 282.094977 \r\nz\r\n\" style=\"fill:#e994b1;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m50b5b8f1af\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"72.701071\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- natural-language-processing -->\r\n      <defs>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 4.890625 31.390625 \r\nL 31.203125 31.390625 \r\nL 31.203125 23.390625 \r\nL 4.890625 23.390625 \r\nz\r\n\" id=\"DejaVuSans-45\"/>\r\n       <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      </defs>\r\n      <g transform=\"translate(76.564196 508.271875)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"63.378906\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"124.658203\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"163.867188\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"227.246094\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"268.359375\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"329.638672\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"357.421875\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"393.505859\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"421.289062\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"482.568359\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"545.947266\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"609.423828\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"672.802734\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"734.082031\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"797.558594\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"859.082031\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"895.166016\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"958.642578\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"999.724609\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"1060.90625\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"1115.886719\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"1177.410156\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"1229.509766\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"1281.609375\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"1309.392578\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1372.771484\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.558214\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- computer-vision -->\r\n      <defs>\r\n       <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      </defs>\r\n      <g transform=\"translate(116.421339 420.546562)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"116.162109\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"213.574219\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"277.050781\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"340.429688\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"379.638672\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"441.162109\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"482.181641\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"518.234375\" xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"577.414062\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"605.197266\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"657.296875\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"685.080078\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"746.261719\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"152.415357\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- pytorch -->\r\n      <defs>\r\n       <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      </defs>\r\n      <g transform=\"translate(156.278482 360.746875)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-121\"/>\r\n       <use x=\"122.65625\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"161.865234\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"223.046875\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"264.128906\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"319.109375\" xlink:href=\"#DejaVuSans-104\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"192.2725\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- tensorflow -->\r\n      <defs>\r\n       <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n       <path d=\"M 4.203125 54.6875 \r\nL 13.1875 54.6875 \r\nL 24.421875 12.015625 \r\nL 35.59375 54.6875 \r\nL 46.1875 54.6875 \r\nL 57.421875 12.015625 \r\nL 68.609375 54.6875 \r\nL 77.59375 54.6875 \r\nL 63.28125 0 \r\nL 52.6875 0 \r\nL 40.921875 44.828125 \r\nL 29.109375 0 \r\nL 18.5 0 \r\nz\r\n\" id=\"DejaVuSans-119\"/>\r\n      </defs>\r\n      <g transform=\"translate(196.135625 380.620312)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"100.732422\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"164.111328\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"216.210938\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"277.392578\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"318.505859\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"353.710938\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"381.494141\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"442.675781\" xlink:href=\"#DejaVuSans-119\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.129643\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- transformers -->\r\n      <g transform=\"translate(235.992768 397.733125)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"141.601562\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"204.980469\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"257.080078\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"292.285156\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"353.466797\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"394.564453\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"491.976562\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"553.5\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"594.613281\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"271.986786\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- attention -->\r\n      <g transform=\"translate(275.849911 371.056562)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"61.279297\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"100.488281\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"139.697266\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"201.220703\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"264.599609\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"303.808594\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"331.591797\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"392.773438\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"311.843929\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- convolutional-neural-networks -->\r\n      <defs>\r\n       <path d=\"M 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 31.109375 \r\nL 44.921875 54.6875 \r\nL 56.390625 54.6875 \r\nL 27.390625 29.109375 \r\nL 57.625 0 \r\nL 45.90625 0 \r\nL 18.109375 26.703125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nz\r\n\" id=\"DejaVuSans-107\"/>\r\n      </defs>\r\n      <g transform=\"translate(315.707054 520.064687)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"116.162109\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"179.541016\" xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"238.720703\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"299.902344\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"327.685547\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"391.064453\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"430.273438\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"458.056641\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"519.238281\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"582.617188\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"643.896484\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"671.679688\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"707.763672\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"771.142578\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"832.666016\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"896.044922\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"937.158203\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"998.4375\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"1026.220703\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"1062.304688\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1125.683594\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"1187.207031\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"1226.416016\" xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"1308.203125\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"1369.384766\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"1410.498047\" xlink:href=\"#DejaVuSans-107\"/>\r\n       <use x=\"1468.408203\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"351.701071\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- keras -->\r\n      <g transform=\"translate(355.564196 345.539375)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-107\"/>\r\n       <use x=\"57.863281\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"119.386719\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"160.5\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"221.779297\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"391.558214\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- graphs -->\r\n      <g transform=\"translate(395.421339 355.472812)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"104.589844\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"165.869141\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"229.345703\" xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"292.724609\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_10\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"431.415357\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- embeddings -->\r\n      <defs>\r\n       <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      </defs>\r\n      <g transform=\"translate(435.278482 393.66875)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"61.523438\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"158.935547\" xlink:href=\"#DejaVuSans-98\"/>\r\n       <use x=\"222.412109\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"283.935547\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"347.412109\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"410.888672\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"438.671875\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"502.050781\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"565.527344\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_11\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"471.2725\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- generative-adversarial-networks -->\r\n      <g transform=\"translate(475.135625 535.15625)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"125\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"188.378906\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"249.902344\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"291.015625\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"352.294922\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"391.503906\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"419.287109\" xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"478.466797\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"539.990234\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"576.074219\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"637.353516\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"700.830078\" xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"760.009766\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"821.533203\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"862.646484\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"914.746094\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"976.025391\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"1017.138672\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"1044.921875\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"1106.201172\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"1133.984375\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"1170.068359\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1233.447266\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"1294.970703\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"1334.179688\" xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"1415.966797\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"1477.148438\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"1518.261719\" xlink:href=\"#DejaVuSans-107\"/>\r\n       <use x=\"1576.171875\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_12\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"511.129643\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- object-detection -->\r\n      <defs>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 -0.984375 \r\nQ 18.40625 -11.421875 14.421875 -16.109375 \r\nQ 10.453125 -20.796875 1.609375 -20.796875 \r\nL -1.8125 -20.796875 \r\nL -1.8125 -13.1875 \r\nL 0.59375 -13.1875 \r\nQ 5.71875 -13.1875 7.5625 -10.8125 \r\nQ 9.421875 -8.453125 9.421875 -0.984375 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-106\"/>\r\n      </defs>\r\n      <g transform=\"translate(514.992768 421.51125)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"61.181641\" xlink:href=\"#DejaVuSans-98\"/>\r\n       <use x=\"124.658203\" xlink:href=\"#DejaVuSans-106\"/>\r\n       <use x=\"152.441406\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"213.964844\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"268.945312\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"308.154297\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"344.238281\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"407.714844\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"469.238281\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"508.447266\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"569.970703\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"624.951172\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"664.160156\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"691.943359\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"753.125\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_13\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"550.986786\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- huggingface -->\r\n      <g transform=\"translate(554.849911 394.187187)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"63.378906\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"126.757812\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"190.234375\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"253.710938\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"281.494141\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"344.873047\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"408.349609\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"443.554688\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"504.833984\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"559.814453\" xlink:href=\"#DejaVuSans-101\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_14\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"590.843929\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- scikit-learn -->\r\n      <g transform=\"translate(594.707054 384.321562)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"107.080078\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"134.863281\" xlink:href=\"#DejaVuSans-107\"/>\r\n       <use x=\"192.773438\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"220.556641\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"259.765625\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"295.849609\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"323.632812\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"385.15625\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"446.435547\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"487.533203\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_15\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"630.701071\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- reinforcement-learning -->\r\n      <g transform=\"translate(634.564196 468.903437)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"41.082031\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"102.605469\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"130.388672\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"193.767578\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"228.972656\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"290.154297\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"331.236328\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"386.216797\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"447.740234\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"545.152344\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"606.675781\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"670.054688\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"709.263672\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"745.347656\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"773.130859\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"834.654297\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"895.933594\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"937.03125\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1000.410156\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"1028.193359\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1091.572266\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_16\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"670.558214\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- representation-learning -->\r\n      <g transform=\"translate(674.421339 472.889063)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"41.082031\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"102.605469\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"166.082031\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"207.164062\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"268.6875\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"320.787109\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"382.310547\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"445.689453\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"484.898438\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"546.177734\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"585.386719\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"613.169922\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"674.351562\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"737.730469\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"773.814453\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"801.597656\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"863.121094\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"924.400391\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"965.498047\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1028.876953\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"1056.660156\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1120.039062\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_17\">\r\n     <g id=\"line2d_17\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"710.415357\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- interpretability -->\r\n      <g transform=\"translate(714.278482 411.468437)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"91.162109\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"130.371094\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"191.894531\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"233.007812\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"296.484375\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"337.566406\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"399.089844\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"438.298828\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"499.578125\" xlink:href=\"#DejaVuSans-98\"/>\r\n       <use x=\"563.054688\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"590.837891\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"618.621094\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"646.404297\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"685.613281\" xlink:href=\"#DejaVuSans-121\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_18\">\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"750.2725\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_18\">\r\n      <!-- image-classification -->\r\n      <g transform=\"translate(754.135625 446.409375)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"125.195312\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"186.474609\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"249.951172\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"311.474609\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"347.558594\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"402.539062\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"430.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"491.601562\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"543.701172\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"595.800781\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"623.583984\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"658.789062\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"686.572266\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"741.552734\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"802.832031\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"842.041016\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"869.824219\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"931.005859\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_19\">\r\n     <g id=\"line2d_19\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"790.129643\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_19\">\r\n      <!-- production -->\r\n      <g transform=\"translate(793.992768 382.676562)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"104.558594\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"165.740234\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"229.216797\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"292.595703\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"347.576172\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"386.785156\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"414.568359\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"475.75\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_20\">\r\n     <g id=\"line2d_20\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"829.986786\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_20\">\r\n      <!-- language-modeling -->\r\n      <g transform=\"translate(833.849911 442.675313)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"152.441406\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"215.917969\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"279.296875\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"340.576172\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"404.052734\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"465.576172\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"501.660156\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"599.072266\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"660.253906\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"723.730469\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"785.253906\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"813.037109\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"840.820312\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"904.199219\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_21\">\r\n     <g id=\"line2d_21\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"869.843929\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_21\">\r\n      <!-- graph-neural-networks -->\r\n      <g transform=\"translate(873.707054 467.01125)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"104.589844\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"165.869141\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"229.345703\" xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"292.724609\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"328.808594\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"392.1875\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"453.710938\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"517.089844\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"558.203125\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"619.482422\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"647.265625\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"683.349609\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"746.728516\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"808.251953\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"847.460938\" xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"929.248047\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"990.429688\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"1031.542969\" xlink:href=\"#DejaVuSans-107\"/>\r\n       <use x=\"1089.453125\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_22\">\r\n     <g id=\"line2d_22\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"909.701071\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_22\">\r\n      <!-- regression -->\r\n      <g transform=\"translate(913.564196 380.729687)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"41.082031\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"102.605469\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"166.082031\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"207.164062\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"268.6875\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"320.787109\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"372.886719\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"400.669922\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"461.851562\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_23\">\r\n     <g id=\"line2d_23\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"949.558214\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_23\">\r\n      <!-- segmentation -->\r\n      <g transform=\"translate(953.421339 404.000313)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"113.623047\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"177.099609\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"274.511719\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"336.035156\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"399.414062\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"438.623047\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"499.902344\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"539.111328\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"566.894531\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"628.076172\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_24\">\r\n     <g id=\"line2d_24\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"989.415357\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_24\">\r\n      <!-- transfer-learning -->\r\n      <g transform=\"translate(993.278482 424.88)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"141.601562\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"204.980469\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"257.080078\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"292.285156\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"353.808594\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"394.828125\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"430.912109\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"458.695312\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"520.21875\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"581.498047\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"622.595703\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"685.974609\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"713.757812\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"777.136719\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_25\">\r\n     <g id=\"line2d_25\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1029.2725\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_25\">\r\n      <!-- data-augmentation -->\r\n      <g transform=\"translate(1033.135625 442.130625)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"124.755859\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"163.964844\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"225.244141\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"261.328125\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"322.607422\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"385.986328\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"449.462891\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"546.875\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"608.398438\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"671.777344\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"710.986328\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"772.265625\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"811.474609\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"839.257812\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"900.439453\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_26\">\r\n     <g id=\"line2d_26\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1069.129643\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_26\">\r\n      <!-- autoencoders -->\r\n      <g transform=\"translate(1072.992768 403.005)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"61.279297\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"124.658203\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"163.867188\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"225.048828\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"286.572266\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"349.951172\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"404.931641\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"466.113281\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"529.589844\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"591.113281\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"632.226562\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_27\">\r\n     <g id=\"line2d_27\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1108.986786\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_27\">\r\n      <!-- self-supervised-learning -->\r\n      <g transform=\"translate(1112.849911 475.765625)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"113.623047\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"141.40625\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"176.533203\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"212.617188\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"264.716797\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"328.095703\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"391.572266\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"453.095703\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"494.208984\" xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"553.388672\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"581.171875\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"633.271484\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"694.794922\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"758.271484\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"794.355469\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"822.138672\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"883.662109\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"944.941406\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"986.039062\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1049.417969\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"1077.201172\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1140.580078\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_28\">\r\n     <g id=\"line2d_28\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1148.843929\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_28\">\r\n      <!-- tensorflow-js -->\r\n      <g transform=\"translate(1152.707054 396.85375)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"100.732422\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"164.111328\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"216.210938\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"277.392578\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"318.505859\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"353.710938\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"381.494141\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"442.675781\" xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"524.462891\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"560.546875\" xlink:href=\"#DejaVuSans-106\"/>\r\n       <use x=\"588.330078\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_29\">\r\n     <g id=\"line2d_29\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1188.701071\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_29\">\r\n      <!-- unsupervised-learning -->\r\n      <g transform=\"translate(1192.564196 463.745312)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"63.378906\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"126.757812\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"178.857422\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"242.236328\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"305.712891\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"367.236328\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"408.349609\" xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"467.529297\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"495.3125\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"547.412109\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"608.935547\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"672.412109\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"708.496094\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"736.279297\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"797.802734\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"859.082031\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"900.179688\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"963.558594\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"991.341797\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"1054.720703\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_30\">\r\n     <g id=\"line2d_30\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1228.558214\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_30\">\r\n      <!-- wandb -->\r\n      <g transform=\"translate(1232.421339 353.87375)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"81.787109\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"143.066406\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"206.445312\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"269.921875\" xlink:href=\"#DejaVuSans-98\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_31\">\r\n     <g id=\"line2d_31\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1268.415357\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_31\">\r\n      <!-- time-series -->\r\n      <g transform=\"translate(1272.278482 385.336563)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"39.208984\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"66.992188\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"164.404297\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"225.927734\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"262.011719\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"314.111328\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"375.634766\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"416.748047\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"444.53125\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"506.054688\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_32\">\r\n     <g id=\"line2d_32\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1308.2725\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_32\">\r\n      <!-- flask -->\r\n      <g transform=\"translate(1312.135625 339.994062)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"35.205078\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"62.988281\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"124.267578\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"176.367188\" xlink:href=\"#DejaVuSans-107\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_33\">\r\n     <g id=\"line2d_33\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1348.129643\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_33\">\r\n      <!-- node-classification -->\r\n      <g transform=\"translate(1351.992768 437.7425)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"63.378906\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"124.560547\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"188.037109\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"249.560547\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"285.644531\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"340.625\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"368.408203\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"429.6875\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"481.787109\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"533.886719\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"561.669922\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"596.875\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"624.658203\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"679.638672\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"740.917969\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"780.126953\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"807.910156\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"869.091797\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_34\">\r\n     <g id=\"line2d_34\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1387.986786\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_34\">\r\n      <!-- question-answering -->\r\n      <defs>\r\n       <path d=\"M 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\nM 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nL 54.390625 -20.796875 \r\nL 45.40625 -20.796875 \r\nz\r\n\" id=\"DejaVuSans-113\"/>\r\n      </defs>\r\n      <g transform=\"translate(1391.849911 444.945938)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-113\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"126.855469\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"188.378906\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"240.478516\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"279.6875\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"307.470703\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"368.652344\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"432.03125\" xlink:href=\"#DejaVuSans-45\"/>\r\n       <use x=\"468.115234\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"529.394531\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"592.773438\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"644.873047\" xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"726.660156\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"788.183594\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"829.296875\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"857.080078\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"920.458984\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_35\">\r\n     <g id=\"line2d_35\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"1427.843929\" xlink:href=\"#m50b5b8f1af\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_35\">\r\n      <!-- pretraining -->\r\n      <g transform=\"translate(1431.707054 384.684687)rotate(-90)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"104.558594\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"166.082031\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"205.291016\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"246.404297\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"307.683594\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"335.466797\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"398.845703\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"426.628906\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"490.007812\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_36\">\r\n     <!-- Tag -->\r\n     <defs>\r\n      <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n     </defs>\r\n     <g transform=\"translate(735.425 551.31375)scale(0.16 -0.16)\">\r\n      <use xlink:href=\"#DejaVuSans-84\"/>\r\n      <use x=\"60.833984\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"122.113281\" xlink:href=\"#DejaVuSans-103\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_36\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m78ff371d6f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m78ff371d6f\" y=\"300.196875\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_37\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(39.41 303.996094)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_37\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m78ff371d6f\" y=\"239.857215\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_38\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(26.685 243.656433)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_38\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m78ff371d6f\" y=\"179.517554\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_39\">\r\n      <!-- 200 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(26.685 183.316773)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_39\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m78ff371d6f\" y=\"119.177894\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_40\">\r\n      <!-- 300 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(26.685 122.977113)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_40\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.7725\" xlink:href=\"#m78ff371d6f\" y=\"58.838234\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_41\">\r\n      <!-- 400 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(26.685 62.637452)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_42\">\r\n     <!-- Number of projects -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 23.09375 72.90625 \r\nL 55.421875 11.921875 \r\nL 55.421875 72.90625 \r\nL 64.984375 72.90625 \r\nL 64.984375 0 \r\nL 51.703125 0 \r\nL 19.390625 60.984375 \r\nL 19.390625 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-78\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n     </defs>\r\n     <g transform=\"translate(19.3575 241.338125)rotate(-90)scale(0.16 -0.16)\">\r\n      <use xlink:href=\"#DejaVuSans-78\"/>\r\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"138.183594\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"235.595703\" xlink:href=\"#DejaVuSans-98\"/>\r\n      <use x=\"299.072266\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"360.595703\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"401.708984\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"433.496094\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"494.677734\" xlink:href=\"#DejaVuSans-102\"/>\r\n      <use x=\"529.882812\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"561.669922\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"625.146484\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"666.228516\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"727.410156\" xlink:href=\"#DejaVuSans-106\"/>\r\n      <use x=\"755.193359\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"816.716797\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"871.697266\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"910.90625\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_41\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_42\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_43\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_44\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_45\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_46\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_47\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_48\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_49\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_50\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_51\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_52\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_53\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_54\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_55\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_56\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_57\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_58\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_59\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_60\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_61\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_62\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_63\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_64\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_65\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_66\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_67\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_68\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_69\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_70\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_71\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_72\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_73\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_74\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"line2d_75\">\r\n    <path clip-path=\"url(#p680fa91f3b)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#424242;stroke-linecap:square;stroke-width:2.7;\"/>\r\n   </g>\r\n   <g id=\"patch_38\">\r\n    <path d=\"M 52.7725 300.196875 \r\nL 52.7725 28.396875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_39\">\r\n    <path d=\"M 1447.7725 300.196875 \r\nL 1447.7725 28.396875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_40\">\r\n    <path d=\"M 52.7725 300.196875 \r\nL 1447.7725 300.196875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_41\">\r\n    <path d=\"M 52.7725 28.396875 \r\nL 1447.7725 28.396875 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_43\">\r\n    <!-- Tag distribution -->\r\n    <g transform=\"translate(671.549062 22.396875)scale(0.2 -0.2)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n     <use x=\"60.833984\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"122.113281\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"185.589844\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"217.376953\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"280.853516\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"308.636719\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"360.736328\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"399.945312\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"441.058594\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"468.841797\" xlink:href=\"#DejaVuSans-98\"/>\r\n     <use x=\"532.318359\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"595.697266\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"634.90625\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"662.689453\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"723.871094\" xlink:href=\"#DejaVuSans-110\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p680fa91f3b\">\r\n   <rect height=\"271.8\" width=\"1395\" x=\"52.7725\" y=\"28.396875\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "all_tags = list(itertools.chain.from_iterable(df.tags.values))\n",
    "tags, tag_counts = zip(*Counter(all_tags).most_common())\n",
    "plt.figure(figsize=(25, 5))\n",
    "ax = sns.barplot(list(tags), list(tag_counts))\n",
    "plt.title(\"Tag distribution\", fontsize=20)\n",
    "plt.xlabel(\"Tag\", fontsize=16)\n",
    "ax.set_xticklabels(tags, rotation=90, fontsize=14)\n",
    "plt.ylabel(\"Number of projects\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Wordcloud\n",
    "Is there enough signal in the title and description that's unique to each tag?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='tag', index=2, options=('natural-language-processing', 'computer-v…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e48cdb5c0ca4211bc6858b8427499ce"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(tag=list(tags))\n",
    "def display_word_cloud(tag='pytorch'):\n",
    "    # Plot word clouds top top tags\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    subset = df[df.tags.apply(lambda tags: tag in tags)]\n",
    "    text = subset.text.values\n",
    "    cloud = WordCloud(\n",
    "        stopwords=STOPWORDS, background_color='black', collocations=False,\n",
    "        width=500, height=300).generate(\" \".join(text))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cloud)"
   ]
  },
  {
   "source": [
    "# Splitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "X = df.text.to_numpy()\n",
    "y = df.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(object):\n",
    "    \"\"\"Label encoder for tag labels.\"\"\"\n",
    "    def __init__(self, class_to_index={}):\n",
    "        self.class_to_index = class_to_index\n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        self.classes = list(self.class_to_index.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_to_index)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<LabelEncoder(num_classes={len(self)})>\"\n",
    "\n",
    "    def fit(self, y):\n",
    "        classes = np.unique(list(itertools.chain.from_iterable(y)))\n",
    "        for i, class_ in enumerate(classes):\n",
    "            self.class_to_index[class_] = i\n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        self.classes = list(self.class_to_index.keys())\n",
    "        return self\n",
    "\n",
    "    def encode(self, y):\n",
    "        y_one_hot = np.zeros((len(y), len(self.class_to_index)),dtype=int)\n",
    "        for i, item in enumerate(y):\n",
    "            for class_ in item:\n",
    "                y_one_hot[i][self.class_to_index[class_]] = 1\n",
    "        return y_one_hot\n",
    "\n",
    "    def decode(self, y):\n",
    "        classes = []\n",
    "        for i, item in enumerate(y):\n",
    "            indices = np.where(item == 1)[0]\n",
    "            classes.append([self.index_to_class[index] for index in indices])\n",
    "        return classes\n",
    "\n",
    "    def save(self, fp):\n",
    "        with open(fp, 'w') as fp:\n",
    "            contents = {'class_to_index': self.class_to_index}\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        with open(fp, 'r') as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode \n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "num_classes = len(label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention': 0,\n",
       " 'autoencoders': 1,\n",
       " 'computer-vision': 2,\n",
       " 'convolutional-neural-networks': 3,\n",
       " 'data-augmentation': 4,\n",
       " 'embeddings': 5,\n",
       " 'flask': 6,\n",
       " 'generative-adversarial-networks': 7,\n",
       " 'graph-neural-networks': 8,\n",
       " 'graphs': 9,\n",
       " 'huggingface': 10,\n",
       " 'image-classification': 11,\n",
       " 'interpretability': 12,\n",
       " 'keras': 13,\n",
       " 'language-modeling': 14,\n",
       " 'natural-language-processing': 15,\n",
       " 'node-classification': 16,\n",
       " 'object-detection': 17,\n",
       " 'pretraining': 18,\n",
       " 'production': 19,\n",
       " 'pytorch': 20,\n",
       " 'question-answering': 21,\n",
       " 'regression': 22,\n",
       " 'reinforcement-learning': 23,\n",
       " 'representation-learning': 24,\n",
       " 'scikit-learn': 25,\n",
       " 'segmentation': 26,\n",
       " 'self-supervised-learning': 27,\n",
       " 'tensorflow': 28,\n",
       " 'tensorflow-js': 29,\n",
       " 'time-series': 30,\n",
       " 'transfer-learning': 31,\n",
       " 'transformers': 32,\n",
       " 'unsupervised-learning': 33,\n",
       " 'wandb': 34}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "\n",
    "label_encoder.class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = label_encoder.encode(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n  File \"c:\\python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\python39\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Python39\\Scripts\\pip.exe\\__main__.py\", line 4, in <module>\nModuleNotFoundError: No module named 'pip'\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-multilearn==0.2.0 -q"
   ]
  },
  {
   "source": [
    "Naive splitiing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.model_selection.measures import get_combination_wise_output_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sizes\n",
    "train_size=0.7\n",
    "val_size=0.15\n",
    "test_size=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split (train)\n",
    "X_train, X_, y_train, y_ = train_test_split(X, y, train_size=train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train: 1010 (0.70)\nremaining: 434 (0.30)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (f\"train: {len(X_train)} ({(len(X_train) / len(X)):.2f})\\n\"\n",
    "       f\"remaining: {len(X_)} ({(len(X_) / len(X)):.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_, y_, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train: 1010 (0.70)\nval: 217 (0.15)\ntest: 217 (0.15)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train: {len(X_train)} ({len(X_train)/len(X):.2f})\\n\"\n",
    "      f\"val: {len(X_val)} ({len(X_val)/len(X):.2f})\\n\"\n",
    "      f\"test: {len(X_test)} ({len(X_test)/len(X):.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts for each class\n",
    "counts = {}\n",
    "counts['train_counts'] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "    y_train, order=1) for combination in row)\n",
    "counts['val_counts'] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "    y_val, order=1) for combination in row)\n",
    "counts['test_counts'] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "    y_test, order=1) for combination in row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       (15,)  (19,)  (33,)  (21,)  (32,)  (14,)  (2,)  (20,)  (24,)  (5,)  \\\n",
       "train    314     37     26     26    145     33   274    191     41    55   \n",
       "val       58      8      4      2     29      8    53     33      7     9   \n",
       "test      57      6      9      4     22     10    61     34      9    11   \n",
       "\n",
       "       ...  (1,)  (7,)  (11,)  (18,)  (4,)  (6,)  (29,)  (8,)  (31,)  (30,)  \n",
       "train  ...    27    50     34     21    33    24     24    32     33     23  \n",
       "val    ...     3     7      6      3     3     5      7    10      7      4  \n",
       "test   ...    11    16     11      6     5     5      9     9      6      7  \n",
       "\n",
       "[3 rows x 35 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>(15,)</th>\n      <th>(19,)</th>\n      <th>(33,)</th>\n      <th>(21,)</th>\n      <th>(32,)</th>\n      <th>(14,)</th>\n      <th>(2,)</th>\n      <th>(20,)</th>\n      <th>(24,)</th>\n      <th>(5,)</th>\n      <th>...</th>\n      <th>(1,)</th>\n      <th>(7,)</th>\n      <th>(11,)</th>\n      <th>(18,)</th>\n      <th>(4,)</th>\n      <th>(6,)</th>\n      <th>(29,)</th>\n      <th>(8,)</th>\n      <th>(31,)</th>\n      <th>(30,)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train</th>\n      <td>314</td>\n      <td>37</td>\n      <td>26</td>\n      <td>26</td>\n      <td>145</td>\n      <td>33</td>\n      <td>274</td>\n      <td>191</td>\n      <td>41</td>\n      <td>55</td>\n      <td>...</td>\n      <td>27</td>\n      <td>50</td>\n      <td>34</td>\n      <td>21</td>\n      <td>33</td>\n      <td>24</td>\n      <td>24</td>\n      <td>32</td>\n      <td>33</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>val</th>\n      <td>58</td>\n      <td>8</td>\n      <td>4</td>\n      <td>2</td>\n      <td>29</td>\n      <td>8</td>\n      <td>53</td>\n      <td>33</td>\n      <td>7</td>\n      <td>9</td>\n      <td>...</td>\n      <td>3</td>\n      <td>7</td>\n      <td>6</td>\n      <td>3</td>\n      <td>3</td>\n      <td>5</td>\n      <td>7</td>\n      <td>10</td>\n      <td>7</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>57</td>\n      <td>6</td>\n      <td>9</td>\n      <td>4</td>\n      <td>22</td>\n      <td>10</td>\n      <td>61</td>\n      <td>34</td>\n      <td>9</td>\n      <td>11</td>\n      <td>...</td>\n      <td>11</td>\n      <td>16</td>\n      <td>11</td>\n      <td>6</td>\n      <td>5</td>\n      <td>5</td>\n      <td>9</td>\n      <td>9</td>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 35 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# View distributions\n",
    "pd.DataFrame({\n",
    "    'train': counts['train_counts'],\n",
    "    'val': counts['val_counts'],\n",
    "    'test': counts['test_counts']\n",
    "}).T.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust counts across splits\n",
    "for k in counts['val_counts'].keys():\n",
    "    counts['val_counts'][k] = int(counts['val_counts'][k] * \\\n",
    "        (train_size/val_size))\n",
    "for k in counts['test_counts'].keys():\n",
    "    counts['test_counts'][k] = int(counts['test_counts'][k] * \\\n",
    "        (train_size/test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       (15,)  (19,)  (33,)  (21,)  (32,)  (14,)  (2,)  (20,)  (24,)  (5,)  \\\n",
       "train    314     37     26     26    145     33   274    191     41    55   \n",
       "val      270     37     18      9    135     37   247    154     32    42   \n",
       "test     266     28     42     18    102     46   284    158     42    51   \n",
       "\n",
       "       ...  (1,)  (7,)  (11,)  (18,)  (4,)  (6,)  (29,)  (8,)  (31,)  (30,)  \n",
       "train  ...    27    50     34     21    33    24     24    32     33     23  \n",
       "val    ...    14    32     28     14    14    23     32    46     32     18  \n",
       "test   ...    51    74     51     28    23    23     42    42     28     32  \n",
       "\n",
       "[3 rows x 35 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>(15,)</th>\n      <th>(19,)</th>\n      <th>(33,)</th>\n      <th>(21,)</th>\n      <th>(32,)</th>\n      <th>(14,)</th>\n      <th>(2,)</th>\n      <th>(20,)</th>\n      <th>(24,)</th>\n      <th>(5,)</th>\n      <th>...</th>\n      <th>(1,)</th>\n      <th>(7,)</th>\n      <th>(11,)</th>\n      <th>(18,)</th>\n      <th>(4,)</th>\n      <th>(6,)</th>\n      <th>(29,)</th>\n      <th>(8,)</th>\n      <th>(31,)</th>\n      <th>(30,)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train</th>\n      <td>314</td>\n      <td>37</td>\n      <td>26</td>\n      <td>26</td>\n      <td>145</td>\n      <td>33</td>\n      <td>274</td>\n      <td>191</td>\n      <td>41</td>\n      <td>55</td>\n      <td>...</td>\n      <td>27</td>\n      <td>50</td>\n      <td>34</td>\n      <td>21</td>\n      <td>33</td>\n      <td>24</td>\n      <td>24</td>\n      <td>32</td>\n      <td>33</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>val</th>\n      <td>270</td>\n      <td>37</td>\n      <td>18</td>\n      <td>9</td>\n      <td>135</td>\n      <td>37</td>\n      <td>247</td>\n      <td>154</td>\n      <td>32</td>\n      <td>42</td>\n      <td>...</td>\n      <td>14</td>\n      <td>32</td>\n      <td>28</td>\n      <td>14</td>\n      <td>14</td>\n      <td>23</td>\n      <td>32</td>\n      <td>46</td>\n      <td>32</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>266</td>\n      <td>28</td>\n      <td>42</td>\n      <td>18</td>\n      <td>102</td>\n      <td>46</td>\n      <td>284</td>\n      <td>158</td>\n      <td>42</td>\n      <td>51</td>\n      <td>...</td>\n      <td>51</td>\n      <td>74</td>\n      <td>51</td>\n      <td>28</td>\n      <td>23</td>\n      <td>23</td>\n      <td>42</td>\n      <td>42</td>\n      <td>28</td>\n      <td>32</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 35 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "dist_df = pd.DataFrame({\n",
    "    'train': counts['train_counts'],\n",
    "    'val': counts['val_counts'],\n",
    "    'test': counts['test_counts']\n",
    "}).T.fillna(0)\n",
    "dist_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.936725114942407"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "np.mean(np.std(dist_df.to_numpy(), axis=0))"
   ]
  },
  {
   "source": [
    "Stratified split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import IterativeStratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_train_test_split(X, y, train_size):\n",
    "    \"\"\"Custom iterative train test split which\n",
    "    'maintains balanced representation with respect\n",
    "    to order-th label combinations.'\n",
    "    \"\"\"\n",
    "    stratifier = IterativeStratification(\n",
    "        n_splits=2, order=1, sample_distribution_per_fold=[1.0-train_size, train_size, ])\n",
    "    train_indices, test_indices = next(stratifier.split(X, y))\n",
    "    X_train, y_train = X[train_indices], y[train_indices]\n",
    "    X_test, y_test = X[test_indices], y[test_indices]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "X = df.text.to_numpy()\n",
    "y = df.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize y\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "y = label_encoder.encode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_, y_train, y_ = iterative_train_test_split(\n",
    "    X, y, train_size=train_size)\n",
    "X_val, X_test, y_val, y_test = iterative_train_test_split(\n",
    "    X_, y_, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train: 1000 (0.69)\nval: 214 (0.15)\ntest: 230 (0.16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train: {len(X_train)} ({len(X_train)/len(X):.2f})\\n\"\n",
    "      f\"val: {len(X_val)} ({len(X_val)/len(X):.2f})\\n\"\n",
    "      f\"test: {len(X_test)} ({len(X_test)/len(X):.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts for each class\n",
    "counts = {}\n",
    "counts['train_counts'] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "    y_train, order=1) for combination in row)\n",
    "counts['val_counts'] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "    y_val, order=1) for combination in row)\n",
    "counts['test_counts'] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "    y_test, order=1) for combination in row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust counts across splits\n",
    "for k in counts['val_counts'].keys():\n",
    "    counts['val_counts'][k] = int(counts['val_counts'][k] * \\\n",
    "        (train_size/val_size))\n",
    "for k in counts['test_counts'].keys():\n",
    "    counts['test_counts'][k] = int(counts['test_counts'][k] * \\\n",
    "        (train_size/test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        (2,)  (4,)  (15,)  (14,)  (30,)  (34,)  (1,)  (26,)  (32,)  (20,)  \\\n",
       "train  272.0  29.0  300.0   36.0   24.0   27.0  29.0   32.0  145.0  181.0   \n",
       "val    270.0  32.0  298.0   28.0   23.0   28.0  32.0   37.0  112.0  177.0   \n",
       "test   270.0  23.0  303.0   42.0   23.0   28.0  23.0   37.0  126.0  182.0   \n",
       "\n",
       "       ...  (28,)  (11,)  (22,)  (8,)  (29,)  (23,)  (31,)  (10,)  (18,)  \\\n",
       "train  ...  149.0   30.0   34.0  38.0   26.0   41.0   32.0   45.0   21.0   \n",
       "val    ...  149.0   46.0   32.0  32.0   32.0   42.0   32.0   46.0   23.0   \n",
       "test   ...  149.0   51.0   37.0  28.0   32.0   42.0   32.0   42.0   18.0   \n",
       "\n",
       "       (12,)  \n",
       "train   38.0  \n",
       "val     42.0  \n",
       "test    37.0  \n",
       "\n",
       "[3 rows x 35 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>(2,)</th>\n      <th>(4,)</th>\n      <th>(15,)</th>\n      <th>(14,)</th>\n      <th>(30,)</th>\n      <th>(34,)</th>\n      <th>(1,)</th>\n      <th>(26,)</th>\n      <th>(32,)</th>\n      <th>(20,)</th>\n      <th>...</th>\n      <th>(28,)</th>\n      <th>(11,)</th>\n      <th>(22,)</th>\n      <th>(8,)</th>\n      <th>(29,)</th>\n      <th>(23,)</th>\n      <th>(31,)</th>\n      <th>(10,)</th>\n      <th>(18,)</th>\n      <th>(12,)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train</th>\n      <td>272.0</td>\n      <td>29.0</td>\n      <td>300.0</td>\n      <td>36.0</td>\n      <td>24.0</td>\n      <td>27.0</td>\n      <td>29.0</td>\n      <td>32.0</td>\n      <td>145.0</td>\n      <td>181.0</td>\n      <td>...</td>\n      <td>149.0</td>\n      <td>30.0</td>\n      <td>34.0</td>\n      <td>38.0</td>\n      <td>26.0</td>\n      <td>41.0</td>\n      <td>32.0</td>\n      <td>45.0</td>\n      <td>21.0</td>\n      <td>38.0</td>\n    </tr>\n    <tr>\n      <th>val</th>\n      <td>270.0</td>\n      <td>32.0</td>\n      <td>298.0</td>\n      <td>28.0</td>\n      <td>23.0</td>\n      <td>28.0</td>\n      <td>32.0</td>\n      <td>37.0</td>\n      <td>112.0</td>\n      <td>177.0</td>\n      <td>...</td>\n      <td>149.0</td>\n      <td>46.0</td>\n      <td>32.0</td>\n      <td>32.0</td>\n      <td>32.0</td>\n      <td>42.0</td>\n      <td>32.0</td>\n      <td>46.0</td>\n      <td>23.0</td>\n      <td>42.0</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>270.0</td>\n      <td>23.0</td>\n      <td>303.0</td>\n      <td>42.0</td>\n      <td>23.0</td>\n      <td>28.0</td>\n      <td>23.0</td>\n      <td>37.0</td>\n      <td>126.0</td>\n      <td>182.0</td>\n      <td>...</td>\n      <td>149.0</td>\n      <td>51.0</td>\n      <td>37.0</td>\n      <td>28.0</td>\n      <td>32.0</td>\n      <td>42.0</td>\n      <td>32.0</td>\n      <td>42.0</td>\n      <td>18.0</td>\n      <td>37.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 35 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "# View distributions\n",
    "pd.DataFrame({\n",
    "    'train': counts['train_counts'],\n",
    "    'val': counts['val_counts'],\n",
    "    'test': counts['test_counts']\n",
    "}).T.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df = pd.DataFrame({\n",
    "    'train': counts['train_counts'],\n",
    "    'val': counts['val_counts'],\n",
    "    'test': counts['test_counts']\n",
    "}).T.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.142338654518357"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "# Standard deviation\n",
    "np.mean(np.std(dist_df.to_numpy(), axis=0))"
   ]
  },
  {
   "source": [
    "Iterative stratification essentially creates splits while \"trying to maintain balanced representation with respect to order-th label combinations\". We used to an order=1 for our iterative split which means we cared about providing representative distribution of each tag across the splits. But we can account for higher-order label relationships as well where we may care about the distribution of label combinations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Preporcessing "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\KATTUBOINA\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS=stopwords.words('english')\n",
    "porter=PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text,lower=True,stem=False,filters=\"[!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~]\",stopwords=STOPWORDS):\n",
    "    \"\"\"\n",
    "    Conditional preprocessing on our text unique to our task.\n",
    "    \"\"\"\n",
    "    if lower:\n",
    "        text=text.lower()\n",
    "    \n",
    "\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
    "    text=pattern.sub('', text)\n",
    "\n",
    "     #spacing and filters\n",
    "    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\n",
    "    text = re.sub(filters, r\"\", text)\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric chars\n",
    "    text = re.sub(' +', ' ', text)  # remove multiple spaces\n",
    "    text = text.strip()\n",
    "\n",
    "\n",
    "    text=re.sub(r'http\\S+', '', text)\n",
    "\n",
    "\n",
    "    if stem:\n",
    "        text=\" \".join([porter.stem(word) for word in text.split(' ')])\n",
    "    \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Checkbox(value=True, description='lower'), Checkbox(value=False, description='stem'), Ou…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f3f15b6603449739360b80cbfd71378"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(lower=True,stem=False)\n",
    "def display_preprocessed_text(lower,stem):\n",
    "    text=\"Conditional image generation using Variational Autoencoders and GANs.\"\n",
    "    preprocessed_text = preprocess(text=text, lower=lower, stem=stem)\n",
    "    print (preprocessed_text) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Albumentations Fast image augmentation library and easy to use wrapper around other libraries.\nalbumentations fast image augmentation library easy use wrapper around libraries\n"
     ]
    }
   ],
   "source": [
    "#Applying the changes to our dataset\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True, stem=False)\n",
    "print (f\"{df.text.values[0]}\\n{preprocessed_df.text.values[0]}\")"
   ]
  },
  {
   "source": [
    "# Model Building"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Some helpfull functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=1234):\n",
    "    \"\"\" Setting seed for reproducibility \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) #For multi-gpu training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(dftrain_size=0.7):\n",
    "    X=df.text.to_numpy()\n",
    "    y=df.tags\n",
    "\n",
    "\n",
    "    #Binarize y\n",
    "    label_encoder=LabelEncoder()\n",
    "    label_encoder.fit(y)\n",
    "    y=label_encoder.encode(y)\n",
    "\n",
    "    #Spliting the data \n",
    "    X_train, X_, y_train, y_ = iterative_train_test_split(X, y, train_size=train_size)\n",
    "    X_val, X_test, y_val, y_test = iterative_train_test_split(X_, y_, train_size=0.5)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test,label_encoder\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\n",
    "\n",
    "        # Set params\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train_step(self, dataloader):\n",
    "        \"\"\"Train step.\"\"\"\n",
    "        # Set model to train mode\n",
    "        self.model.train()\n",
    "        loss = 0.0\n",
    "\n",
    "        # Iterate over train batches\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Step\n",
    "            batch = [item.to(self.device) for item in batch]  # Set device\n",
    "            inputs, targets = batch[:-1], batch[-1]\n",
    "            self.optimizer.zero_grad()  # Reset gradients\n",
    "            z = self.model(inputs)  # Forward pass\n",
    "            J = self.loss_fn(z, targets)  # Define loss\n",
    "            J.backward()  # Backward pass\n",
    "            self.optimizer.step()  # Update weights\n",
    "\n",
    "            # Cumulative Metrics\n",
    "            loss += (J.detach().item() - loss) / (i + 1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def eval_step(self, dataloader):\n",
    "        \"\"\"Validation or test step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        loss = 0.0\n",
    "        y_trues, y_probs = [], []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Step\n",
    "                batch = [item.to(self.device) for item in batch]  # Set device\n",
    "                inputs, y_true = batch[:-1], batch[-1]\n",
    "                z = self.model(inputs)  # Forward pass\n",
    "                J = self.loss_fn(z, y_true).item()\n",
    "\n",
    "                # Cumulative Metrics\n",
    "                loss += (J - loss) / (i + 1)\n",
    "\n",
    "                # Store outputs\n",
    "                y_prob = torch.sigmoid(z).cpu().numpy()\n",
    "                y_probs.extend(y_prob)\n",
    "                y_trues.extend(y_true.cpu().numpy())\n",
    "\n",
    "        return loss, np.vstack(y_trues), np.vstack(y_probs)\n",
    "\n",
    "    def predict_step(self, dataloader):\n",
    "        \"\"\"Prediction step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        y_probs = []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Forward pass w/ inputs\n",
    "                inputs, targets = batch[:-1], batch[-1]\n",
    "                y_prob = self.model(inputs)\n",
    "\n",
    "                # Store outputs\n",
    "                y_probs.extend(y_prob)\n",
    "\n",
    "        return np.vstack(y_probs)\n",
    "\n",
    "    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n",
    "        best_val_loss = np.inf\n",
    "        for epoch in range(num_epochs):\n",
    "            # Steps\n",
    "            train_loss = self.train_step(dataloader=train_dataloader)\n",
    "            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = self.model\n",
    "                _patience = patience  # reset _patience\n",
    "            else:\n",
    "                _patience -= 1\n",
    "            if not _patience:  # 0\n",
    "                print(\"Stopping early!\")\n",
    "                break\n",
    "\n",
    "            # Logging\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1} | \"\n",
    "                f\"train_loss: {train_loss:.5f}, \"\n",
    "                f\"val_loss: {val_loss:.5f}, \"\n",
    "                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n",
    "                f\"_patience: {_patience}\"\n",
    "            )\n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(y_true,y_pred,classes):\n",
    "    \"\"\"Getting the per class performance\"\"\"\n",
    "    #Getting metrics\n",
    "    performance={'overall':{}, 'class':{}}\n",
    "    metrics=precision_recall_fscore_support(y_true,y_pred)\n",
    "\n",
    "\n",
    "    #overall performance \n",
    "    performance['overall']['precision'] = np.mean(metrics[0])\n",
    "    performance['overall']['recall'] = np.mean(metrics[1])\n",
    "    performance['overall']['f1'] = np.mean(metrics[2])\n",
    "    performance['overall']['num_samples'] = np.float64(np.sum(metrics[3]))\n",
    "\n",
    "\n",
    "\n",
    "    # Per-class performance\n",
    "    for i in range(len(classes)):\n",
    "        performance['class'][classes[i]] = {\n",
    "            \"precision\": metrics[0][i],\n",
    "            \"recall\": metrics[1][i],\n",
    "            \"f1\": metrics[2][i],\n",
    "            \"num_samples\": np.float64(metrics[3][i])\n",
    "        }\n",
    "\n",
    "    \n",
    "\n",
    "    return performance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Random \n",
    "What will the random performance look like\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train: (1000,), y_train: (1000, 35)\nX_val: (227,), y_val: (227, 35)\nX_test: (217,), y_test: (217, 35)\n"
     ]
    }
   ],
   "source": [
    "#Getting the data splits\n",
    "preprocessed_df=df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True, stem=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)\n",
    "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<LabelEncoder(num_classes=35)>\n['attention', 'autoencoders', 'computer-vision', 'convolutional-neural-networks', 'data-augmentation', 'embeddings', 'flask', 'generative-adversarial-networks', 'graph-neural-networks', 'graphs', 'huggingface', 'image-classification', 'interpretability', 'keras', 'language-modeling', 'natural-language-processing', 'node-classification', 'object-detection', 'pretraining', 'production', 'pytorch', 'question-answering', 'regression', 'reinforcement-learning', 'representation-learning', 'scikit-learn', 'segmentation', 'self-supervised-learning', 'tensorflow', 'tensorflow-js', 'time-series', 'transfer-learning', 'transformers', 'unsupervised-learning', 'wandb']\n"
     ]
    }
   ],
   "source": [
    "# Label encoder\n",
    "print (label_encoder)\n",
    "print (label_encoder.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(217, 35)\n[[0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1]\n [0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1]\n [1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1]\n [0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0]\n [0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate random predictions\n",
    "y_pred = np.random.randint(low=0, high=2, size=(len(y_test), len(label_encoder.classes)))\n",
    "print (y_pred.shape)\n",
    "print (y_pred[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"precision\": 0.0662941602216066,\n  \"recall\": 0.5065299488415251,\n  \"f1\": 0.10819194263879019,\n  \"num_samples\": 480.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "performance = get_performance(\n",
    "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "print (json.dumps(performance['overall'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.06291428571428571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Percentage of 1s (tag presence)\n",
    "tag_p = np.sum(np.sum(y_train)) / (len(y_train) * len(label_encoder.classes))\n",
    "print (tag_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate weighted random predictions\n",
    "y_pred = np.random.choice(\n",
    "    np.arange(0, 2), size=(len(y_test), len(label_encoder.classes)),\n",
    "    p=[1-tag_p, tag_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.06240947992100066"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "# Validate percentage\n",
    "np.sum(np.sum(y_pred)) / (len(y_pred) * len(label_encoder.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"precision\": 0.060484184552507536,\n  \"recall\": 0.053727634571230636,\n  \"f1\": 0.048704498064854516,\n  \"num_samples\": 480.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "performance = get_performance(\n",
    "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "print (json.dumps(performance['overall'], indent=2))\n"
   ]
  },
  {
   "source": [
    "## Rule Based \n",
    "We need to signals our inputs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data splits\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "400\n35\n"
     ]
    }
   ],
   "source": [
    "# Restrict to relevant tags\n",
    "print (len(tags_dict))\n",
    "tags_dict = {tag: tags_dict[tag] for tag in label_encoder.classes}\n",
    "print (len(tags_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention': 'attention',\n",
       " 'autoencoders': 'autoencoders',\n",
       " 'ae': 'autoencoders',\n",
       " 'computer vision': 'computer-vision',\n",
       " 'cv': 'computer-vision',\n",
       " 'vision': 'computer-vision',\n",
       " 'convolutional neural networks': 'convolutional-neural-networks',\n",
       " 'cnn': 'convolutional-neural-networks',\n",
       " 'data augmentation': 'data-augmentation',\n",
       " 'embeddings': 'embeddings',\n",
       " 'flask': 'flask',\n",
       " 'generative adversarial networks': 'generative-adversarial-networks',\n",
       " 'gan': 'generative-adversarial-networks',\n",
       " 'graph neural networks': 'graph-neural-networks',\n",
       " 'gnn': 'graph-neural-networks',\n",
       " 'graphs': 'graphs',\n",
       " 'huggingface': 'huggingface',\n",
       " 'image classification': 'image-classification',\n",
       " 'interpretability': 'interpretability',\n",
       " 'keras': 'keras',\n",
       " 'language modeling': 'language-modeling',\n",
       " 'lm': 'language-modeling',\n",
       " 'natural language processing': 'natural-language-processing',\n",
       " 'nlp': 'natural-language-processing',\n",
       " 'nlproc': 'natural-language-processing',\n",
       " 'node classification': 'node-classification',\n",
       " 'object detection': 'object-detection',\n",
       " 'pretraining': 'pretraining',\n",
       " 'pre training': 'pretraining',\n",
       " 'production': 'production',\n",
       " 'pytorch': 'pytorch',\n",
       " 'question answering': 'question-answering',\n",
       " 'qa': 'question-answering',\n",
       " 'regression': 'regression',\n",
       " 'reinforcement learning': 'reinforcement-learning',\n",
       " 'rl': 'reinforcement-learning',\n",
       " 'representation learning': 'representation-learning',\n",
       " 'scikit learn': 'scikit-learn',\n",
       " 'sklearn': 'scikit-learn',\n",
       " 'segmentation': 'segmentation',\n",
       " 'image segmentation': 'segmentation',\n",
       " 'self supervised learning': 'self-supervised-learning',\n",
       " 'tensorflow': 'tensorflow',\n",
       " 'tf': 'tensorflow',\n",
       " 'tensorflow js': 'tensorflow-js',\n",
       " 'tf js': 'tensorflow-js',\n",
       " 'time series': 'time-series',\n",
       " 'time series analysis': 'time-series',\n",
       " 'transfer learning': 'transfer-learning',\n",
       " 'transformers': 'transformers',\n",
       " 'unsupervised learning': 'unsupervised-learning',\n",
       " 'wandb': 'wandb',\n",
       " 'weights biases': 'wandb'}"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "#Map aliases\n",
    "aliases={}\n",
    "for tag,values in tags_dict.items():\n",
    "    aliases[preprocess(tag)]=tag\n",
    "    for alias in values['aliases']:\n",
    "        aliases[preprocess(alias)]=tag\n",
    "\n",
    "aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(text,aliases,tags_dict):\n",
    "    \"\"\"\n",
    "    if a token matches an alias ,then add the corresponding tag class\n",
    "    \"\"\"\n",
    "    classes=[]\n",
    "    for alias,tag in aliases.items():\n",
    "        if alias in text:\n",
    "            classes.append(tag)\n",
    "            for parent in tags_dict[tag][\"parents\"]:\n",
    "                classes.append(parent)\n",
    "        \n",
    "    \n",
    "    return list(set(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['computer-vision',\n",
       " 'generative-adversarial-networks',\n",
       " 'data-augmentation',\n",
       " 'object-detection']"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "#Sample for above function\n",
    "# Sample\n",
    "text = \"This project extends gans for data augmentation specifically for object detection tasks.\"\n",
    "get_classes(text=preprocess(text), aliases=aliases, tags_dict=tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction \n",
    "y_pred=[]\n",
    "for text in X_test:\n",
    "    classes=get_classes(text,aliases,tags_dict)\n",
    "    y_pred.append(classes)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Labels\n",
    "y_pred=label_encoder.encode(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n    \"precision\": 0.3958730158730158,\n    \"recall\": 0.10910285028326902,\n    \"f1\": 0.15424993307346246,\n    \"num_samples\": 480.0\n}\n"
     ]
    }
   ],
   "source": [
    "performance = get_performance(y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "print (json.dumps(performance['overall'], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"precision\": 1.0,\n  \"recall\": 0.12,\n  \"f1\": 0.21428571428571425,\n  \"num_samples\": 25.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Inspection\n",
    "tag = \"transformers\"\n",
    "print (json.dumps(performance[\"class\"][tag], indent=2))"
   ]
  },
  {
   "source": [
    "### Stemming "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "generative-adversarial-networks\ngenerative-adversarial-networks\n"
     ]
    }
   ],
   "source": [
    "print (aliases[preprocess('gan')])\n",
    "# print (aliases[preprocess('gans')]) # this won't find any match\n",
    "print (aliases[preprocess('generative adversarial networks')])\n",
    "# print (aliases[preprocess('generative adversarial network')]) # this won't find any match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "democraci\ndemocraci\n"
     ]
    }
   ],
   "source": [
    "print (porter.stem(\"democracy\"))\n",
    "print (porter.stem(\"democracies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data splits\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True, stem=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attent': 'attention',\n",
       " 'autoencod': 'autoencoders',\n",
       " 'ae': 'autoencoders',\n",
       " 'comput vision': 'computer-vision',\n",
       " 'cv': 'computer-vision',\n",
       " 'vision': 'computer-vision',\n",
       " 'convolut neural network': 'convolutional-neural-networks',\n",
       " 'cnn': 'convolutional-neural-networks',\n",
       " 'data augment': 'data-augmentation',\n",
       " 'embed': 'embeddings',\n",
       " 'flask': 'flask',\n",
       " 'gener adversari network': 'generative-adversarial-networks',\n",
       " 'gan': 'generative-adversarial-networks',\n",
       " 'graph neural network': 'graph-neural-networks',\n",
       " 'gnn': 'graph-neural-networks',\n",
       " 'graph': 'graphs',\n",
       " 'huggingfac': 'huggingface',\n",
       " 'imag classif': 'image-classification',\n",
       " 'interpret': 'interpretability',\n",
       " 'kera': 'keras',\n",
       " 'languag model': 'language-modeling',\n",
       " 'lm': 'language-modeling',\n",
       " 'natur languag process': 'natural-language-processing',\n",
       " 'nlp': 'natural-language-processing',\n",
       " 'nlproc': 'natural-language-processing',\n",
       " 'node classif': 'node-classification',\n",
       " 'object detect': 'object-detection',\n",
       " 'pretrain': 'pretraining',\n",
       " 'pre train': 'pretraining',\n",
       " 'product': 'production',\n",
       " 'pytorch': 'pytorch',\n",
       " 'question answer': 'question-answering',\n",
       " 'qa': 'question-answering',\n",
       " 'regress': 'regression',\n",
       " 'reinforc learn': 'reinforcement-learning',\n",
       " 'rl': 'reinforcement-learning',\n",
       " 'represent learn': 'representation-learning',\n",
       " 'scikit learn': 'scikit-learn',\n",
       " 'sklearn': 'scikit-learn',\n",
       " 'segment': 'segmentation',\n",
       " 'imag segment': 'segmentation',\n",
       " 'self supervis learn': 'self-supervised-learning',\n",
       " 'tensorflow': 'tensorflow',\n",
       " 'tf': 'tensorflow',\n",
       " 'tensorflow js': 'tensorflow-js',\n",
       " 'tf js': 'tensorflow-js',\n",
       " 'time seri': 'time-series',\n",
       " 'time seri analysi': 'time-series',\n",
       " 'transfer learn': 'transfer-learning',\n",
       " 'transform': 'transformers',\n",
       " 'unsupervis learn': 'unsupervised-learning',\n",
       " 'wandb': 'wandb',\n",
       " 'weight bias': 'wandb'}"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "# Map aliases\n",
    "aliases = {}\n",
    "for tag, values in tags_dict.items():\n",
    "    aliases[preprocess(tag, stem=True)] = tag\n",
    "    for alias in values['aliases']:\n",
    "        aliases[preprocess(alias, stem=True)] = tag\n",
    "aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "generative-adversarial-networks\ngenerative-adversarial-networks\ngenerative-adversarial-networks\ngenerative-adversarial-networks\n"
     ]
    }
   ],
   "source": [
    "# Checks (we will write proper tests soon)\n",
    "print (aliases[preprocess('gan', stem=True)])\n",
    "print (aliases[preprocess('gans', stem=True)])\n",
    "print (aliases[preprocess('generative adversarial network', stem=True)])\n",
    "print (aliases[preprocess('generative adversarial networks', stem=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['computer-vision',\n",
       " 'generative-adversarial-networks',\n",
       " 'data-augmentation',\n",
       " 'object-detection']"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "# Sample\n",
    "text = \"This project extends gans for data augmentation specifically for object detection tasks.\"\n",
    "get_classes(text=preprocess(text, stem=True), aliases=aliases, tags_dict=tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred = []\n",
    "for text in X_test:\n",
    "    classes = get_classes(text, aliases, tags_dict)\n",
    "    y_pred.append(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "y_pred = label_encoder.encode(y_pred)"
   ]
  },
  {
   "source": [
    "#### Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n    \"precision\": 0.4197732426303855,\n    \"recall\": 0.09591888816059155,\n    \"f1\": 0.14942598936879561,\n    \"num_samples\": 473.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "performance = get_performance(y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "print (json.dumps(performance['overall'], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"precision\": 0.8888888888888888,\n  \"recall\": 0.2962962962962963,\n  \"f1\": 0.4444444444444444,\n  \"num_samples\": 27.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Inspection\n",
    "tag = \"transformers\"\n",
    "print (json.dumps(performance[\"class\"][tag], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP, FP, FN samples\n",
    "index = label_encoder.class_to_index[tag]\n",
    "tp, fp, fn = [], [], []\n",
    "for i in range(len(y_test)):\n",
    "    true = y_test[i][index]\n",
    "    pred = y_pred[i][index]\n",
    "    if true and pred:\n",
    "        tp.append(i)\n",
    "    elif not true and pred:\n",
    "        fp.append(i)\n",
    "    elif true and not pred:\n",
    "        fn.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 46, 94, 165, 169, 190, 194, 199]\n[49]\n[4, 14, 15, 18, 28, 54, 61, 63, 72, 75, 89, 99, 137, 141, 142, 160, 163, 174, 206]\n"
     ]
    }
   ],
   "source": [
    "print (tp)\n",
    "print (fp)\n",
    "print (fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Insight Project Insight is designed to create NLP as a service with code base for both front end GUI (streamlit) and backend server (FastAPI) the usage of transformers \ntrue: ['attention', 'huggingface', 'natural-language-processing', 'pytorch', 'transfer-learning', 'transformers']\npred: ['natural-language-processing', 'transformers']\n\n"
     ]
    }
   ],
   "source": [
    "index = tp[0]\n",
    "print (X_test[index])\n",
    "print (f\"true: {label_encoder.decode([y_test[index]])[0]}\")\n",
    "print (f\"pred: {label_encoder.decode([y_pred[index]])[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted tags\n",
    "sorted_tags_by_f1 = OrderedDict(sorted(\n",
    "        performance['class'].items(), key=lambda tag: tag[1]['f1'], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='tag', index=3, options=('interpretability', 'segmentation', 'produ…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fe9247e915b43a3bd56bc7b7969fbcb"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(tag=list(sorted_tags_by_f1.keys()))\n",
    "def display_tag_analysis(tag='transformers'):\n",
    "    # Performance\n",
    "    print (json.dumps(performance[\"class\"][tag], indent=2))\n",
    "\n",
    "    # TP, FP, FN samples\n",
    "    index = label_encoder.class_to_index[tag]\n",
    "    tp, fp, fn = [], [], []\n",
    "    for i in range(len(y_test)):\n",
    "        true = y_test[i][index]\n",
    "        pred = y_pred[i][index]\n",
    "        if true and pred:\n",
    "            tp.append(i)\n",
    "        elif not true and pred:\n",
    "            fp.append(i)\n",
    "        elif true and not pred:\n",
    "            fn.append(i)\n",
    "\n",
    "    # Samples\n",
    "    num_samples = 3\n",
    "    if len(tp):\n",
    "        print (\"\\n=== True positives ===\\n\")\n",
    "        for i in tp[:num_samples]:\n",
    "            print (f\"  {X_test[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")\n",
    "    if len(fp):\n",
    "        print (\"=== False positives ===\\n\")\n",
    "        for i in fp[:num_samples]:\n",
    "            print (f\"  {X_test[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")\n",
    "    if len(fn):\n",
    "        print (\"=== False negatives ===\\n\")\n",
    "        for i in fn[:num_samples]:\n",
    "            print (f\"  {X_test[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")"
   ]
  },
  {
   "source": [
    "#### Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "transfer learn transform self supervis learn\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['transfer-learning',\n",
       " 'transformers',\n",
       " 'self-supervised-learning',\n",
       " 'natural-language-processing']"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "# Infer\n",
    "text = \"Transfer learning with transformers for self-supervised learning\"\n",
    "print (preprocess(text, stem=True))\n",
    "get_classes(text=preprocess(text, stem=True), aliases=aliases, tags_dict=tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "transfer learn bert self supervis learn\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['transfer-learning', 'self-supervised-learning']"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "# Infer\n",
    "text = \"Transfer learning with BERT for self-supervised learning\"\n",
    "print (preprocess(text, stem=True))\n",
    "get_classes(text=preprocess(text, stem=True), aliases=aliases, tags_dict=tags_dict)"
   ]
  },
  {
   "source": [
    "Limitations : Failed to learned or generalize any imlpicit pattern to poredit the labels becaouse we treat the tokens as isolated entiries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Simple ML"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting seeds\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting data splits\n",
    "preprocessed_df=df.copy()\n",
    "preprocessed_df.text=preprocessed_df.text.apply(preprocess,lower=True,stem=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Albumentations Fast image augmentation library and easy to use wrapper around other libraries.\n(1000, 3605)\n  (0, 1861)\t0.35101282857580157\n  (0, 2276)\t0.3322638000685066\n  (0, 260)\t0.312389028946533\n  (0, 3573)\t0.3634469148077632\n  (0, 3414)\t0.22983052113836\n  (0, 3256)\t0.10614373238476807\n  (0, 1012)\t0.2636058411307363\n  (0, 192)\t0.10345779731276887\n  (0, 1862)\t0.2072370467198702\n  (0, 304)\t0.2659773634649721\n  (0, 1596)\t0.18753138760686572\n  (0, 1222)\t0.2861993155492261\n  (0, 153)\t0.4020707144370318\n"
     ]
    }
   ],
   "source": [
    "#Using Tf-idf \n",
    "vectorizer=TfidfVectorizer()\n",
    "print(X_train[0])\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "print (X_train.shape)\n",
    "print (X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(model):\n",
    "    \"\"\"Fit and evaluate each model.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    performance = get_performance(\n",
    "        y_true=y_test, y_pred=y_pred, classes=list(label_encoder.classes))\n",
    "    return performance['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"logistic-regression\": {\n    \"precision\": 0.3312317312317312,\n    \"recall\": 0.06427712804436943,\n    \"f1\": 0.10210451648045632,\n    \"num_samples\": 480.0\n  },\n  \"k-nearest-neighbors\": {\n    \"precision\": 0.6531082519964507,\n    \"recall\": 0.3227342860341629,\n    \"f1\": 0.41299426353846336,\n    \"num_samples\": 480.0\n  },\n  \"random-forest\": {\n    \"precision\": 0.5304325238867477,\n    \"recall\": 0.20684071484071484,\n    \"f1\": 0.2838581472925646,\n    \"num_samples\": 480.0\n  },\n  \"gradient-boosting-machine\": {\n    \"precision\": 0.7445866383444024,\n    \"recall\": 0.4956736941903197,\n    \"f1\": 0.5807018407182406,\n    \"num_samples\": 480.0\n  },\n  \"support-vector-machine\": {\n    \"precision\": 0.8026174159136747,\n    \"recall\": 0.3700476348364279,\n    \"f1\": 0.48829320873570664,\n    \"num_samples\": 480.0\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "performance = {}\n",
    "performance['logistic-regression'] = fit_and_evaluate(OneVsRestClassifier(\n",
    "    LogisticRegression(), n_jobs=1))\n",
    "performance['k-nearest-neighbors'] = fit_and_evaluate(\n",
    "    KNeighborsClassifier())\n",
    "performance['random-forest'] = fit_and_evaluate(\n",
    "    RandomForestClassifier(n_jobs=-1))\n",
    "performance['gradient-boosting-machine'] = fit_and_evaluate(OneVsRestClassifier(\n",
    "    GradientBoostingClassifier()))\n",
    "performance['support-vector-machine'] = fit_and_evaluate(OneVsRestClassifier(\n",
    "    LinearSVC(), n_jobs=-1))\n",
    "print (json.dumps(performance, indent=2))"
   ]
  },
  {
   "source": [
    "Limitations:\n",
    "TF-IDF representations don't encapsulate much signal beyond frequency but we require more fine-grained token representations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## CNN w/ Embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the seeds\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data splits\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)\n",
    "X_test_raw = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "cuda = True\n",
    "device = torch.device('cuda' if (\n",
    "    torch.cuda.is_available() and cuda) else 'cpu')\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "if device.type == 'cuda':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, char_level, num_tokens=None, \n",
    "                 pad_token='<PAD>', oov_token='<UNK>',\n",
    "                 token_to_index=None):\n",
    "        self.char_level = char_level\n",
    "        self.separator = '' if self.char_level else ' '\n",
    "        if num_tokens: num_tokens -= 2 # pad + unk tokens\n",
    "        self.num_tokens = num_tokens\n",
    "        self.oov_token = oov_token\n",
    "        if not token_to_index:\n",
    "            token_to_index = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Tokenizer(num_tokens={len(self)})>\"\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        if self.char_level:\n",
    "            all_tokens = [token for text in texts for token in text]\n",
    "        if not self.char_level:\n",
    "            all_tokens = [token for text in texts for token in text.split(' ')]\n",
    "        counts = Counter(all_tokens).most_common(self.num_tokens)\n",
    "        self.min_token_freq = counts[-1][1]\n",
    "        for token, count in counts:\n",
    "            index = len(self)\n",
    "            self.token_to_index[token] = index\n",
    "            self.index_to_token[index] = token\n",
    "        return self\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            if not self.char_level:\n",
    "                text = text.split(' ')\n",
    "            sequence = []\n",
    "            for token in text:\n",
    "                sequence.append(self.token_to_index.get(\n",
    "                    token, self.token_to_index[self.oov_token]))\n",
    "            sequences.append(np.asarray(sequence))\n",
    "        return sequences\n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = []\n",
    "            for index in sequence:\n",
    "                text.append(self.index_to_token.get(index, self.oov_token))\n",
    "            texts.append(self.separator.join([token for token in text]))\n",
    "        return texts\n",
    "\n",
    "    def save(self, fp):\n",
    "        with open(fp, 'w') as fp:\n",
    "            contents = {\n",
    "                'char_level': self.char_level,\n",
    "                'oov_token': self.oov_token,\n",
    "                'token_to_index': self.token_to_index\n",
    "            }\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        with open(fp, 'r') as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<Tokenizer(num_tokens=136)>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "char_level = True\n",
    "tokenizer = Tokenizer(char_level=char_level)\n",
    "tokenizer.fit_on_texts(texts=X_train)\n",
    "vocab_size = len(tokenizer)\n",
    "print (tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " ' ': 2,\n",
       " 'e': 3,\n",
       " 'n': 4,\n",
       " 'i': 5,\n",
       " 't': 6,\n",
       " 'a': 7,\n",
       " 'o': 8,\n",
       " 'r': 9,\n",
       " 's': 10,\n",
       " 'l': 11,\n",
       " 'c': 12,\n",
       " 'd': 13,\n",
       " 'g': 14,\n",
       " 'h': 15,\n",
       " 'm': 16,\n",
       " 'u': 17,\n",
       " 'p': 18,\n",
       " 'f': 19,\n",
       " 'y': 20,\n",
       " 'w': 21,\n",
       " 'b': 22,\n",
       " 'T': 23,\n",
       " 'v': 24,\n",
       " '.': 25,\n",
       " 'A': 26,\n",
       " '-': 27,\n",
       " 'L': 28,\n",
       " 'k': 29,\n",
       " 'P': 30,\n",
       " 'S': 31,\n",
       " 'N': 32,\n",
       " 'C': 33,\n",
       " 'M': 34,\n",
       " 'D': 35,\n",
       " 'I': 36,\n",
       " ',': 37,\n",
       " 'x': 38,\n",
       " 'R': 39,\n",
       " 'F': 40,\n",
       " 'G': 41,\n",
       " 'E': 42,\n",
       " 'B': 43,\n",
       " ':': 44,\n",
       " '2': 45,\n",
       " 'O': 46,\n",
       " 'z': 47,\n",
       " 'H': 48,\n",
       " 'W': 49,\n",
       " '0': 50,\n",
       " 'V': 51,\n",
       " '(': 52,\n",
       " ')': 53,\n",
       " 'j': 54,\n",
       " 'U': 55,\n",
       " 'K': 56,\n",
       " 'q': 57,\n",
       " '1': 58,\n",
       " '\"': 59,\n",
       " 'Q': 60,\n",
       " '\\r': 61,\n",
       " '\\n': 62,\n",
       " '/': 63,\n",
       " '&': 64,\n",
       " 'Y': 65,\n",
       " '3': 66,\n",
       " '+': 67,\n",
       " \"'\": 68,\n",
       " '?': 69,\n",
       " 'J': 70,\n",
       " '5': 71,\n",
       " '9': 72,\n",
       " 'X': 73,\n",
       " '8': 74,\n",
       " '6': 75,\n",
       " '4': 76,\n",
       " '’': 77,\n",
       " '!': 78,\n",
       " 'Z': 79,\n",
       " '—': 80,\n",
       " '7': 81,\n",
       " '“': 82,\n",
       " '”': 83,\n",
       " '|': 84,\n",
       " '🤗': 85,\n",
       " '️': 86,\n",
       " '%': 87,\n",
       " '=': 88,\n",
       " '–': 89,\n",
       " '💻': 90,\n",
       " '❤': 91,\n",
       " '_': 92,\n",
       " '🎨': 93,\n",
       " '>': 94,\n",
       " '🌊': 95,\n",
       " '^': 96,\n",
       " '🚗': 97,\n",
       " '🕶': 98,\n",
       " '<': 99,\n",
       " '\\u2060': 100,\n",
       " '⚡': 101,\n",
       " ';': 102,\n",
       " '@': 103,\n",
       " '#': 104,\n",
       " '🏥': 105,\n",
       " '💤': 106,\n",
       " '✨': 107,\n",
       " '😎': 108,\n",
       " '🔍': 109,\n",
       " '[': 110,\n",
       " ']': 111,\n",
       " '🦠': 112,\n",
       " '🍼': 113,\n",
       " '\\u2009': 114,\n",
       " '基': 115,\n",
       " '于': 116,\n",
       " '📈': 117,\n",
       " '⭐': 118,\n",
       " '🔥': 119,\n",
       " '*': 120,\n",
       " '…': 121,\n",
       " '🎐': 122,\n",
       " 'ç': 123,\n",
       " '🦄': 124,\n",
       " '✏': 125,\n",
       " '✂': 126,\n",
       " '🏡': 127,\n",
       " '𝘢': 128,\n",
       " '𝘯': 129,\n",
       " '𝘺': 130,\n",
       " '🐳': 131,\n",
       " '🤖': 132,\n",
       " '💬': 133,\n",
       " '🎥': 134,\n",
       " '👀': 135}"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "tokenizer.token_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text to indices:\n  (preprocessed) → Albumentations Fast image augmentation library and easy to use wrapper around other libraries.\n  (tokenized) → [26 11 22 17 16  3  4  6  7  6  5  8  4 10  2 40  7 10  6  2  5 16  7 14\n  3  2  7 17 14 16  3  4  6  7  6  5  8  4  2 11  5 22  9  7  9 20  2  7\n  4 13  2  3  7 10 20  2  6  8  2 17 10  3  2 21  9  7 18 18  3  9  2  7\n  9  8 17  4 13  2  8  6 15  3  9  2 11  5 22  9  7  9  5  3 10 25]\n"
     ]
    }
   ],
   "source": [
    "# Convert texts to sequences of indices\n",
    "# Convert texts to sequences of indices\n",
    "X_train = np.array(tokenizer.texts_to_sequences(X_train))\n",
    "X_val = np.array(tokenizer.texts_to_sequences(X_val))\n",
    "X_test = np.array(tokenizer.texts_to_sequences(X_test))\n",
    "preprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\n",
    "print (\"Text to indices:\\n\"\n",
    "    f\"  (preprocessed) → {preprocessed_text}\\n\"\n",
    "    f\"  (tokenized) → {X_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "class counts: [120  41 388 106  41  75  34  73  51  78  64  51  55  93  51 429  33  69\n  30  51 258  32  49  59  57  60  48  40 213  40  34  46 196  39  39],\nclass weights: {0: 0.008333333333333333, 1: 0.024390243902439025, 2: 0.002577319587628866, 3: 0.009433962264150943, 4: 0.024390243902439025, 5: 0.013333333333333334, 6: 0.029411764705882353, 7: 0.0136986301369863, 8: 0.0196078431372549, 9: 0.01282051282051282, 10: 0.015625, 11: 0.0196078431372549, 12: 0.01818181818181818, 13: 0.010752688172043012, 14: 0.0196078431372549, 15: 0.002331002331002331, 16: 0.030303030303030304, 17: 0.014492753623188406, 18: 0.03333333333333333, 19: 0.0196078431372549, 20: 0.003875968992248062, 21: 0.03125, 22: 0.02040816326530612, 23: 0.01694915254237288, 24: 0.017543859649122806, 25: 0.016666666666666666, 26: 0.020833333333333332, 27: 0.025, 28: 0.004694835680751174, 29: 0.025, 30: 0.029411764705882353, 31: 0.021739130434782608, 32: 0.00510204081632653, 33: 0.02564102564102564, 34: 0.02564102564102564}\n"
     ]
    }
   ],
   "source": [
    "# Class weights\n",
    "counts = np.bincount([label_encoder.class_to_index[class_] for class_ in all_tags])\n",
    "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
    "print (f\"class counts: {counts},\\nclass weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_seq_len=0):\n",
    "    \"\"\"Pad sequences to max length in sequence.\"\"\"\n",
    "    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n",
    "    padded_sequences = np.zeros((len(sequences), max_seq_len))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        padded_sequences[i][:len(sequence)] = sequence\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, max_filter_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.max_filter_size = max_filter_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Dataset(N={len(self)})>\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        return [X, y]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Processing on a batch.\"\"\"\n",
    "        # Get inputs\n",
    "        batch = np.array(batch, dtype=object)\n",
    "        X = batch[:, 0]\n",
    "        y = np.stack(batch[:, 1], axis=0)\n",
    "\n",
    "        # Pad inputs\n",
    "        X = pad_sequences(sequences=X, max_seq_len=self.max_filter_size)\n",
    "\n",
    "        # Cast\n",
    "        X = torch.LongTensor(X.astype(np.int32))\n",
    "        y = torch.FloatTensor(y.astype(np.int32))\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data splits:\n  Train dataset:<Dataset(N=1000)>\n  Val dataset: <Dataset(N=227)>\n  Test dataset: <Dataset(N=217)>\nSample point:\n  X: [26 11 22 17 16  3  4  6  7  6  5  8  4 10  2 40  7 10  6  2  5 16  7 14\n  3  2  7 17 14 16  3  4  6  7  6  5  8  4  2 11  5 22  9  7  9 20  2  7\n  4 13  2  3  7 10 20  2  6  8  2 17 10  3  2 21  9  7 18 18  3  9  2  7\n  9  8 17  4 13  2  8  6 15  3  9  2 11  5 22  9  7  9  5  3 10 25]\n  y: [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "filter_sizes = list(range(1, 11))\n",
    "train_dataset = CNNTextDataset(\n",
    "    X=X_train, y=y_train, max_filter_size=max(filter_sizes))\n",
    "val_dataset = CNNTextDataset(\n",
    "    X=X_val, y=y_val, max_filter_size=max(filter_sizes))\n",
    "test_dataset = CNNTextDataset(\n",
    "    X=X_test, y=y_test, max_filter_size=max(filter_sizes))\n",
    "print (\"Data splits:\\n\"\n",
    "    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n",
    "    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n",
    "    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  X: {train_dataset[0][0]}\\n\"\n",
    "    f\"  y: {train_dataset[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample batch:\n  X: [128, 226]\n  y: [128, 35]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create dataloaders\n",
    "batch_size = 128\n",
    "train_dataloader = train_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "val_dataloader = val_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "test_dataloader = test_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print (\"Sample batch:\\n\"\n",
    "    f\"  X: {list(batch_X.size())}\\n\"\n",
    "    f\"  y: {list(batch_y.size())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "embedding_dim = 128\n",
    "num_filters = 128\n",
    "hidden_dim = 128\n",
    "dropout_p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, num_filters, filter_sizes,\n",
    "                 hidden_dim, dropout_p, num_classes, padding_idx=0):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Initialize embeddings\n",
    "        self.embeddings = nn.Embedding(\n",
    "                embedding_dim=embedding_dim, num_embeddings=vocab_size,\n",
    "                padding_idx=padding_idx)\n",
    "\n",
    "        # Conv weights\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.conv = nn.ModuleList(\n",
    "            [nn.Conv1d(in_channels=embedding_dim,\n",
    "                       out_channels=num_filters,\n",
    "                       kernel_size=f) for f in filter_sizes])\n",
    "\n",
    "        # FC weights\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(num_filters*len(filter_sizes), hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs, channel_first=False):\n",
    "\n",
    "        # Embed\n",
    "        x_in, = inputs\n",
    "        x_in = self.embeddings(x_in)\n",
    "        if not channel_first:\n",
    "            x_in = x_in.transpose(1, 2)  # (N, channels, sequence length)\n",
    "\n",
    "        z = []\n",
    "        max_seq_len = x_in.shape[2]\n",
    "        for i, f in enumerate(self.filter_sizes):\n",
    "\n",
    "            # `SAME` padding\n",
    "            padding_left = int(\n",
    "                (self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2)\n",
    "            padding_right = int(math.ceil(\n",
    "                (self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2))\n",
    "\n",
    "            # Conv\n",
    "            _z = self.conv[i](F.pad(x_in, (padding_left, padding_right)))\n",
    "\n",
    "            # Pool\n",
    "            _z = F.max_pool1d(_z, _z.size(2)).squeeze(2)\n",
    "            z.append(_z)\n",
    "\n",
    "        # Concat outputs\n",
    "        z = torch.cat(z, 1)\n",
    "\n",
    "        # FC\n",
    "        z = self.fc1(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc2(z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method Module.named_parameters of CNN(\n  (embeddings): Embedding(136, 128, padding_idx=0)\n  (conv): ModuleList(\n    (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n    (1): Conv1d(128, 128, kernel_size=(2,), stride=(1,))\n    (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n    (3): Conv1d(128, 128, kernel_size=(4,), stride=(1,))\n    (4): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n    (5): Conv1d(128, 128, kernel_size=(6,), stride=(1,))\n    (6): Conv1d(128, 128, kernel_size=(7,), stride=(1,))\n    (7): Conv1d(128, 128, kernel_size=(8,), stride=(1,))\n    (8): Conv1d(128, 128, kernel_size=(9,), stride=(1,))\n    (9): Conv1d(128, 128, kernel_size=(10,), stride=(1,))\n  )\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=1280, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=35, bias=True)\n)>\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = CNN(\n",
    "    embedding_dim=embedding_dim, vocab_size=vocab_size,\n",
    "    num_filters=num_filters, filter_sizes=filter_sizes,\n",
    "    hidden_dim=hidden_dim, dropout_p=dropout_p,\n",
    "    num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "print (model.named_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "lr = 2e-4\n",
    "num_epochs = 200\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "class_weights_tensor = torch.Tensor(np.array(list(class_weights.values())))\n",
    "loss_fn = nn.BCEWithLogitsLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer & scheduler\n",
    "# Define optimizer & scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer module\n",
    "trainer = Trainer(\n",
    "    model=model, device=device, loss_fn=loss_fn, \n",
    "    optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 | train_loss: 0.00813, val_loss: 0.00326, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.00440, val_loss: 0.00280, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 3 | train_loss: 0.00414, val_loss: 0.00296, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 4 | train_loss: 0.00397, val_loss: 0.00283, lr: 2.00E-04, _patience: 8\n",
      "Epoch: 5 | train_loss: 0.00370, val_loss: 0.00266, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 6 | train_loss: 0.00347, val_loss: 0.00260, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 7 | train_loss: 0.00336, val_loss: 0.00258, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 8 | train_loss: 0.00324, val_loss: 0.00256, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 9 | train_loss: 0.00317, val_loss: 0.00254, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 10 | train_loss: 0.00300, val_loss: 0.00251, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 11 | train_loss: 0.00301, val_loss: 0.00248, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 12 | train_loss: 0.00292, val_loss: 0.00245, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 13 | train_loss: 0.00286, val_loss: 0.00242, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 14 | train_loss: 0.00280, val_loss: 0.00239, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 15 | train_loss: 0.00267, val_loss: 0.00235, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 16 | train_loss: 0.00266, val_loss: 0.00231, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 17 | train_loss: 0.00259, val_loss: 0.00227, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 18 | train_loss: 0.00250, val_loss: 0.00223, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 19 | train_loss: 0.00240, val_loss: 0.00219, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 20 | train_loss: 0.00241, val_loss: 0.00215, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 21 | train_loss: 0.00228, val_loss: 0.00212, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 22 | train_loss: 0.00222, val_loss: 0.00207, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 23 | train_loss: 0.00215, val_loss: 0.00204, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 24 | train_loss: 0.00210, val_loss: 0.00200, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 25 | train_loss: 0.00203, val_loss: 0.00196, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 26 | train_loss: 0.00198, val_loss: 0.00193, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 27 | train_loss: 0.00188, val_loss: 0.00190, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 28 | train_loss: 0.00186, val_loss: 0.00187, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 29 | train_loss: 0.00180, val_loss: 0.00184, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 30 | train_loss: 0.00179, val_loss: 0.00182, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 31 | train_loss: 0.00170, val_loss: 0.00179, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 32 | train_loss: 0.00167, val_loss: 0.00177, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 33 | train_loss: 0.00162, val_loss: 0.00175, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 34 | train_loss: 0.00157, val_loss: 0.00172, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 35 | train_loss: 0.00153, val_loss: 0.00172, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 36 | train_loss: 0.00148, val_loss: 0.00169, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 37 | train_loss: 0.00142, val_loss: 0.00167, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 38 | train_loss: 0.00139, val_loss: 0.00165, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 39 | train_loss: 0.00136, val_loss: 0.00164, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 40 | train_loss: 0.00134, val_loss: 0.00164, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 41 | train_loss: 0.00131, val_loss: 0.00162, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 42 | train_loss: 0.00125, val_loss: 0.00159, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 43 | train_loss: 0.00127, val_loss: 0.00160, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 44 | train_loss: 0.00122, val_loss: 0.00158, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 45 | train_loss: 0.00117, val_loss: 0.00158, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 46 | train_loss: 0.00113, val_loss: 0.00160, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 47 | train_loss: 0.00112, val_loss: 0.00156, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 48 | train_loss: 0.00109, val_loss: 0.00156, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 49 | train_loss: 0.00106, val_loss: 0.00156, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 50 | train_loss: 0.00106, val_loss: 0.00156, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 51 | train_loss: 0.00100, val_loss: 0.00156, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 52 | train_loss: 0.00097, val_loss: 0.00155, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 53 | train_loss: 0.00095, val_loss: 0.00154, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 54 | train_loss: 0.00092, val_loss: 0.00151, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 55 | train_loss: 0.00089, val_loss: 0.00154, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 56 | train_loss: 0.00087, val_loss: 0.00155, lr: 2.00E-04, _patience: 8\n",
      "Epoch: 57 | train_loss: 0.00087, val_loss: 0.00150, lr: 2.00E-04, _patience: 10\n",
      "Epoch: 58 | train_loss: 0.00083, val_loss: 0.00151, lr: 2.00E-04, _patience: 9\n",
      "Epoch: 59 | train_loss: 0.00083, val_loss: 0.00152, lr: 2.00E-04, _patience: 8\n",
      "Epoch: 60 | train_loss: 0.00081, val_loss: 0.00154, lr: 2.00E-04, _patience: 7\n",
      "Epoch: 61 | train_loss: 0.00077, val_loss: 0.00155, lr: 2.00E-04, _patience: 6\n",
      "Epoch: 62 | train_loss: 0.00076, val_loss: 0.00150, lr: 2.00E-04, _patience: 5\n",
      "Epoch: 63 | train_loss: 0.00076, val_loss: 0.00151, lr: 2.00E-05, _patience: 4\n",
      "Epoch: 64 | train_loss: 0.00073, val_loss: 0.00150, lr: 2.00E-05, _patience: 3\n",
      "Epoch: 65 | train_loss: 0.00070, val_loss: 0.00153, lr: 2.00E-05, _patience: 2\n",
      "Epoch: 66 | train_loss: 0.00072, val_loss: 0.00154, lr: 2.00E-05, _patience: 1\n",
      "Stopping early!\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "best_model = trainer.train(\n",
    "    num_epochs, patience, train_dataloader, val_dataloader)"
   ]
  },
  {
   "source": [
    "### Evaluation "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x26cff937688>"
      ]
     },
     "metadata": {},
     "execution_count": 122
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 385.78125 262.19625\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 385.78125 262.19625 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 378.58125 224.64 \r\nL 378.58125 7.2 \r\nL 43.78125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m8ae3f2933a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.299079\" xlink:href=\"#m8ae3f2933a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(44.347516 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"114.533822\" xlink:href=\"#m8ae3f2933a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0.2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(106.582259 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"176.768565\" xlink:href=\"#m8ae3f2933a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 0.4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(168.817002 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.003308\" xlink:href=\"#m8ae3f2933a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0.6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(231.051746 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"301.238051\" xlink:href=\"#m8ae3f2933a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0.8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(293.286489 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.472795\" xlink:href=\"#m8ae3f2933a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1.0 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(355.521232 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Threshold -->\r\n     <defs>\r\n      <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n     </defs>\r\n     <g transform=\"translate(186.432031 252.916563)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-84\"/>\r\n      <use x=\"61.083984\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"124.462891\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"165.544922\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"227.068359\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"279.167969\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"342.546875\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"403.728516\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"431.511719\" xlink:href=\"#DejaVuSans-100\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m4f30884c94\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4f30884c94\" y=\"214.846174\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(20.878125 218.645393)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4f30884c94\" y=\"175.293667\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(20.878125 179.092885)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4f30884c94\" y=\"135.741159\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(20.878125 139.540378)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4f30884c94\" y=\"96.188651\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(20.878125 99.98787)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4f30884c94\" y=\"56.636144\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(20.878125 60.435363)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4f30884c94\" y=\"17.083636\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 20.882855)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- Performance -->\r\n     <defs>\r\n      <path d=\"M 19.671875 64.796875 \r\nL 19.671875 37.40625 \r\nL 32.078125 37.40625 \r\nQ 38.96875 37.40625 42.71875 40.96875 \r\nQ 46.484375 44.53125 46.484375 51.125 \r\nQ 46.484375 57.671875 42.71875 61.234375 \r\nQ 38.96875 64.796875 32.078125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.34375 72.90625 50.609375 67.359375 \r\nQ 56.890625 61.8125 56.890625 51.125 \r\nQ 56.890625 40.328125 50.609375 34.8125 \r\nQ 44.34375 29.296875 32.078125 29.296875 \r\nL 19.671875 29.296875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-80\"/>\r\n      <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n      <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 147.867656)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"60.255859\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"121.779297\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"162.892578\" xlink:href=\"#DejaVuSans-102\"/>\r\n      <use x=\"198.097656\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"259.279297\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"300.376953\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"397.789062\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"459.068359\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"522.447266\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"577.427734\" xlink:href=\"#DejaVuSans-101\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p6e7da71814)\" d=\"M 58.999432 157.899253 \r\nL 59.00291 157.91767 \r\nL 59.06777 157.670902 \r\nL 59.228774 156.902172 \r\nL 59.812312 154.783342 \r\nL 59.929148 154.298762 \r\nL 60.026357 153.866159 \r\nL 61.408288 148.785263 \r\nL 61.594048 147.870432 \r\nL 62.249113 145.784745 \r\nL 62.374808 145.309906 \r\nL 62.473072 144.974738 \r\nL 62.593718 144.613645 \r\nL 62.927563 143.484208 \r\nL 62.985202 143.296352 \r\nL 63.135757 142.666951 \r\nL 63.214433 142.414492 \r\nL 65.169465 135.604099 \r\nL 65.269688 135.343101 \r\nL 65.369252 135.051078 \r\nL 65.417797 134.88955 \r\nL 65.56701 134.47542 \r\nL 65.646108 134.161997 \r\nL 65.912014 133.542975 \r\nL 66.398112 132.118776 \r\nL 66.543882 131.585719 \r\nL 66.979006 130.310873 \r\nL 67.228982 129.523875 \r\nL 67.289498 129.220657 \r\nL 67.445569 128.710475 \r\nL 67.715867 127.903638 \r\nL 68.201478 126.710245 \r\nL 68.432521 126.295784 \r\nL 68.79528 125.255488 \r\nL 68.870967 125.088307 \r\nL 69.009101 124.639424 \r\nL 73.991963 110.843755 \r\nL 74.134746 110.588836 \r\nL 75.393599 107.201388 \r\nL 75.431714 107.22402 \r\nL 75.450554 107.115802 \r\nL 75.520176 106.948528 \r\nL 75.949365 105.956069 \r\nL 76.341051 105.192145 \r\nL 76.483254 104.734764 \r\nL 76.66733 104.393421 \r\nL 76.788983 103.992504 \r\nL 77.196905 102.85329 \r\nL 77.207053 102.875773 \r\nL 77.262363 102.72883 \r\nL 77.862427 101.191965 \r\nL 77.992315 100.583374 \r\nL 78.123586 100.430207 \r\nL 78.403629 99.773664 \r\nL 78.530866 99.306143 \r\nL 78.733411 99.086664 \r\nL 78.848298 98.803249 \r\nL 78.907761 98.48671 \r\nL 79.038031 98.136518 \r\nL 79.233785 97.880505 \r\nL 79.495568 97.322461 \r\nL 79.632322 97.08484 \r\nL 79.751155 96.823945 \r\nL 80.188807 96.000885 \r\nL 80.250653 95.768116 \r\nL 80.351542 95.601293 \r\nL 80.466436 95.422474 \r\nL 80.587235 95.153653 \r\nL 80.958385 94.395903 \r\nL 81.183954 93.718734 \r\nL 81.193617 93.740593 \r\nL 81.225836 93.636877 \r\nL 81.455128 93.285715 \r\nL 81.840552 92.47882 \r\nL 81.960962 91.86152 \r\nL 82.742369 90.057022 \r\nL 82.884163 89.487219 \r\nL 83.09023 89.104269 \r\nL 83.436999 88.507749 \r\nL 83.634531 88.14991 \r\nL 83.763387 87.959447 \r\nL 84.093771 87.135761 \r\nL 84.23979 86.903349 \r\nL 84.370193 86.553141 \r\nL 84.733367 86.14357 \r\nL 84.9169 85.828509 \r\nL 85.800798 84.576457 \r\nL 86.173239 84.030967 \r\nL 86.273673 83.663279 \r\nL 86.825119 82.96301 \r\nL 86.934493 82.817561 \r\nL 87.066613 82.567507 \r\nL 87.295194 82.253248 \r\nL 87.638648 81.809352 \r\nL 87.776559 81.55498 \r\nL 88.410127 80.41852 \r\nL 88.427536 80.438891 \r\nL 88.479042 80.352372 \r\nL 89.094584 79.494411 \r\nL 89.158303 79.318401 \r\nL 89.323037 79.141933 \r\nL 89.527506 78.940868 \r\nL 89.622127 78.763331 \r\nL 89.72013 78.645723 \r\nL 89.888347 78.332926 \r\nL 90.108674 77.94867 \r\nL 90.378444 77.607199 \r\nL 90.568186 77.309235 \r\nL 90.644942 77.126886 \r\nL 92.002299 74.524482 \r\nL 92.539946 73.625774 \r\nL 92.602442 73.384455 \r\nL 92.635994 73.403703 \r\nL 92.845593 73.180718 \r\nL 94.117458 71.043966 \r\nL 94.246738 70.842984 \r\nL 94.573066 70.388716 \r\nL 94.674001 70.356744 \r\nL 94.909567 70.158844 \r\nL 95.096325 69.852844 \r\nL 95.293168 69.545546 \r\nL 95.501567 69.288468 \r\nL 95.515816 69.236943 \r\nL 95.530714 69.255411 \r\nL 96.064249 69.037606 \r\nL 96.309909 68.569604 \r\nL 96.705567 68.221803 \r\nL 96.853074 67.906482 \r\nL 97.164346 67.483938 \r\nL 97.207566 67.502061 \r\nL 97.218022 67.44906 \r\nL 97.680506 66.952947 \r\nL 97.707613 66.989006 \r\nL 97.85309 66.900041 \r\nL 97.985411 66.8645 \r\nL 98.392235 66.488314 \r\nL 98.571148 66.326292 \r\nL 98.653971 66.290126 \r\nL 98.691066 66.127592 \r\nL 99.076685 65.928165 \r\nL 99.240296 65.600993 \r\nL 99.246366 65.618765 \r\nL 99.517352 65.235147 \r\nL 99.552467 65.252836 \r\nL 99.556195 65.197859 \r\nL 99.813295 64.867145 \r\nL 99.912973 64.479459 \r\nL 100.068094 64.312693 \r\nL 100.205642 64.29194 \r\nL 100.612238 63.767358 \r\nL 100.62498 63.784719 \r\nL 100.638918 63.728521 \r\nL 100.861728 63.559674 \r\nL 101.387774 62.818076 \r\nL 101.728235 62.812511 \r\nL 102.218099 62.354579 \r\nL 102.46375 62.158569 \r\nL 102.486564 62.10093 \r\nL 102.734835 62.060232 \r\nL 102.964771 61.572705 \r\nL 103.35888 61.239821 \r\nL 104.592588 60.118468 \r\nL 104.791917 60.075492 \r\nL 104.989317 59.673947 \r\nL 105.443209 59.554077 \r\nL 105.465925 59.494072 \r\nL 105.507049 59.510466 \r\nL 106.324818 58.988995 \r\nL 106.442339 58.701629 \r\nL 106.477046 58.579832 \r\nL 106.791874 58.412917 \r\nL 106.98383 57.983997 \r\nL 107.542654 57.444925 \r\nL 107.65762 57.398864 \r\nL 107.962224 56.651783 \r\nL 108.189161 56.463904 \r\nL 108.530925 55.865232 \r\nL 108.71651 55.738655 \r\nL 108.803404 55.357711 \r\nL 108.809112 55.37299 \r\nL 109.125058 54.990101 \r\nL 109.555687 54.198958 \r\nL 109.595842 54.228877 \r\nL 109.646569 54.243855 \r\nL 109.653872 54.17907 \r\nL 109.694922 54.049343 \r\nL 109.885849 54.014232 \r\nL 110.256894 53.73306 \r\nL 110.399144 53.697407 \r\nL 110.47523 53.566445 \r\nL 111.04911 53.303881 \r\nL 112.900784 51.299175 \r\nL 113.013196 51.245599 \r\nL 113.175193 51.042031 \r\nL 113.285703 50.906038 \r\nL 113.291805 50.920084 \r\nL 113.421007 50.783819 \r\nL 113.421252 50.797832 \r\nL 114.062807 50.346701 \r\nL 114.287411 50.154295 \r\nL 114.612028 50.099223 \r\nL 115.143244 49.836761 \r\nL 115.312948 49.642609 \r\nL 115.698409 49.600521 \r\nL 115.749869 49.461426 \r\nL 116.849591 48.899794 \r\nL 116.960462 48.772589 \r\nL 117.528022 48.446488 \r\nL 117.642951 48.389025 \r\nL 117.777749 48.176255 \r\nL 118.091891 47.85985 \r\nL 118.337973 47.827877 \r\nL 118.378225 47.684698 \r\nL 119.347517 46.990853 \r\nL 119.407945 47.016647 \r\nL 119.413328 46.944244 \r\nL 120.03129 46.645194 \r\nL 120.167614 46.365888 \r\nL 120.836502 45.864764 \r\nL 121.196048 45.65605 \r\nL 121.583151 45.459014 \r\nL 121.604638 45.236429 \r\nL 122.005415 45.186734 \r\nL 122.096116 45.062343 \r\nL 122.429302 45.012266 \r\nL 122.606272 44.812246 \r\nL 122.625084 44.824478 \r\nL 122.684788 44.83672 \r\nL 122.688942 44.76166 \r\nL 122.774288 44.686535 \r\nL 123.345771 44.647884 \r\nL 123.63581 44.433775 \r\nL 124.026145 44.382415 \r\nL 124.438903 44.15513 \r\nL 124.583119 43.951153 \r\nL 125.295214 43.298464 \r\nL 125.470384 43.333731 \r\nL 125.822664 43.025533 \r\nL 126.098914 42.716225 \r\nL 126.193821 42.650235 \r\nL 126.406388 42.584126 \r\nL 126.722825 42.517898 \r\nL 126.799811 42.362226 \r\nL 127.209375 42.139523 \r\nL 127.331217 41.837587 \r\nL 127.766154 41.803954 \r\nL 128.035272 41.65768 \r\nL 128.07603 41.578814 \r\nL 128.102038 41.601143 \r\nL 128.340145 41.544452 \r\nL 128.58191 41.306836 \r\nL 128.685989 41.23853 \r\nL 129.032584 40.931205 \r\nL 129.241794 40.873253 \r\nL 129.481477 40.815141 \r\nL 129.502042 40.73509 \r\nL 129.682976 40.665817 \r\nL 129.687933 40.585587 \r\nL 130.081694 40.263925 \r\nL 130.145386 40.113285 \r\nL 130.914686 39.608891 \r\nL 131.963096 39.600696 \r\nL 132.045771 39.529361 \r\nL 133.057523 39.160317 \r\nL 133.281547 39.0882 \r\nL 133.496206 39.015948 \r\nL 133.607286 38.850538 \r\nL 134.164144 38.818859 \r\nL 134.20824 38.735788 \r\nL 134.600342 38.662827 \r\nL 134.747339 38.339364 \r\nL 134.993843 38.285882 \r\nL 135.094175 38.118208 \r\nL 135.348757 38.04421 \r\nL 135.442978 37.97007 \r\nL 135.874715 37.915629 \r\nL 135.879287 37.831256 \r\nL 136.015183 37.662268 \r\nL 136.030587 37.577653 \r\nL 137.791588 37.030866 \r\nL 137.793749 37.040452 \r\nL 137.833951 36.878924 \r\nL 138.450494 36.582698 \r\nL 138.843504 36.534119 \r\nL 139.505795 36.041894 \r\nL 139.939572 35.973235 \r\nL 140.407384 35.651073 \r\nL 140.703892 35.475649 \r\nL 140.776106 35.299882 \r\nL 141.589085 34.991268 \r\nL 141.589697 34.902713 \r\nL 142.832224 34.573505 \r\nL 143.073776 34.314432 \r\nL 143.841333 34.242012 \r\nL 144.047088 34.160898 \r\nL 144.356997 34.186361 \r\nL 144.673684 34.104989 \r\nL 145.164541 34.040344 \r\nL 145.177988 33.950167 \r\nL 145.764253 33.69569 \r\nL 148.983655 32.940542 \r\nL 149.333078 32.756891 \r\nL 150.439106 32.74402 \r\nL 150.503085 32.651535 \r\nL 151.305232 32.396884 \r\nL 152.676868 32.350581 \r\nL 152.942847 31.984248 \r\nL 153.736528 31.943817 \r\nL 153.884462 31.762918 \r\nL 155.292422 31.414255 \r\nL 155.68674 31.334021 \r\nL 155.706855 31.238839 \r\nL 156.116298 31.172803 \r\nL 156.140178 31.077239 \r\nL 156.736681 31.010576 \r\nL 156.903004 30.825756 \r\nL 157.083767 30.736699 \r\nL 157.374127 30.647454 \r\nL 157.481284 30.550965 \r\nL 157.823419 30.461385 \r\nL 158.010034 30.267806 \r\nL 158.139875 30.080649 \r\nL 160.008442 30.031131 \r\nL 160.06133 29.9335 \r\nL 160.671687 29.96071 \r\nL 163.006137 29.542782 \r\nL 164.002058 29.483538 \r\nL 164.010024 29.384148 \r\nL 164.011861 29.390747 \r\nL 165.096921 29.024016 \r\nL 165.684493 28.823452 \r\nL 166.087945 28.729296 \r\nL 166.298013 28.634937 \r\nL 166.404735 28.540372 \r\nL 166.703144 28.451765 \r\nL 168.054452 28.488875 \r\nL 168.148162 28.393585 \r\nL 168.822537 28.316462 \r\nL 170.813331 28.288005 \r\nL 172.581223 27.912448 \r\nL 172.796224 27.705434 \r\nL 174.592577 27.64858 \r\nL 175.048861 27.550177 \r\nL 176.503163 27.480548 \r\nL 177.035362 27.381462 \r\nL 179.194873 27.439445 \r\nL 179.240805 27.333864 \r\nL 180.111652 27.133681 \r\nL 180.228649 27.027628 \r\nL 180.956189 26.949341 \r\nL 180.982323 26.842763 \r\nL 181.412112 26.758027 \r\nL 181.624063 26.656424 \r\nL 181.924085 26.661879 \r\nL 182.242368 26.559993 \r\nL 184.984304 26.103983 \r\nL 185.011745 25.994947 \r\nL 185.585286 25.901074 \r\nL 185.717594 25.906183 \r\nL 186.15292 25.691988 \r\nL 186.374348 25.592025 \r\nL 186.417536 25.481929 \r\nL 188.07763 25.400752 \r\nL 188.190342 25.294829 \r\nL 188.530094 25.193407 \r\nL 193.303554 25.206026 \r\nL 194.171452 24.762588 \r\nL 194.262529 24.767178 \r\nL 194.492674 24.66256 \r\nL 194.776468 24.548609 \r\nL 198.083989 24.51922 \r\nL 199.342022 24.301247 \r\nL 199.410424 24.185273 \r\nL 200.0194 24.202605 \r\nL 200.279639 24.090495 \r\nL 201.88691 24.142238 \r\nL 202.118947 24.033368 \r\nL 202.185143 24.037655 \r\nL 202.233079 23.919872 \r\nL 202.635057 23.924095 \r\nL 202.792737 23.806096 \r\nL 204.776567 23.716666 \r\nL 204.843523 23.601929 \r\nL 205.613185 23.371597 \r\nL 205.775586 23.37552 \r\nL 206.116394 23.263716 \r\nL 206.401709 23.267581 \r\nL 206.424884 23.147694 \r\nL 206.436328 23.151491 \r\nL 207.315261 23.182041 \r\nL 207.650264 23.065112 \r\nL 210.644788 23.000148 \r\nL 210.888111 22.881646 \r\nL 212.121643 22.91136 \r\nL 212.297695 22.791984 \r\nL 213.023937 22.799317 \r\nL 213.070305 22.67949 \r\nL 213.746544 22.690306 \r\nL 214.518264 22.573494 \r\nL 217.792214 22.616355 \r\nL 217.914422 22.494729 \r\nL 220.095096 22.533785 \r\nL 220.34352 22.407513 \r\nL 222.335473 22.435626 \r\nL 222.33883 22.308538 \r\nL 222.385681 22.311992 \r\nL 223.477789 22.339784 \r\nL 223.712005 22.093621 \r\nL 224.122533 21.845811 \r\nL 224.272341 21.848999 \r\nL 224.355229 21.722838 \r\nL 224.963695 21.599369 \r\nL 226.134184 21.617639 \r\nL 226.314595 21.487256 \r\nL 226.755893 21.493206 \r\nL 226.832364 21.365369 \r\nL 227.266763 21.240002 \r\nL 230.778101 21.192276 \r\nL 230.953188 21.058074 \r\nL 232.123863 21.074669 \r\nL 232.316069 20.93973 \r\nL 234.678266 20.95862 \r\nL 235.001436 20.689405 \r\nL 236.160797 20.707192 \r\nL 236.256242 20.433178 \r\nL 236.274029 20.435544 \r\nL 236.851502 20.449807 \r\nL 240.106107 20.493335 \r\nL 240.196878 20.353614 \r\nL 241.669857 20.377292 \r\nL 241.728448 20.238659 \r\nL 250.650785 20.246632 \r\nL 250.735343 20.100609 \r\nL 253.540767 20.135527 \r\nL 253.941613 19.98966 \r\nL 255.518309 20.01231 \r\nL 255.583874 19.860333 \r\nL 256.349993 19.871205 \r\nL 256.494199 19.724613 \r\nL 259.330115 19.77536 \r\nL 259.421776 19.621087 \r\nL 260.734431 19.635411 \r\nL 260.902897 19.47979 \r\nL 267.865776 19.561864 \r\nL 267.970495 19.398582 \r\nL 267.971774 19.40052 \r\nL 276.144259 19.482835 \r\nL 276.360503 19.317271 \r\nL 278.304475 19.342787 \r\nL 278.521961 19.17084 \r\nL 279.528677 19.181913 \r\nL 279.608487 18.835297 \r\nL 279.61049 18.83685 \r\nL 280.020295 18.84309 \r\nL 280.219345 18.6728 \r\nL 283.328112 18.706118 \r\nL 283.349738 18.527159 \r\nL 284.298512 18.351345 \r\nL 284.679364 18.354837 \r\nL 284.835663 18.176247 \r\nL 285.735286 18.180286 \r\nL 285.847275 17.998357 \r\nL 292.109267 18.042718 \r\nL 292.206214 17.852393 \r\nL 312.430631 18.040166 \r\nL 316.391187 18.087507 \r\nL 316.394915 17.837496 \r\nL 318.708332 17.857153 \r\nL 318.826609 17.600663 \r\nL 324.157948 17.646261 \r\nL 324.180742 17.365751 \r\nL 331.203956 17.41657 \r\nL 331.302869 17.083636 \r\nL 363.363068 17.083636 \r\nL 363.363068 17.083636 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p6e7da71814)\" d=\"M 58.999432 17.083636 \r\nL 59.112493 17.173447 \r\nL 61.954018 17.263257 \r\nL 62.072184 17.353068 \r\nL 64.794554 17.442878 \r\nL 64.903995 17.532688 \r\nL 66.914815 17.622499 \r\nL 67.034046 17.712309 \r\nL 67.120071 17.80212 \r\nL 67.228982 17.89193 \r\nL 68.145204 17.981741 \r\nL 68.245639 18.161361 \r\nL 69.186092 18.251172 \r\nL 69.288877 18.430793 \r\nL 69.64305 18.520603 \r\nL 69.749342 18.610413 \r\nL 69.948068 18.700224 \r\nL 70.055308 18.790034 \r\nL 70.220667 18.879845 \r\nL 70.336345 19.059466 \r\nL 70.722989 19.149276 \r\nL 70.836037 19.239086 \r\nL 72.780983 19.328897 \r\nL 72.90442 19.418707 \r\nL 73.802608 19.508518 \r\nL 73.901779 19.598328 \r\nL 74.021794 19.688138 \r\nL 74.746103 19.777949 \r\nL 74.846797 19.867759 \r\nL 75.08067 19.95757 \r\nL 75.188662 20.04738 \r\nL 75.393599 20.137191 \r\nL 75.560037 20.316811 \r\nL 76.156797 20.406622 \r\nL 76.283696 20.586243 \r\nL 76.922941 20.676053 \r\nL 77.039785 20.765863 \r\nL 77.738524 20.855674 \r\nL 77.87513 21.035295 \r\nL 79.36447 21.125105 \r\nL 79.462556 21.214916 \r\nL 79.880359 21.304726 \r\nL 79.987825 21.484347 \r\nL 80.419752 21.574157 \r\nL 80.493476 21.663968 \r\nL 81.049993 21.753778 \r\nL 81.154528 21.933399 \r\nL 81.300828 22.023209 \r\nL 81.418191 22.292641 \r\nL 82.086917 22.382451 \r\nL 82.163254 22.472261 \r\nL 82.76371 22.562072 \r\nL 82.894763 22.651882 \r\nL 82.970757 22.741693 \r\nL 83.120295 22.921313 \r\nL 83.273434 23.011124 \r\nL 83.436999 23.100934 \r\nL 83.875353 23.190745 \r\nL 83.99848 23.280555 \r\nL 85.183153 23.370366 \r\nL 85.292717 23.549986 \r\nL 85.512481 23.639797 \r\nL 85.621906 23.729607 \r\nL 86.05419 23.819418 \r\nL 86.161431 24.088849 \r\nL 86.374112 24.178659 \r\nL 86.622093 24.35828 \r\nL 86.861934 24.448091 \r\nL 87.033907 24.537901 \r\nL 87.341467 24.627711 \r\nL 87.573604 24.897143 \r\nL 88.410127 24.986953 \r\nL 88.516301 25.076763 \r\nL 88.663725 25.166574 \r\nL 88.778511 25.525816 \r\nL 89.623076 25.615626 \r\nL 89.72013 25.885057 \r\nL 90.323337 25.974868 \r\nL 90.475607 26.154488 \r\nL 91.443531 26.244299 \r\nL 91.531621 26.334109 \r\nL 91.669902 26.42392 \r\nL 92.036867 26.603541 \r\nL 92.502082 26.693351 \r\nL 92.602442 26.872972 \r\nL 92.778851 27.052593 \r\nL 93.078122 27.142403 \r\nL 93.184338 27.322024 \r\nL 93.432248 27.411834 \r\nL 93.508069 27.591455 \r\nL 94.573066 27.681266 \r\nL 94.674001 27.771076 \r\nL 94.856883 27.860886 \r\nL 94.926635 28.040507 \r\nL 95.815732 28.130318 \r\nL 96.030288 28.489559 \r\nL 96.646155 28.57937 \r\nL 96.802565 28.66918 \r\nL 97.330855 28.758991 \r\nL 97.552543 28.848801 \r\nL 97.680506 28.938611 \r\nL 97.81867 29.208043 \r\nL 98.073349 29.297853 \r\nL 98.392235 29.567284 \r\nL 98.872241 29.657095 \r\nL 99.104594 29.746905 \r\nL 99.517352 29.836716 \r\nL 99.621728 29.926526 \r\nL 100.190438 30.016336 \r\nL 100.299427 30.106147 \r\nL 100.40926 30.195957 \r\nL 100.51728 30.285768 \r\nL 100.930265 30.375578 \r\nL 101.015695 30.465388 \r\nL 101.345059 30.555199 \r\nL 101.526397 30.73482 \r\nL 101.930684 30.914441 \r\nL 102.404932 31.004251 \r\nL 102.486564 31.094061 \r\nL 103.161902 31.363493 \r\nL 103.49397 31.453303 \r\nL 103.58086 31.543113 \r\nL 103.832289 31.722734 \r\nL 104.309175 31.812545 \r\nL 104.832731 32.171786 \r\nL 105.465925 32.261597 \r\nL 105.645534 32.441218 \r\nL 105.745166 32.531028 \r\nL 105.779501 32.620838 \r\nL 106.213487 32.710649 \r\nL 106.477046 32.89027 \r\nL 107.187101 32.98008 \r\nL 107.278901 33.069891 \r\nL 108.248494 33.159701 \r\nL 108.530925 33.339322 \r\nL 109.35519 33.429132 \r\nL 109.483454 33.698563 \r\nL 109.578816 33.788374 \r\nL 109.694922 33.967995 \r\nL 110.353809 34.506857 \r\nL 110.47523 34.596668 \r\nL 111.673536 34.686478 \r\nL 111.888778 34.776288 \r\nL 112.065391 34.866099 \r\nL 112.182365 34.955909 \r\nL 112.415631 35.04572 \r\nL 112.500995 35.13553 \r\nL 112.627145 35.315151 \r\nL 112.900784 35.404961 \r\nL 113.117719 35.494772 \r\nL 113.421007 35.584582 \r\nL 113.421252 35.674393 \r\nL 113.554047 35.764203 \r\nL 113.699644 35.854013 \r\nL 114.287411 35.943824 \r\nL 114.612028 36.033634 \r\nL 115.201432 36.123445 \r\nL 115.48012 36.303066 \r\nL 115.918075 36.392876 \r\nL 116.154953 36.662307 \r\nL 116.474312 36.752118 \r\nL 116.534053 36.841928 \r\nL 117.047918 36.931738 \r\nL 117.6556 37.20117 \r\nL 117.857642 37.29098 \r\nL 117.999701 37.470601 \r\nL 118.213589 37.560411 \r\nL 118.464962 37.740032 \r\nL 118.978721 37.829843 \r\nL 119.039292 37.919653 \r\nL 119.38853 38.009463 \r\nL 119.413328 38.099274 \r\nL 119.765395 38.189084 \r\nL 119.891257 38.458516 \r\nL 120.070569 38.548326 \r\nL 120.167614 38.638136 \r\nL 121.15868 38.727947 \r\nL 121.196048 38.817757 \r\nL 121.503221 38.907568 \r\nL 121.604638 38.997378 \r\nL 122.011939 39.176999 \r\nL 122.096116 39.35662 \r\nL 122.526347 39.626051 \r\nL 122.625084 39.805672 \r\nL 122.774288 39.895482 \r\nL 124.026145 40.434345 \r\nL 124.480556 40.524155 \r\nL 124.786917 40.703776 \r\nL 125.108186 40.793586 \r\nL 125.470384 41.152828 \r\nL 126.332468 41.242638 \r\nL 126.527739 41.422259 \r\nL 127.224194 41.51207 \r\nL 127.331217 41.60188 \r\nL 127.692725 41.781501 \r\nL 127.766154 41.961122 \r\nL 128.07603 42.050932 \r\nL 128.102038 42.230553 \r\nL 129.444332 42.859226 \r\nL 129.502042 42.949036 \r\nL 129.829009 43.038847 \r\nL 130.265035 43.128657 \r\nL 130.429922 43.398088 \r\nL 130.831056 43.487899 \r\nL 130.914686 43.66752 \r\nL 131.505949 43.847141 \r\nL 131.520555 43.936951 \r\nL 131.76348 44.116572 \r\nL 131.776083 44.206382 \r\nL 133.526169 44.924866 \r\nL 133.828158 45.014676 \r\nL 134.164144 45.373918 \r\nL 134.716207 45.463728 \r\nL 134.848487 45.733159 \r\nL 135.348757 45.91278 \r\nL 135.442978 46.002591 \r\nL 135.6738 46.182211 \r\nL 136.336814 46.272022 \r\nL 136.862354 46.451643 \r\nL 137.733034 46.541453 \r\nL 137.965748 46.900695 \r\nL 138.251184 47.170126 \r\nL 138.427059 47.259936 \r\nL 138.579473 47.529368 \r\nL 138.93575 47.708988 \r\nL 139.256406 47.798799 \r\nL 139.718162 48.06823 \r\nL 140.097679 48.158041 \r\nL 140.165294 48.337661 \r\nL 141.078503 48.427472 \r\nL 141.126559 48.517282 \r\nL 141.589697 48.876524 \r\nL 142.215885 48.966334 \r\nL 142.339457 49.056145 \r\nL 142.981049 49.145955 \r\nL 143.073776 49.235766 \r\nL 144.047088 49.505197 \r\nL 144.356997 49.774628 \r\nL 144.673684 49.864438 \r\nL 145.060397 50.044059 \r\nL 145.177988 50.13387 \r\nL 145.506731 50.22368 \r\nL 145.764253 50.313491 \r\nL 146.300745 50.403301 \r\nL 146.374592 50.493111 \r\nL 146.514235 50.582922 \r\nL 147.011222 50.672732 \r\nL 147.152572 50.762543 \r\nL 148.16436 50.852353 \r\nL 148.272482 50.942163 \r\nL 149.333078 51.031974 \r\nL 150.020881 51.481026 \r\nL 150.138777 51.840268 \r\nL 150.503085 51.930078 \r\nL 150.822388 52.019888 \r\nL 150.891477 52.109699 \r\nL 150.983222 52.199509 \r\nL 152.023582 52.28932 \r\nL 152.356184 52.648561 \r\nL 153.261037 53.007803 \r\nL 153.736528 53.456855 \r\nL 154.171093 53.546666 \r\nL 154.328709 53.726286 \r\nL 154.962297 53.816097 \r\nL 155.706855 54.085528 \r\nL 156.76491 54.804011 \r\nL 156.776891 54.893822 \r\nL 157.083767 54.983632 \r\nL 157.823419 55.163253 \r\nL 158.139875 55.253063 \r\nL 159.328652 55.522495 \r\nL 159.524531 55.702116 \r\nL 160.06133 55.881736 \r\nL 160.402973 56.061357 \r\nL 161.781184 56.510409 \r\nL 162.167637 56.60022 \r\nL 162.947889 57.049272 \r\nL 163.207701 57.318703 \r\nL 163.514855 57.498324 \r\nL 163.61707 57.588134 \r\nL 164.010024 57.767755 \r\nL 164.011861 57.857566 \r\nL 164.484754 57.947376 \r\nL 164.600416 58.216807 \r\nL 165.684493 58.306618 \r\nL 165.848396 58.396428 \r\nL 166.298013 58.486238 \r\nL 166.404735 58.576049 \r\nL 166.687453 58.665859 \r\nL 166.80687 58.84548 \r\nL 167.573889 59.025101 \r\nL 168.041849 59.204722 \r\nL 168.148162 59.384343 \r\nL 168.822537 59.743584 \r\nL 169.610903 60.282447 \r\nL 169.670635 60.372257 \r\nL 170.813331 60.821309 \r\nL 171.930163 60.91112 \r\nL 172.300359 61.090741 \r\nL 172.494922 61.270361 \r\nL 172.796224 61.360172 \r\nL 174.592577 62.078655 \r\nL 175.048861 62.168466 \r\nL 175.521366 62.348086 \r\nL 175.602436 62.437897 \r\nL 177.035362 62.797138 \r\nL 177.124 62.886949 \r\nL 177.933177 63.06657 \r\nL 177.979295 63.15638 \r\nL 178.43366 63.336001 \r\nL 178.465246 63.425811 \r\nL 178.998669 63.605432 \r\nL 179.240805 63.695243 \r\nL 180.041598 63.785053 \r\nL 180.228649 63.874863 \r\nL 180.614871 64.054484 \r\nL 180.805037 64.323916 \r\nL 181.20733 64.413726 \r\nL 181.412112 64.683157 \r\nL 181.624063 64.772968 \r\nL 182.242368 64.952588 \r\nL 182.925904 65.042399 \r\nL 182.938851 65.132209 \r\nL 183.290185 65.491451 \r\nL 183.526859 65.671072 \r\nL 185.011745 66.389555 \r\nL 185.487597 66.479366 \r\nL 185.717594 66.748797 \r\nL 186.15292 66.838607 \r\nL 186.417536 67.018228 \r\nL 187.702138 67.287659 \r\nL 188.190342 67.646901 \r\nL 188.360255 67.736711 \r\nL 188.530094 67.826522 \r\nL 189.664462 68.095953 \r\nL 189.816207 68.275574 \r\nL 190.313361 68.455195 \r\nL 190.334246 68.545005 \r\nL 190.552242 68.634816 \r\nL 190.745859 68.814436 \r\nL 191.592 69.083868 \r\nL 192.670541 69.712541 \r\nL 193.136368 70.071782 \r\nL 193.490197 70.161593 \r\nL 194.088035 70.251403 \r\nL 194.262529 70.431024 \r\nL 194.426618 70.520834 \r\nL 194.492674 70.610645 \r\nL 195.141462 70.700455 \r\nL 195.614922 70.880076 \r\nL 195.828931 71.149507 \r\nL 197.088466 71.598559 \r\nL 197.234184 71.77818 \r\nL 197.349512 71.957801 \r\nL 197.795317 72.137422 \r\nL 197.840424 72.227232 \r\nL 198.083989 72.317043 \r\nL 199.039977 72.406853 \r\nL 201.09744 73.574389 \r\nL 201.204514 73.664199 \r\nL 201.780726 73.84382 \r\nL 201.993669 74.113251 \r\nL 202.233079 74.382682 \r\nL 204.644955 75.011355 \r\nL 204.843523 75.190976 \r\nL 205.440814 75.280786 \r\nL 206.695017 75.99927 \r\nL 206.802462 76.268701 \r\nL 207.650264 76.627943 \r\nL 208.190225 76.807564 \r\nL 208.882692 77.166805 \r\nL 209.795632 77.346426 \r\nL 210.343652 77.795478 \r\nL 210.343949 77.885289 \r\nL 211.161815 78.24453 \r\nL 212.121643 78.783393 \r\nL 212.297695 78.873203 \r\nL 217.914422 80.759222 \r\nL 218.134061 81.028653 \r\nL 218.967377 81.298084 \r\nL 219.207732 81.387895 \r\nL 219.888403 81.567516 \r\nL 220.095096 81.747136 \r\nL 220.688854 81.836947 \r\nL 221.581837 82.016568 \r\nL 222.317816 82.375809 \r\nL 222.385681 82.55543 \r\nL 222.872049 82.914672 \r\nL 223.477789 83.273914 \r\nL 223.642026 83.363724 \r\nL 223.712005 83.543345 \r\nL 223.917473 83.633155 \r\nL 224.122533 83.812776 \r\nL 224.355229 83.992397 \r\nL 224.810234 84.082207 \r\nL 225.098015 84.351639 \r\nL 225.129435 84.441449 \r\nL 226.022696 84.62107 \r\nL 226.134184 84.71088 \r\nL 226.727108 84.800691 \r\nL 226.832364 84.980311 \r\nL 227.069251 85.070122 \r\nL 227.797218 85.519174 \r\nL 228.086038 85.788605 \r\nL 228.425456 86.147847 \r\nL 229.105255 86.596899 \r\nL 229.229411 86.77652 \r\nL 230.778101 87.854245 \r\nL 231.519681 87.944055 \r\nL 231.524429 88.033866 \r\nL 232.316069 88.393107 \r\nL 234.292573 88.752349 \r\nL 234.656677 88.93197 \r\nL 234.787733 89.02178 \r\nL 234.823937 89.111591 \r\nL 235.153506 89.201401 \r\nL 235.964343 89.470832 \r\nL 236.463694 90.009695 \r\nL 236.736397 90.189316 \r\nL 236.851502 90.368936 \r\nL 237.272398 90.638368 \r\nL 238.185561 90.907799 \r\nL 238.563112 91.08742 \r\nL 239.217056 91.536472 \r\nL 239.921059 91.805903 \r\nL 240.196878 91.985524 \r\nL 240.688477 92.165145 \r\nL 240.721398 92.254955 \r\nL 240.958063 92.434576 \r\nL 241.094665 92.614197 \r\nL 241.610839 92.793818 \r\nL 241.728448 92.973439 \r\nL 242.575174 93.33268 \r\nL 242.590587 93.512301 \r\nL 242.767714 93.691922 \r\nL 242.978969 93.961353 \r\nL 243.230027 94.410405 \r\nL 243.576418 95.039078 \r\nL 244.032665 95.218699 \r\nL 244.083893 95.48813 \r\nL 244.699444 95.667751 \r\nL 244.767977 95.757561 \r\nL 245.241863 96.026993 \r\nL 245.258834 96.206614 \r\nL 245.542275 96.386234 \r\nL 246.911574 96.655666 \r\nL 247.492275 97.014907 \r\nL 247.922871 97.194528 \r\nL 248.024566 97.374149 \r\nL 248.253293 97.64358 \r\nL 248.339705 97.733391 \r\nL 248.780299 97.913011 \r\nL 249.021749 98.362064 \r\nL 251.740834 99.260168 \r\nL 251.760365 99.349978 \r\nL 252.478557 99.529599 \r\nL 252.672081 99.79903 \r\nL 253.364455 100.068461 \r\nL 253.941613 100.427703 \r\nL 255.070037 100.966566 \r\nL 255.304105 101.146186 \r\nL 255.327679 101.235997 \r\nL 256.450854 101.86467 \r\nL 256.726561 102.403532 \r\nL 256.927244 102.583153 \r\nL 257.674017 102.852584 \r\nL 257.717826 102.942395 \r\nL 258.53994 103.391447 \r\nL 258.738935 103.571068 \r\nL 259.105209 103.840499 \r\nL 259.366709 104.199741 \r\nL 259.571287 104.379361 \r\nL 259.840465 104.558982 \r\nL 259.858178 104.648793 \r\nL 260.619864 104.828414 \r\nL 261.107512 105.187655 \r\nL 261.750328 105.367276 \r\nL 261.801593 105.457086 \r\nL 262.235472 105.636707 \r\nL 262.295825 105.726518 \r\nL 262.85451 105.906139 \r\nL 263.00773 105.995949 \r\nL 264.217391 106.26538 \r\nL 264.524926 106.624622 \r\nL 265.380462 107.253295 \r\nL 265.41266 107.343105 \r\nL 265.731638 107.522726 \r\nL 266.117572 107.792157 \r\nL 266.944193 108.061589 \r\nL 267.341107 108.42083 \r\nL 267.83215 108.600451 \r\nL 267.971774 108.780072 \r\nL 268.258499 108.959693 \r\nL 268.290827 109.049503 \r\nL 268.628872 109.229124 \r\nL 269.878391 109.498555 \r\nL 269.980773 109.678176 \r\nL 270.537937 109.947607 \r\nL 272.307637 110.217039 \r\nL 272.402396 110.396659 \r\nL 272.816114 110.666091 \r\nL 272.987102 110.845711 \r\nL 274.595236 111.294764 \r\nL 274.823721 111.654005 \r\nL 274.847425 111.743816 \r\nL 275.410301 111.923436 \r\nL 276.360503 112.64192 \r\nL 276.748292 112.821541 \r\nL 276.778784 112.911351 \r\nL 278.087285 113.360403 \r\nL 278.304475 113.809455 \r\nL 279.023761 113.899266 \r\nL 279.105908 114.078886 \r\nL 279.608487 114.348318 \r\nL 279.61049 114.438128 \r\nL 279.954025 114.617749 \r\nL 280.219345 115.066801 \r\nL 280.591981 115.246422 \r\nL 280.872548 115.426043 \r\nL 280.905914 115.515853 \r\nL 282.113925 116.144526 \r\nL 282.115539 116.234336 \r\nL 282.531353 116.413957 \r\nL 282.546265 116.503768 \r\nL 283.240865 116.95282 \r\nL 283.349738 117.132441 \r\nL 284.17762 117.222251 \r\nL 284.679364 117.671303 \r\nL 284.806766 117.761114 \r\nL 284.835663 117.850924 \r\nL 285.451381 118.030545 \r\nL 285.487326 118.120355 \r\nL 286.322905 118.479597 \r\nL 286.424118 118.659218 \r\nL 287.511199 119.10827 \r\nL 287.869832 119.467511 \r\nL 287.984028 119.647132 \r\nL 288.463052 120.185995 \r\nL 288.551245 120.365616 \r\nL 289.478095 120.994289 \r\nL 289.48774 121.084099 \r\nL 289.862342 121.26372 \r\nL 290.901608 121.982203 \r\nL 290.950611 122.161824 \r\nL 292.91734 123.239549 \r\nL 293.198759 123.598791 \r\nL 293.266754 123.778411 \r\nL 293.671811 124.047843 \r\nL 293.722668 124.137653 \r\nL 294.112849 124.407084 \r\nL 294.207014 124.676516 \r\nL 294.830912 125.125568 \r\nL 294.868934 125.215378 \r\nL 295.416119 125.57462 \r\nL 295.454642 125.66443 \r\nL 295.811902 125.933861 \r\nL 295.930439 126.203293 \r\nL 296.416677 126.382914 \r\nL 296.490422 126.472724 \r\nL 297.29779 126.831966 \r\nL 297.450342 127.011586 \r\nL 297.812165 127.281018 \r\nL 297.829618 127.370828 \r\nL 298.213308 127.550449 \r\nL 298.622353 128.089311 \r\nL 298.629048 128.179122 \r\nL 299.040671 128.358743 \r\nL 299.170002 128.538364 \r\nL 299.76912 129.256847 \r\nL 300.687661 129.526278 \r\nL 300.829103 129.795709 \r\nL 300.855849 129.88552 \r\nL 301.175791 130.154951 \r\nL 301.188441 130.244761 \r\nL 301.608224 130.514193 \r\nL 301.728671 130.783624 \r\nL 302.640906 131.771539 \r\nL 302.904613 132.400211 \r\nL 302.907599 132.490022 \r\nL 303.488318 132.669643 \r\nL 304.241843 133.298316 \r\nL 304.306314 133.477936 \r\nL 304.421197 133.747368 \r\nL 304.473167 133.926989 \r\nL 304.805851 134.28623 \r\nL 306.084583 135.363955 \r\nL 306.500509 135.543576 \r\nL 306.557672 135.723197 \r\nL 306.931031 136.082439 \r\nL 307.114817 136.44168 \r\nL 308.225046 137.339784 \r\nL 308.479813 137.788836 \r\nL 309.155383 138.148078 \r\nL 309.272696 138.417509 \r\nL 310.887618 139.405424 \r\nL 311.0191 139.674855 \r\nL 311.233601 139.854476 \r\nL 311.402234 140.034097 \r\nL 312.019621 140.483149 \r\nL 312.167907 140.75258 \r\nL 312.51849 141.022011 \r\nL 312.80796 141.381253 \r\nL 313.140014 141.650684 \r\nL 313.162048 141.740495 \r\nL 313.524891 141.920116 \r\nL 313.575432 142.009926 \r\nL 313.987593 142.189547 \r\nL 314.168986 142.638599 \r\nL 314.549301 142.81822 \r\nL 314.55544 142.90803 \r\nL 315.086192 143.177461 \r\nL 315.131541 143.357082 \r\nL 315.400144 143.536703 \r\nL 315.405931 143.626514 \r\nL 316.315848 144.344997 \r\nL 316.394915 144.434807 \r\nL 317.17296 144.704239 \r\nL 317.572694 145.422722 \r\nL 317.613349 145.512532 \r\nL 318.385737 145.961584 \r\nL 318.549641 146.141205 \r\nL 318.855042 146.410636 \r\nL 320.090967 147.847603 \r\nL 320.469723 148.566086 \r\nL 320.852226 148.835518 \r\nL 321.073051 149.015139 \r\nL 321.799089 149.194759 \r\nL 321.960396 149.37438 \r\nL 322.17664 149.643811 \r\nL 323.057994 150.541916 \r\nL 323.197841 151.080778 \r\nL 323.804434 151.260399 \r\nL 323.826876 151.44002 \r\nL 324.129607 151.709451 \r\nL 324.180742 151.978882 \r\nL 324.454928 152.158503 \r\nL 324.567548 152.517745 \r\nL 324.921321 152.966797 \r\nL 325.037558 153.236228 \r\nL 325.373841 153.505659 \r\nL 325.380852 153.59547 \r\nL 325.985849 153.775091 \r\nL 326.159193 153.954711 \r\nL 326.352031 154.852816 \r\nL 326.851048 155.301868 \r\nL 326.971885 155.481489 \r\nL 327.163739 155.75092 \r\nL 327.448367 156.199972 \r\nL 327.653631 156.828645 \r\nL 328.072617 157.187886 \r\nL 328.22274 157.367507 \r\nL 328.402297 157.636939 \r\nL 328.733461 157.90637 \r\nL 329.068427 158.085991 \r\nL 329.485614 158.984095 \r\nL 329.626351 159.433147 \r\nL 330.237599 160.331251 \r\nL 330.480923 160.510872 \r\nL 330.724469 160.959924 \r\nL 330.957072 161.139545 \r\nL 330.965325 161.229355 \r\nL 331.18552 161.498787 \r\nL 331.368138 161.858028 \r\nL 331.806784 162.037649 \r\nL 332.011955 162.21727 \r\nL 332.12847 162.576512 \r\nL 332.405772 162.845943 \r\nL 332.862427 163.025564 \r\nL 332.936283 163.294995 \r\nL 333.192385 163.474616 \r\nL 333.214253 163.744047 \r\nL 333.660077 164.193099 \r\nL 333.833235 164.642151 \r\nL 333.93532 164.911582 \r\nL 333.943221 165.001393 \r\nL 334.266929 165.270824 \r\nL 334.532546 165.809687 \r\nL 334.772346 165.989307 \r\nL 335.027521 166.348549 \r\nL 335.392274 166.707791 \r\nL 336.509458 168.593809 \r\nL 336.645893 169.132672 \r\nL 336.970732 169.581724 \r\nL 337.166129 169.851155 \r\nL 337.196509 170.030776 \r\nL 337.519327 170.210397 \r\nL 337.677147 170.569639 \r\nL 337.88037 170.749259 \r\nL 338.341236 171.737174 \r\nL 338.529641 172.096416 \r\nL 338.694805 172.365847 \r\nL 339.079274 172.545468 \r\nL 339.327123 172.99452 \r\nL 339.801213 173.533382 \r\nL 340.094763 173.982434 \r\nL 340.147864 174.341676 \r\nL 340.368504 174.700918 \r\nL 340.465025 174.880539 \r\nL 341.097213 175.688832 \r\nL 341.207384 176.137884 \r\nL 341.289327 176.497126 \r\nL 342.094599 177.035989 \r\nL 342.201877 177.215609 \r\nL 342.434109 177.39523 \r\nL 342.547081 177.574851 \r\nL 342.71807 177.754472 \r\nL 342.897497 178.113714 \r\nL 342.953937 178.383145 \r\nL 343.116431 178.562766 \r\nL 343.121513 178.652576 \r\nL 343.505908 179.011818 \r\nL 343.843359 179.55068 \r\nL 344.105879 179.909922 \r\nL 344.338983 180.089543 \r\nL 344.409277 180.358974 \r\nL 344.531653 180.628405 \r\nL 344.804838 180.808026 \r\nL 344.989347 181.167268 \r\nL 345.256485 181.436699 \r\nL 345.451252 181.70613 \r\nL 345.550443 182.065372 \r\nL 346.056268 182.334803 \r\nL 346.294843 182.604234 \r\nL 346.466796 183.143097 \r\nL 346.716704 183.412528 \r\nL 346.919074 183.77177 \r\nL 349.501429 189.519637 \r\nL 349.736535 189.699257 \r\nL 349.860247 190.058499 \r\nL 350.302342 190.597362 \r\nL 350.519996 190.776982 \r\nL 351.064288 192.213949 \r\nL 351.232902 192.39357 \r\nL 351.407211 192.842622 \r\nL 351.645749 193.022243 \r\nL 351.875885 193.291674 \r\nL 352.56544 194.369399 \r\nL 352.644044 194.63883 \r\nL 352.828628 194.818451 \r\nL 352.998541 194.998072 \r\nL 353.646865 196.075797 \r\nL 353.888816 196.345228 \r\nL 354.666045 197.961816 \r\nL 354.679417 198.051626 \r\nL 354.900298 198.321057 \r\nL 355.558304 199.488593 \r\nL 355.805374 199.668214 \r\nL 355.919385 200.027455 \r\nL 355.951954 200.207076 \r\nL 356.200026 200.386697 \r\nL 356.315261 200.656128 \r\nL 356.5806 201.464422 \r\nL 357.474529 203.709682 \r\nL 357.778465 204.697597 \r\nL 358.201179 205.32627 \r\nL 358.203219 205.505891 \r\nL 358.407908 205.685512 \r\nL 358.528151 205.954943 \r\nL 358.673358 206.583616 \r\nL 358.985789 207.48172 \r\nL 359.020566 207.661341 \r\nL 359.331884 207.930772 \r\nL 359.451255 208.200203 \r\nL 359.619981 208.379824 \r\nL 359.736421 208.739066 \r\nL 360.352343 209.367739 \r\nL 360.635636 210.086222 \r\nL 360.728596 210.355653 \r\nL 361.034368 210.535274 \r\nL 361.102381 210.894516 \r\nL 361.468971 211.074137 \r\nL 361.597078 211.343568 \r\nL 362.082927 212.241672 \r\nL 362.487538 212.421293 \r\nL 362.662014 212.780534 \r\nL 362.798986 213.049966 \r\nL 362.888496 213.319397 \r\nL 362.955879 213.588828 \r\nL 363.238782 214.03788 \r\nL 363.351365 214.666553 \r\nL 363.363068 214.756364 \r\nL 363.363068 214.756364 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 43.78125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 224.64 \r\nL 378.58125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 378.58125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 7.2 \r\nL 378.58125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 50.78125 219.64 \r\nL 127.790625 219.64 \r\nQ 129.790625 219.64 129.790625 217.64 \r\nL 129.790625 189.28375 \r\nQ 129.790625 187.28375 127.790625 187.28375 \r\nL 50.78125 187.28375 \r\nQ 48.78125 187.28375 48.78125 189.28375 \r\nL 48.78125 217.64 \r\nQ 48.78125 219.64 50.78125 219.64 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\">\r\n     <path d=\"M 52.78125 195.382188 \r\nL 72.78125 195.382188 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\"/>\r\n    <g id=\"text_15\">\r\n     <!-- Precision -->\r\n     <defs>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n     </defs>\r\n     <g transform=\"translate(80.78125 198.882188)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"60.287109\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"101.369141\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"162.892578\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"217.873047\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"245.65625\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"297.755859\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"325.539062\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"386.720703\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 52.78125 210.060313 \r\nL 72.78125 210.060313 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_16\">\r\n     <!-- Recall -->\r\n     <defs>\r\n      <path d=\"M 44.390625 34.1875 \r\nQ 47.5625 33.109375 50.5625 29.59375 \r\nQ 53.5625 26.078125 56.59375 19.921875 \r\nL 66.609375 0 \r\nL 56 0 \r\nL 46.6875 18.703125 \r\nQ 43.0625 26.03125 39.671875 28.421875 \r\nQ 36.28125 30.8125 30.421875 30.8125 \r\nL 19.671875 30.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nL 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.578125 72.90625 50.734375 67.671875 \r\nQ 56.890625 62.453125 56.890625 51.90625 \r\nQ 56.890625 45.015625 53.6875 40.46875 \r\nQ 50.484375 35.9375 44.390625 34.1875 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 38.921875 \r\nL 32.078125 38.921875 \r\nQ 39.203125 38.921875 42.84375 42.21875 \r\nQ 46.484375 45.515625 46.484375 51.90625 \r\nQ 46.484375 58.296875 42.84375 61.546875 \r\nQ 39.203125 64.796875 32.078125 64.796875 \r\nz\r\n\" id=\"DejaVuSans-82\"/>\r\n     </defs>\r\n     <g transform=\"translate(80.78125 213.560313)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"69.419922\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"130.943359\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"185.923828\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"247.203125\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"274.986328\" xlink:href=\"#DejaVuSans-108\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p6e7da71814\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "#Threshold-PR curve\n",
    "train_loss, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader)\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_true.ravel(), y_prob.ravel())\n",
    "plt.plot(thresholds, precisions[:-1], \"r--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"b-\", label=\"Recall\")\n",
    "plt.ylabel(\"Performance\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the best threshold\n",
    "def find_best_threshold(y_true, y_prob):\n",
    "    \"\"\"Find the best threshold for maximum F1.\"\"\"\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    f1s = (2 * precisions * recalls) / (precisions + recalls)\n",
    "    return thresholds[np.argmax(f1s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.25237343"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "# Best threshold for f1\n",
    "threshold = find_best_threshold(y_true.ravel(), y_prob.ravel())\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine predictions using threshold\n",
    "test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
    "y_pred = np.array([np.where(prob >= threshold, 1, 0) for prob in y_prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"precision\": 0.8230372567294096,\n  \"recall\": 0.4863642979899138,\n  \"f1\": 0.580268181774235,\n  \"num_samples\": 480.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "performance = get_performance(\n",
    "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "print (json.dumps(performance['overall'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='tag', index=3, options=('interpretability', 'segmentation', 'produ…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a15453e6944f4d95a1301f0d09d16383"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(tag=list(sorted_tags_by_f1.keys()))\n",
    "def display_tag_analysis(tag='transformers'):\n",
    "    # Performance\n",
    "    print (json.dumps(performance[\"class\"][tag], indent=2))\n",
    "    \n",
    "    # TP, FP, FN samples\n",
    "    index = label_encoder.class_to_index[tag]\n",
    "    tp, fp, fn = [], [], []\n",
    "    for i in range(len(y_test)):\n",
    "        true = y_test[i][index]\n",
    "        pred = y_pred[i][index]\n",
    "        if true and pred:\n",
    "            tp.append(i)\n",
    "        elif not true and pred:\n",
    "            fp.append(i)\n",
    "        elif true and not pred:\n",
    "            fn.append(i)\n",
    "            \n",
    "    # Samples\n",
    "    num_samples = 3\n",
    "    if len(tp): \n",
    "        print (\"\\n=== True positives ===\\n\")\n",
    "        for i in tp[:num_samples]:        \n",
    "            print (f\"  {X_test_raw[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")\n",
    "    if len(fp): \n",
    "        print (\"=== False positives ===\\n\")\n",
    "        for i in fp[:num_samples]:        \n",
    "            print (f\"  {X_test_raw[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")\n",
    "    if len(fn): \n",
    "        print (\"=== False negatives ===\\n\")\n",
    "        for i in fn[:num_samples]:        \n",
    "            print (f\"  {X_test_raw[i]}\") \n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts\n",
    "dir = Path(\"cnn\")\n",
    "dir.mkdir(parents=True, exist_ok=True)\n",
    "tokenizer.save(fp=Path(dir, 'tokenzier.json'))\n",
    "label_encoder.save(fp=Path(dir, 'label_encoder.json'))\n",
    "torch.save(best_model.state_dict(), Path(dir, 'model.pt'))\n",
    "with open(Path(dir, 'performance.json'), \"w\") as fp:\n",
    "    json.dump(performance, indent=2, sort_keys=False, fp=fp)"
   ]
  },
  {
   "source": [
    "### Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embeddings): Embedding(136, 128, padding_idx=0)\n",
       "  (conv): ModuleList(\n",
       "    (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "    (1): Conv1d(128, 128, kernel_size=(2,), stride=(1,))\n",
       "    (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
       "    (3): Conv1d(128, 128, kernel_size=(4,), stride=(1,))\n",
       "    (4): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
       "    (5): Conv1d(128, 128, kernel_size=(6,), stride=(1,))\n",
       "    (6): Conv1d(128, 128, kernel_size=(7,), stride=(1,))\n",
       "    (7): Conv1d(128, 128, kernel_size=(8,), stride=(1,))\n",
       "    (8): Conv1d(128, 128, kernel_size=(9,), stride=(1,))\n",
       "    (9): Conv1d(128, 128, kernel_size=(10,), stride=(1,))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=35, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "# Load artifacts\n",
    "device = torch.device(\"cpu\")\n",
    "tokenizer = Tokenizer.load(fp=Path(dir, 'tokenzier.json'))\n",
    "label_encoder = LabelEncoder.load(fp=Path(dir, 'label_encoder.json'))\n",
    "model = CNN(\n",
    "    embedding_dim=embedding_dim, vocab_size=vocab_size,\n",
    "    num_filters=num_filters, filter_sizes=filter_sizes,\n",
    "    hidden_dim=hidden_dim, dropout_p=dropout_p,\n",
    "    num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(Path(dir, 'model.pt'), map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "text = \"Transfer learning with BERT for self-supervised learning\"\n",
    "X = np.array(tokenizer.texts_to_sequences([preprocess(text)]))\n",
    "y_filler = label_encoder.encode([np.array([label_encoder.classes[0]]*len(X))])\n",
    "dataset = CNNTextDataset(\n",
    "    X=X, y=y_filler, max_filter_size=max(filter_sizes))\n",
    "dataloader = dataset.create_dataloader(\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['self-supervised-learning', 'transfer-learning']]"
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "# Inference\n",
    "y_prob = trainer.predict_step(dataloader)\n",
    "y_pred = np.array([np.where(prob >= threshold, 1, 0) for prob in y_prob])\n",
    "label_encoder.decode(y_pred)"
   ]
  },
  {
   "source": [
    "limitations: \n",
    "representation: embeddings are not contextual."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# RNN w/Embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data splits\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)\n",
    "X_test_raw = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "cuda = True\n",
    "device = torch.device('cuda' if (\n",
    "    torch.cuda.is_available() and cuda) else 'cpu')\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "if device.type == 'cuda':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X tokenizer:\n  <Tokenizer(num_tokens=136)>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "char_level = True\n",
    "tokenizer = Tokenizer(char_level=char_level)\n",
    "tokenizer.fit_on_texts(texts=X_train)\n",
    "vocab_size = len(tokenizer)\n",
    "print (\"X tokenizer:\\n\"\n",
    "    f\"  {tokenizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " ' ': 2,\n",
       " 'e': 3,\n",
       " 'n': 4,\n",
       " 'i': 5,\n",
       " 't': 6,\n",
       " 'a': 7,\n",
       " 'o': 8,\n",
       " 'r': 9,\n",
       " 's': 10,\n",
       " 'l': 11,\n",
       " 'c': 12,\n",
       " 'd': 13,\n",
       " 'g': 14,\n",
       " 'h': 15,\n",
       " 'm': 16,\n",
       " 'u': 17,\n",
       " 'p': 18,\n",
       " 'f': 19,\n",
       " 'y': 20,\n",
       " 'w': 21,\n",
       " 'b': 22,\n",
       " 'T': 23,\n",
       " 'v': 24,\n",
       " '.': 25,\n",
       " 'A': 26,\n",
       " '-': 27,\n",
       " 'L': 28,\n",
       " 'k': 29,\n",
       " 'P': 30,\n",
       " 'S': 31,\n",
       " 'N': 32,\n",
       " 'C': 33,\n",
       " 'M': 34,\n",
       " 'D': 35,\n",
       " 'I': 36,\n",
       " ',': 37,\n",
       " 'x': 38,\n",
       " 'R': 39,\n",
       " 'F': 40,\n",
       " 'G': 41,\n",
       " 'E': 42,\n",
       " 'B': 43,\n",
       " ':': 44,\n",
       " '2': 45,\n",
       " 'O': 46,\n",
       " 'z': 47,\n",
       " 'H': 48,\n",
       " 'W': 49,\n",
       " '0': 50,\n",
       " 'V': 51,\n",
       " '(': 52,\n",
       " ')': 53,\n",
       " 'j': 54,\n",
       " 'U': 55,\n",
       " 'K': 56,\n",
       " 'q': 57,\n",
       " '1': 58,\n",
       " '\"': 59,\n",
       " 'Q': 60,\n",
       " '\\r': 61,\n",
       " '\\n': 62,\n",
       " '/': 63,\n",
       " '&': 64,\n",
       " 'Y': 65,\n",
       " '3': 66,\n",
       " '+': 67,\n",
       " \"'\": 68,\n",
       " '?': 69,\n",
       " 'J': 70,\n",
       " '5': 71,\n",
       " '9': 72,\n",
       " 'X': 73,\n",
       " '8': 74,\n",
       " '6': 75,\n",
       " '4': 76,\n",
       " '’': 77,\n",
       " '!': 78,\n",
       " 'Z': 79,\n",
       " '—': 80,\n",
       " '7': 81,\n",
       " '“': 82,\n",
       " '”': 83,\n",
       " '|': 84,\n",
       " '🤗': 85,\n",
       " '️': 86,\n",
       " '%': 87,\n",
       " '=': 88,\n",
       " '–': 89,\n",
       " '💻': 90,\n",
       " '❤': 91,\n",
       " '_': 92,\n",
       " '🎨': 93,\n",
       " '>': 94,\n",
       " '🌊': 95,\n",
       " '^': 96,\n",
       " '🚗': 97,\n",
       " '🕶': 98,\n",
       " '<': 99,\n",
       " '\\u2060': 100,\n",
       " '⚡': 101,\n",
       " ';': 102,\n",
       " '@': 103,\n",
       " '#': 104,\n",
       " '🏥': 105,\n",
       " '💤': 106,\n",
       " '✨': 107,\n",
       " '😎': 108,\n",
       " '🔍': 109,\n",
       " '[': 110,\n",
       " ']': 111,\n",
       " '🦠': 112,\n",
       " '🍼': 113,\n",
       " '\\u2009': 114,\n",
       " '基': 115,\n",
       " '于': 116,\n",
       " '📈': 117,\n",
       " '⭐': 118,\n",
       " '🔥': 119,\n",
       " '*': 120,\n",
       " '…': 121,\n",
       " '🎐': 122,\n",
       " 'ç': 123,\n",
       " '🦄': 124,\n",
       " '✏': 125,\n",
       " '✂': 126,\n",
       " '🏡': 127,\n",
       " '𝘢': 128,\n",
       " '𝘯': 129,\n",
       " '𝘺': 130,\n",
       " '🐳': 131,\n",
       " '🤖': 132,\n",
       " '💬': 133,\n",
       " '🎥': 134,\n",
       " '👀': 135}"
      ]
     },
     "metadata": {},
     "execution_count": 353
    }
   ],
   "source": [
    "tokenizer.token_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text to indices:\n  (preprocessed) → Albumentations Fast image augmentation library and easy to use wrapper around other libraries.\n  (tokenized) → [26 11 22 17 16  3  4  6  7  6  5  8  4 10  2 40  7 10  6  2  5 16  7 14\n  3  2  7 17 14 16  3  4  6  7  6  5  8  4  2 11  5 22  9  7  9 20  2  7\n  4 13  2  3  7 10 20  2  6  8  2 17 10  3  2 21  9  7 18 18  3  9  2  7\n  9  8 17  4 13  2  8  6 15  3  9  2 11  5 22  9  7  9  5  3 10 25]\n"
     ]
    }
   ],
   "source": [
    "# Convert texts to sequences of indices\n",
    "X_train = np.array(tokenizer.texts_to_sequences(X_train))\n",
    "X_val = np.array(tokenizer.texts_to_sequences(X_val))\n",
    "X_test = np.array(tokenizer.texts_to_sequences(X_test))\n",
    "preprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\n",
    "print (\"Text to indices:\\n\"\n",
    "    f\"  (preprocessed) → {preprocessed_text}\\n\"\n",
    "    f\"  (tokenized) → {X_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "class counts:\n  [120  41 388 106  41  75  34  73  51  78  64  51  55  93  51 429  33  69\n  30  51 258  32  49  59  57  60  48  40 213  40  34  46 196  39  39]\nclass weights:\n  {0: 0.008333333333333333, 1: 0.024390243902439025, 2: 0.002577319587628866, 3: 0.009433962264150943, 4: 0.024390243902439025, 5: 0.013333333333333334, 6: 0.029411764705882353, 7: 0.0136986301369863, 8: 0.0196078431372549, 9: 0.01282051282051282, 10: 0.015625, 11: 0.0196078431372549, 12: 0.01818181818181818, 13: 0.010752688172043012, 14: 0.0196078431372549, 15: 0.002331002331002331, 16: 0.030303030303030304, 17: 0.014492753623188406, 18: 0.03333333333333333, 19: 0.0196078431372549, 20: 0.003875968992248062, 21: 0.03125, 22: 0.02040816326530612, 23: 0.01694915254237288, 24: 0.017543859649122806, 25: 0.016666666666666666, 26: 0.020833333333333332, 27: 0.025, 28: 0.004694835680751174, 29: 0.025, 30: 0.029411764705882353, 31: 0.021739130434782608, 32: 0.00510204081632653, 33: 0.02564102564102564, 34: 0.02564102564102564}\n"
     ]
    }
   ],
   "source": [
    "# Class weights\n",
    "counts = np.bincount([label_encoder.class_to_index[class_] for class_ in all_tags])\n",
    "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
    "print (\"class counts:\\n\"\n",
    "    f\"  {counts}\\n\"\n",
    "    \"class weights:\\n\"\n",
    "    f\"  {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Dataset(N={len(self)})>\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        return [X, len(X), y]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Processing on a batch.\"\"\"\n",
    "        # Get inputs\n",
    "        batch = np.array(batch, dtype=object)\n",
    "        X = batch[:, 0]\n",
    "        seq_lens = batch[:, 1]\n",
    "        y = np.stack(batch[:, 2], axis=0)\n",
    "\n",
    "        # Pad inputs\n",
    "        X = pad_sequences(sequences=X)\n",
    "\n",
    "        # Cast\n",
    "        X = torch.LongTensor(X.astype(np.int32))\n",
    "        seq_lens = torch.LongTensor(seq_lens.astype(np.int32))\n",
    "        y = torch.FloatTensor(y.astype(np.int32))\n",
    "\n",
    "        return X, seq_lens, y\n",
    "\n",
    "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data splits:\n  Train dataset:<Dataset(N=1000)>\n  Val dataset: <Dataset(N=227)>\n  Test dataset: <Dataset(N=217)>\nSample point:\n  X: [26 11 22 17 16  3  4  6  7  6  5  8  4 10  2 40  7 10  6  2  5 16  7 14\n  3  2  7 17 14 16  3  4  6  7  6  5  8  4  2 11  5 22  9  7  9 20  2  7\n  4 13  2  3  7 10 20  2  6  8  2 17 10  3  2 21  9  7 18 18  3  9  2  7\n  9  8 17  4 13  2  8  6 15  3  9  2 11  5 22  9  7  9  5  3 10 25]\n  seq_len: 94\n  y: [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "batch_size = 128\n",
    "train_dataset = RNNTextDataset(\n",
    "    X=X_train, y=y_train)\n",
    "val_dataset = RNNTextDataset(\n",
    "    X=X_val, y=y_val)\n",
    "test_dataset = RNNTextDataset(\n",
    "    X=X_test, y=y_test)\n",
    "print (\"Data splits:\\n\"\n",
    "    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n",
    "    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n",
    "    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  X: {train_dataset[0][0]}\\n\"\n",
    "    f\"  seq_len: {train_dataset[0][1]}\\n\"\n",
    "    f\"  y: {train_dataset[0][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 226])\nSample batch:\n  X: [128, 226]\n  seq_lens: [128]\n  y: [128, 35]\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "train_dataloader = train_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "val_dataloader = val_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "test_dataloader = test_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "batch_X, batch_seq_lens, batch_y = next(iter(train_dataloader))\n",
    "print (batch_X.shape)\n",
    "print (\"Sample batch:\\n\"\n",
    "    f\"  X: {list(batch_X.size())}\\n\"\n",
    "    f\"  seq_lens: {list(batch_seq_lens.size())}\\n\"\n",
    "    f\"  y: {list(batch_y.size())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "embedding_dim = 128\n",
    "rnn_hidden_dim = 128\n",
    "hidden_dim = 128\n",
    "dropout_p =0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_last_relevant_hidden(hiddens, seq_lens):\n",
    "    \"\"\"Extract and collect the last relevant \n",
    "    hidden state based on the sequence length.\"\"\"\n",
    "    seq_lens = seq_lens.long().detach().cpu().numpy() - 1\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(seq_lens):\n",
    "        out.append(hiddens[batch_index, column_index])\n",
    "    return torch.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, rnn_hidden_dim,\n",
    "                 hidden_dim, dropout_p, num_classes, padding_idx=0):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                       num_embeddings=vocab_size,\n",
    "                                       padding_idx=padding_idx)\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.GRU(embedding_dim, rnn_hidden_dim, \n",
    "                          batch_first=True, bidirectional=True)\n",
    "     \n",
    "        # FC weights\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_dim*2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Inputs\n",
    "        x_in, seq_lens = inputs\n",
    "\n",
    "        # Embed\n",
    "        x_in = self.embeddings(x_in)\n",
    "            \n",
    "        # Rnn outputs\n",
    "        out, h_n = self.rnn(x_in)\n",
    "        z = gather_last_relevant_hidden(hiddens=out, seq_lens=seq_lens)\n",
    "\n",
    "        # FC layers\n",
    "        z = self.fc1(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc2(z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method Module.named_parameters of RNN(\n  (embeddings): Embedding(136, 128, padding_idx=0)\n  (rnn): GRU(128, 128, batch_first=True, bidirectional=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=256, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=35, bias=True)\n)>\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = RNN(\n",
    "    embedding_dim=embedding_dim, vocab_size=vocab_size,\n",
    "    rnn_hidden_dim=rnn_hidden_dim, hidden_dim=hidden_dim, \n",
    "    dropout_p=dropout_p, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "print (model.named_parameters)"
   ]
  },
  {
   "source": [
    "Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "lr = 2e-3\n",
    "num_epochs = 200\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "class_weights_tensor = torch.Tensor(np.array(list(class_weights.values())))\n",
    "loss_fn = nn.BCEWithLogitsLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer & scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer module\n",
    "trainer = Trainer(\n",
    "    model=model, device=device, loss_fn=loss_fn, \n",
    "    optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 | train_loss: 0.00776, val_loss: 0.00330, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.00406, val_loss: 0.00313, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 3 | train_loss: 0.00344, val_loss: 0.00276, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 4 | train_loss: 0.00311, val_loss: 0.00272, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 5 | train_loss: 0.00302, val_loss: 0.00267, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 6 | train_loss: 0.00296, val_loss: 0.00265, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 7 | train_loss: 0.00291, val_loss: 0.00262, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 8 | train_loss: 0.00287, val_loss: 0.00261, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 9 | train_loss: 0.00285, val_loss: 0.00260, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 10 | train_loss: 0.00282, val_loss: 0.00259, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 11 | train_loss: 0.00279, val_loss: 0.00258, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 12 | train_loss: 0.00279, val_loss: 0.00257, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 13 | train_loss: 0.00274, val_loss: 0.00256, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 14 | train_loss: 0.00271, val_loss: 0.00255, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 15 | train_loss: 0.00270, val_loss: 0.00255, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 16 | train_loss: 0.00267, val_loss: 0.00254, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 17 | train_loss: 0.00264, val_loss: 0.00254, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 18 | train_loss: 0.00263, val_loss: 0.00255, lr: 2.00E-03, _patience: 9\n",
      "Epoch: 19 | train_loss: 0.00259, val_loss: 0.00254, lr: 2.00E-03, _patience: 8\n",
      "Epoch: 20 | train_loss: 0.00255, val_loss: 0.00254, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 21 | train_loss: 0.00255, val_loss: 0.00253, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 22 | train_loss: 0.00253, val_loss: 0.00252, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 23 | train_loss: 0.00247, val_loss: 0.00253, lr: 2.00E-03, _patience: 9\n",
      "Epoch: 24 | train_loss: 0.00242, val_loss: 0.00252, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 25 | train_loss: 0.00239, val_loss: 0.00251, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 26 | train_loss: 0.00236, val_loss: 0.00250, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 27 | train_loss: 0.00232, val_loss: 0.00250, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 28 | train_loss: 0.00226, val_loss: 0.00252, lr: 2.00E-03, _patience: 9\n",
      "Epoch: 29 | train_loss: 0.00224, val_loss: 0.00252, lr: 2.00E-03, _patience: 8\n",
      "Epoch: 30 | train_loss: 0.00217, val_loss: 0.00251, lr: 2.00E-03, _patience: 7\n",
      "Epoch: 31 | train_loss: 0.00211, val_loss: 0.00250, lr: 2.00E-03, _patience: 6\n",
      "Epoch: 32 | train_loss: 0.00206, val_loss: 0.00249, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 33 | train_loss: 0.00201, val_loss: 0.00247, lr: 2.00E-03, _patience: 10\n",
      "Epoch: 34 | train_loss: 0.00196, val_loss: 0.00249, lr: 2.00E-03, _patience: 9\n",
      "Epoch: 35 | train_loss: 0.00189, val_loss: 0.00250, lr: 2.00E-03, _patience: 8\n",
      "Epoch: 36 | train_loss: 0.00183, val_loss: 0.00250, lr: 2.00E-03, _patience: 7\n",
      "Epoch: 37 | train_loss: 0.00178, val_loss: 0.00256, lr: 2.00E-03, _patience: 6\n",
      "Epoch: 38 | train_loss: 0.00173, val_loss: 0.00256, lr: 2.00E-03, _patience: 5\n",
      "Epoch: 39 | train_loss: 0.00167, val_loss: 0.00255, lr: 2.00E-04, _patience: 4\n",
      "Epoch: 40 | train_loss: 0.00163, val_loss: 0.00257, lr: 2.00E-04, _patience: 3\n",
      "Epoch: 41 | train_loss: 0.00158, val_loss: 0.00256, lr: 2.00E-04, _patience: 2\n",
      "Epoch: 42 | train_loss: 0.00158, val_loss: 0.00255, lr: 2.00E-04, _patience: 1\n",
      "Stopping early!\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "best_model = trainer.train(\n",
    "    num_epochs, patience, train_dataloader, val_dataloader)"
   ]
  },
  {
   "source": [
    "Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x294ba10e808>"
      ]
     },
     "metadata": {},
     "execution_count": 368
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 385.78125 262.19625\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 385.78125 262.19625 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 378.58125 224.64 \r\nL 378.58125 7.2 \r\nL 43.78125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m28e513c5ea\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"57.250751\" xlink:href=\"#m28e513c5ea\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(49.299188 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"118.480594\" xlink:href=\"#m28e513c5ea\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0.2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(110.529031 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.710437\" xlink:href=\"#m28e513c5ea\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 0.4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(171.758874 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"240.94028\" xlink:href=\"#m28e513c5ea\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0.6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(232.988717 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"302.170123\" xlink:href=\"#m28e513c5ea\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0.8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(294.21856 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.399965\" xlink:href=\"#m28e513c5ea\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1.0 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(355.448403 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Threshold -->\r\n     <defs>\r\n      <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n     </defs>\r\n     <g transform=\"translate(186.432031 252.916563)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-84\"/>\r\n      <use x=\"61.083984\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"124.462891\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"165.544922\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"227.068359\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"279.167969\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"342.546875\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"403.728516\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"431.511719\" xlink:href=\"#DejaVuSans-100\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m3fb67e2281\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m3fb67e2281\" y=\"214.846174\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(20.878125 218.645393)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m3fb67e2281\" y=\"175.293667\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(20.878125 179.092885)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m3fb67e2281\" y=\"135.741159\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(20.878125 139.540378)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m3fb67e2281\" y=\"96.188651\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(20.878125 99.98787)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m3fb67e2281\" y=\"56.636144\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(20.878125 60.435363)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m3fb67e2281\" y=\"17.083636\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 20.882855)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- Performance -->\r\n     <defs>\r\n      <path d=\"M 19.671875 64.796875 \r\nL 19.671875 37.40625 \r\nL 32.078125 37.40625 \r\nQ 38.96875 37.40625 42.71875 40.96875 \r\nQ 46.484375 44.53125 46.484375 51.125 \r\nQ 46.484375 57.671875 42.71875 61.234375 \r\nQ 38.96875 64.796875 32.078125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.34375 72.90625 50.609375 67.359375 \r\nQ 56.890625 61.8125 56.890625 51.125 \r\nQ 56.890625 40.328125 50.609375 34.8125 \r\nQ 44.34375 29.296875 32.078125 29.296875 \r\nL 19.671875 29.296875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-80\"/>\r\n      <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n      <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 147.867656)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"60.255859\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"121.779297\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"162.892578\" xlink:href=\"#DejaVuSans-102\"/>\r\n      <use x=\"198.097656\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"259.279297\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"300.376953\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"397.789062\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"459.068359\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"522.447266\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"577.427734\" xlink:href=\"#DejaVuSans-101\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p6e270b4024)\" d=\"M 58.999432 193.640822 \r\nL 58.999541 193.64942 \r\nL 59.106479 193.331972 \r\nL 59.275281 192.854788 \r\nL 59.32919 192.680004 \r\nL 59.425542 192.403299 \r\nL 59.681643 191.695798 \r\nL 62.953029 184.307152 \r\nL 63.07143 184.057726 \r\nL 63.212908 183.861387 \r\nL 63.347785 183.603137 \r\nL 63.897589 182.578405 \r\nL 64.373552 181.694875 \r\nL 64.499363 181.486044 \r\nL 64.7556 181.036215 \r\nL 64.943909 180.658375 \r\nL 65.532874 179.743283 \r\nL 65.926767 179.073202 \r\nL 65.930466 179.083784 \r\nL 71.074193 171.178641 \r\nL 71.199245 170.970942 \r\nL 71.313517 170.765812 \r\nL 71.389248 170.641175 \r\nL 72.614777 169.10219 \r\nL 72.754194 168.887741 \r\nL 72.761469 168.899558 \r\nL 73.09381 168.466257 \r\nL 73.389052 167.946325 \r\nL 73.478955 167.795539 \r\nL 73.590991 167.617598 \r\nL 73.592399 167.634958 \r\nL 73.955951 167.035451 \r\nL 74.836581 165.90518 \r\nL 74.963307 165.787136 \r\nL 75.422737 165.163421 \r\nL 75.582793 164.980061 \r\nL 76.981879 163.147541 \r\nL 77.128602 162.951403 \r\nL 77.133107 162.970261 \r\nL 77.858297 162.153125 \r\nL 78.080725 161.838746 \r\nL 78.639985 161.242471 \r\nL 78.77213 161.01381 \r\nL 78.931128 160.822312 \r\nL 79.202114 160.630644 \r\nL 81.21074 158.556371 \r\nL 81.214038 158.568771 \r\nL 81.322791 158.441562 \r\nL 81.505022 158.263954 \r\nL 81.626653 158.068343 \r\nL 81.633109 158.089082 \r\nL 82.557016 156.867358 \r\nL 82.562156 156.888518 \r\nL 82.565178 156.909685 \r\nL 82.652184 156.737279 \r\nL 83.279575 156.090637 \r\nL 83.318472 156.149252 \r\nL 83.708024 155.535947 \r\nL 83.89916 155.328122 \r\nL 84.132847 155.06476 \r\nL 84.151866 155.086544 \r\nL 84.231931 155.010971 \r\nL 84.776109 154.466225 \r\nL 84.958075 154.294303 \r\nL 84.961686 154.316348 \r\nL 85.050674 154.21894 \r\nL 85.298882 153.890212 \r\nL 85.463043 153.718987 \r\nL 85.470184 153.741218 \r\nL 85.47624 153.763457 \r\nL 85.549866 153.63396 \r\nL 85.744008 153.385569 \r\nL 85.876513 153.114569 \r\nL 85.751656 153.397834 \r\nL 85.883294 153.116639 \r\nL 85.971071 153.080049 \r\nL 85.983792 153.059661 \r\nL 86.100447 152.871179 \r\nL 86.327695 152.585765 \r\nL 86.46772 152.379166 \r\nL 86.478049 152.391361 \r\nL 86.52331 152.429757 \r\nL 86.58216 152.345906 \r\nL 87.010904 151.94768 \r\nL 87.179633 151.769943 \r\nL 87.672972 151.220005 \r\nL 87.678121 151.243124 \r\nL 87.773778 151.167456 \r\nL 87.986045 150.928827 \r\nL 88.197614 150.640698 \r\nL 88.356656 150.473688 \r\nL 88.452551 150.396155 \r\nL 88.626524 150.193708 \r\nL 89.494408 149.186589 \r\nL 89.494983 149.210391 \r\nL 89.591462 149.115645 \r\nL 90.946546 148.076879 \r\nL 91.097711 147.887205 \r\nL 91.120562 147.899178 \r\nL 91.122818 147.923717 \r\nL 91.196845 147.810545 \r\nL 91.220636 147.808657 \r\nL 92.166791 146.815558 \r\nL 92.276981 146.632539 \r\nL 92.611873 146.411992 \r\nL 92.618917 146.423782 \r\nL 92.62131 146.448829 \r\nL 92.718562 146.314602 \r\nL 92.974046 146.133053 \r\nL 93.003508 146.158249 \r\nL 93.029281 146.064247 \r\nL 93.046947 146.050798 \r\nL 93.175966 145.833065 \r\nL 93.29672 145.720752 \r\nL 93.501806 145.482578 \r\nL 93.539593 145.556824 \r\nL 93.623978 145.419318 \r\nL 93.928987 145.063967 \r\nL 94.11703 144.836802 \r\nL 94.436932 144.285659 \r\nL 94.590255 144.078207 \r\nL 94.703363 143.875267 \r\nL 94.829161 143.767012 \r\nL 94.859989 143.804473 \r\nL 95.878147 142.587587 \r\nL 96.048455 142.349996 \r\nL 95.908288 142.610058 \r\nL 96.054751 142.37653 \r\nL 96.540912 141.872476 \r\nL 96.763372 141.596055 \r\nL 97.381981 141.036747 \r\nL 97.493216 140.902067 \r\nL 97.720252 140.799694 \r\nL 97.720963 140.82698 \r\nL 97.82499 140.646964 \r\nL 97.958811 140.482552 \r\nL 98.020106 140.53168 \r\nL 98.914121 139.356414 \r\nL 99.046382 139.413763 \r\nL 99.063348 139.396592 \r\nL 99.183629 139.245697 \r\nL 99.364398 139.181692 \r\nL 99.210974 139.312185 \r\nL 99.365087 139.209721 \r\nL 99.476832 139.018237 \r\nL 99.61683 138.811779 \r\nL 99.658778 138.87865 \r\nL 99.711802 138.81889 \r\nL 100.776303 137.761061 \r\nL 100.91241 137.716433 \r\nL 101.022326 137.514137 \r\nL 101.169947 137.555321 \r\nL 101.196238 137.518375 \r\nL 101.275014 137.485565 \r\nL 101.240872 137.578168 \r\nL 101.30067 137.506218 \r\nL 101.309744 137.535117 \r\nL 101.39066 137.433957 \r\nL 101.408634 137.454625 \r\nL 101.484646 137.380049 \r\nL 102.296309 136.84441 \r\nL 102.418533 136.739911 \r\nL 102.518345 136.567194 \r\nL 102.686399 136.248868 \r\nL 102.864645 136.368977 \r\nL 102.885224 136.33003 \r\nL 103.000186 136.125164 \r\nL 103.241186 135.721353 \r\nL 103.396211 135.512647 \r\nL 103.819978 134.98921 \r\nL 103.928795 134.775482 \r\nL 103.829617 134.99886 \r\nL 103.941892 134.835376 \r\nL 104.485707 134.34782 \r\nL 104.596837 134.264939 \r\nL 104.602695 134.29511 \r\nL 104.612325 134.325295 \r\nL 104.675148 134.242263 \r\nL 104.709819 134.272474 \r\nL 104.923958 134.073529 \r\nL 104.931536 134.103798 \r\nL 105.006535 134.12259 \r\nL 105.025531 134.080753 \r\nL 105.330289 133.636095 \r\nL 105.434904 133.548394 \r\nL 105.448641 133.578859 \r\nL 105.453964 133.609339 \r\nL 105.508101 133.545524 \r\nL 105.691739 133.37175 \r\nL 105.912238 132.91598 \r\nL 106.005476 132.912238 \r\nL 106.01126 132.868806 \r\nL 106.051862 132.886746 \r\nL 106.262534 132.46712 \r\nL 106.648231 132.139682 \r\nL 106.759251 132.037103 \r\nL 107.302975 131.651376 \r\nL 107.410031 131.523807 \r\nL 107.428745 131.563533 \r\nL 107.53029 131.381296 \r\nL 107.65782 131.221182 \r\nL 108.949873 130.408862 \r\nL 109.062617 130.098781 \r\nL 109.125951 130.066818 \r\nL 109.104875 130.138726 \r\nL 109.15586 130.098813 \r\nL 109.181028 130.130874 \r\nL 109.198067 129.986432 \r\nL 109.314899 129.793075 \r\nL 109.422954 129.7682 \r\nL 109.353854 129.833088 \r\nL 109.423064 129.800377 \r\nL 109.434514 129.832572 \r\nL 109.482096 129.710985 \r\nL 109.519527 129.7511 \r\nL 109.693653 129.571377 \r\nL 109.702823 129.579199 \r\nL 109.828861 129.316444 \r\nL 109.856265 129.413671 \r\nL 109.927906 129.314877 \r\nL 110.16502 128.982404 \r\nL 110.228258 129.030125 \r\nL 110.552377 128.942891 \r\nL 110.552564 128.975598 \r\nL 110.651764 128.857423 \r\nL 110.661486 128.890182 \r\nL 110.797926 128.713474 \r\nL 110.833609 128.720967 \r\nL 110.909658 128.467135 \r\nL 110.98712 128.499972 \r\nL 111.023662 128.398028 \r\nL 111.128459 128.27026 \r\nL 111.130649 128.277539 \r\nL 111.238572 128.233561 \r\nL 111.238754 128.207906 \r\nL 111.38487 127.905951 \r\nL 111.410581 127.913059 \r\nL 112.482646 126.657214 \r\nL 112.540123 126.69714 \r\nL 112.585574 126.670258 \r\nL 113.049659 125.981291 \r\nL 113.147299 125.999885 \r\nL 113.156072 125.972387 \r\nL 113.225998 126.01232 \r\nL 113.250546 125.929684 \r\nL 113.323921 125.969652 \r\nL 113.376133 125.89299 \r\nL 113.53035 125.661569 \r\nL 113.594273 125.701573 \r\nL 113.615992 125.617895 \r\nL 114.084508 125.0053 \r\nL 114.119274 125.039452 \r\nL 114.143786 124.982576 \r\nL 114.301097 124.794151 \r\nL 114.327989 124.834027 \r\nL 114.441752 124.667466 \r\nL 114.453576 124.701783 \r\nL 114.481162 124.644238 \r\nL 114.500514 124.655333 \r\nL 114.790492 124.364086 \r\nL 114.790957 124.334964 \r\nL 114.83952 124.438622 \r\nL 114.880285 124.414933 \r\nL 115.525586 123.705692 \r\nL 115.534277 123.775586 \r\nL 115.620457 123.656031 \r\nL 115.762476 123.746474 \r\nL 116.175924 123.232202 \r\nL 116.279225 123.307737 \r\nL 116.310603 123.221036 \r\nL 116.649973 122.831199 \r\nL 116.736801 122.623577 \r\nL 116.83653 122.628063 \r\nL 116.84877 122.596982 \r\nL 116.967003 122.418772 \r\nL 117.081846 122.42314 \r\nL 117.082147 122.391853 \r\nL 117.201689 122.01886 \r\nL 117.205946 122.054565 \r\nL 117.291766 122.062864 \r\nL 117.303339 122.031241 \r\nL 118.256989 121.111397 \r\nL 118.363666 121.151106 \r\nL 118.385454 121.085972 \r\nL 118.767497 120.468957 \r\nL 118.802766 120.50838 \r\nL 118.860069 120.409007 \r\nL 119.035682 120.209632 \r\nL 119.199681 120.009414 \r\nL 119.243408 119.875464 \r\nL 119.353798 119.878228 \r\nL 119.36102 119.811065 \r\nL 119.507747 119.645349 \r\nL 119.539069 119.681719 \r\nL 119.565994 119.546542 \r\nL 119.734619 119.216184 \r\nL 119.824768 119.085978 \r\nL 119.805398 119.223171 \r\nL 119.827049 119.122578 \r\nL 119.857459 119.195855 \r\nL 119.882696 119.058278 \r\nL 120.475543 118.200733 \r\nL 120.5176 118.130188 \r\nL 120.553302 118.203999 \r\nL 120.587987 118.133354 \r\nL 120.699062 118.099554 \r\nL 120.982375 117.963608 \r\nL 121.026576 118.002194 \r\nL 121.076187 117.96655 \r\nL 122.211992 116.783952 \r\nL 122.252516 116.821504 \r\nL 122.337173 116.859084 \r\nL 122.364905 116.785187 \r\nL 122.519798 116.6014 \r\nL 122.587416 116.377692 \r\nL 122.724704 116.115409 \r\nL 123.037026 115.622755 \r\nL 123.14821 115.736542 \r\nL 123.180715 115.660186 \r\nL 123.316794 115.429999 \r\nL 123.471189 115.429171 \r\nL 123.512973 115.390682 \r\nL 123.54268 115.466859 \r\nL 123.627346 115.619032 \r\nL 123.657227 115.502843 \r\nL 123.887913 115.188757 \r\nL 123.90465 115.227275 \r\nL 123.922342 115.188147 \r\nL 124.095136 115.030594 \r\nL 124.266753 114.752837 \r\nL 124.268368 114.830138 \r\nL 124.458502 114.510191 \r\nL 124.636593 114.106681 \r\nL 124.684626 114.144033 \r\nL 124.694019 114.103704 \r\nL 124.889938 113.899915 \r\nL 125.301268 113.527832 \r\nL 125.369849 113.281614 \r\nL 125.429519 113.437717 \r\nL 125.531936 113.272892 \r\nL 125.685693 113.098196 \r\nL 125.696828 113.137418 \r\nL 125.707221 113.176672 \r\nL 125.757457 113.093506 \r\nL 125.782744 113.13279 \r\nL 125.988877 112.961123 \r\nL 126.021623 113.039905 \r\nL 126.035063 113.079344 \r\nL 126.067708 112.953706 \r\nL 126.228144 112.862107 \r\nL 126.229001 112.901669 \r\nL 126.342618 112.687978 \r\nL 126.405518 112.594887 \r\nL 126.621998 112.418705 \r\nL 126.719095 112.244456 \r\nL 126.872692 112.195238 \r\nL 126.8773 112.2351 \r\nL 126.989032 112.268797 \r\nL 126.998667 112.225698 \r\nL 127.032311 112.262578 \r\nL 127.118546 112.176191 \r\nL 127.272139 111.952817 \r\nL 127.302973 111.99293 \r\nL 127.317248 112.033078 \r\nL 127.342795 111.858819 \r\nL 127.37841 111.815161 \r\nL 127.428441 111.895545 \r\nL 127.443241 111.935787 \r\nL 127.47965 111.848353 \r\nL 127.515083 111.844852 \r\nL 127.609976 111.62542 \r\nL 127.689391 111.401154 \r\nL 127.720052 111.441478 \r\nL 127.733807 111.477998 \r\nL 127.87266 111.122781 \r\nL 128.138737 111.235986 \r\nL 128.157952 111.191327 \r\nL 128.349094 110.96745 \r\nL 128.469216 110.697518 \r\nL 128.574616 110.64317 \r\nL 128.580095 110.683795 \r\nL 129.988866 109.255105 \r\nL 129.99033 109.296271 \r\nL 130.247297 109.154657 \r\nL 130.310686 109.053849 \r\nL 130.575313 108.987665 \r\nL 130.590623 109.028989 \r\nL 130.617908 109.111748 \r\nL 130.692692 109.016492 \r\nL 130.736296 108.921064 \r\nL 130.762377 109.045448 \r\nL 131.348605 108.497075 \r\nL 131.353769 108.448669 \r\nL 131.395853 108.531917 \r\nL 131.602898 108.282174 \r\nL 131.685908 108.087002 \r\nL 131.709608 108.128689 \r\nL 131.957661 107.918016 \r\nL 132.092732 107.952427 \r\nL 132.224322 107.945042 \r\nL 132.248706 108.028862 \r\nL 132.315767 107.930231 \r\nL 132.962505 107.168295 \r\nL 133.051464 107.244159 \r\nL 132.979366 107.160093 \r\nL 133.073895 107.193924 \r\nL 133.215284 107.118853 \r\nL 133.314945 107.195051 \r\nL 133.345734 107.14451 \r\nL 133.447275 107.034894 \r\nL 133.544011 107.068851 \r\nL 133.555745 107.018061 \r\nL 133.664552 106.754688 \r\nL 133.698161 106.839563 \r\nL 133.7316 106.685999 \r\nL 133.832734 106.634714 \r\nL 133.837762 106.719718 \r\nL 133.896894 106.847526 \r\nL 133.941127 106.744768 \r\nL 133.955251 106.693316 \r\nL 134.038188 106.778699 \r\nL 134.256469 106.502583 \r\nL 134.297399 106.545387 \r\nL 134.432981 106.47534 \r\nL 134.49403 106.518255 \r\nL 134.522661 106.414144 \r\nL 134.702476 106.0386 \r\nL 134.742886 106.081553 \r\nL 134.811005 106.11503 \r\nL 134.817848 106.062427 \r\nL 135.207021 105.332511 \r\nL 135.211145 105.375664 \r\nL 135.220461 105.41886 \r\nL 135.256081 105.311683 \r\nL 135.268435 105.354911 \r\nL 135.970706 104.585712 \r\nL 136.067255 104.409909 \r\nL 136.302772 104.4304 \r\nL 136.54953 103.710177 \r\nL 136.570816 103.753664 \r\nL 136.782957 103.392716 \r\nL 136.89343 103.267311 \r\nL 136.936668 103.310904 \r\nL 137.109932 103.159053 \r\nL 137.245022 103.189807 \r\nL 137.25004 103.132985 \r\nL 137.601677 102.375069 \r\nL 137.644715 102.418831 \r\nL 137.799183 102.187618 \r\nL 137.902722 102.042994 \r\nL 138.183421 101.66326 \r\nL 138.345106 101.428075 \r\nL 138.538808 101.19191 \r\nL 139.094603 100.66433 \r\nL 139.32907 100.776575 \r\nL 139.497499 100.41291 \r\nL 139.936735 99.484694 \r\nL 140.111112 99.048898 \r\nL 140.230015 98.824057 \r\nL 140.348025 98.723793 \r\nL 140.441892 98.857047 \r\nL 140.463543 98.730697 \r\nL 140.503004 98.775191 \r\nL 140.544464 98.711903 \r\nL 141.096107 98.04609 \r\nL 141.219591 97.832107 \r\nL 141.3075 97.921679 \r\nL 141.341897 97.836673 \r\nL 141.391961 97.510741 \r\nL 141.426039 97.600453 \r\nL 141.506895 97.624916 \r\nL 141.515827 97.559392 \r\nL 141.63713 97.362382 \r\nL 141.760614 97.410779 \r\nL 141.942728 97.281755 \r\nL 141.957637 97.327014 \r\nL 141.972427 97.194224 \r\nL 142.629534 96.392297 \r\nL 142.718192 96.551935 \r\nL 142.834878 96.59758 \r\nL 142.854832 96.529621 \r\nL 143.036846 96.257003 \r\nL 143.098679 96.051713 \r\nL 143.156571 96.142952 \r\nL 143.333457 96.234403 \r\nL 143.355829 96.097042 \r\nL 143.972097 94.842381 \r\nL 144.133956 94.81756 \r\nL 144.14527 94.746996 \r\nL 144.778071 94.336459 \r\nL 145.106926 93.945375 \r\nL 145.213156 93.99129 \r\nL 145.220893 93.919051 \r\nL 145.33882 93.820238 \r\nL 145.556527 93.721068 \r\nL 145.576828 93.767069 \r\nL 145.639391 93.813125 \r\nL 146.278834 93.450955 \r\nL 146.281562 93.377203 \r\nL 146.371178 93.470018 \r\nL 146.709914 92.922848 \r\nL 146.737058 92.969261 \r\nL 146.799347 93.015731 \r\nL 146.823297 92.94108 \r\nL 147.001187 92.98759 \r\nL 147.009636 92.91283 \r\nL 147.452796 92.584113 \r\nL 147.466162 92.630632 \r\nL 147.524173 92.601842 \r\nL 147.624326 92.8355 \r\nL 147.781003 92.579577 \r\nL 147.920709 92.398305 \r\nL 148.020735 92.245722 \r\nL 148.208971 92.386475 \r\nL 148.222484 92.309986 \r\nL 148.740652 91.959767 \r\nL 148.742887 92.006889 \r\nL 148.822776 91.821655 \r\nL 148.955293 91.760572 \r\nL 148.96657 91.807836 \r\nL 148.993486 91.651894 \r\nL 149.040574 91.668365 \r\nL 149.147863 91.684899 \r\nL 149.209687 91.449409 \r\nL 149.293025 91.496836 \r\nL 150.191698 90.457521 \r\nL 150.216598 90.505229 \r\nL 150.312719 90.309922 \r\nL 151.309027 88.420824 \r\nL 151.797478 88.538469 \r\nL 151.804157 88.453584 \r\nL 152.419313 87.00923 \r\nL 152.450872 87.056993 \r\nL 152.455526 86.969646 \r\nL 152.570935 86.619059 \r\nL 152.759481 86.354855 \r\nL 152.837874 86.178113 \r\nL 152.933995 86.273285 \r\nL 152.986421 86.320969 \r\nL 153.042242 86.232331 \r\nL 153.193106 85.882523 \r\nL 153.207431 85.930234 \r\nL 153.300933 85.798903 \r\nL 153.340212 85.846688 \r\nL 153.357046 85.89454 \r\nL 153.402428 85.714817 \r\nL 153.445548 85.810603 \r\nL 153.533658 85.720495 \r\nL 153.545391 85.63026 \r\nL 153.62248 85.774266 \r\nL 153.640098 85.683816 \r\nL 153.648392 85.731923 \r\nL 153.714988 85.550582 \r\nL 153.90201 85.277615 \r\nL 154.170328 84.867969 \r\nL 154.281102 84.591811 \r\nL 154.37079 84.639656 \r\nL 154.41047 84.547311 \r\nL 154.529748 83.990478 \r\nL 154.558862 84.038166 \r\nL 154.597849 84.133747 \r\nL 154.645166 84.040381 \r\nL 154.788768 84.232234 \r\nL 154.820182 84.04484 \r\nL 154.910381 83.668433 \r\nL 154.940864 83.71637 \r\nL 155.162978 83.337656 \r\nL 155.311954 83.004482 \r\nL 155.413869 83.196113 \r\nL 155.441879 83.100367 \r\nL 155.61476 82.763895 \r\nL 155.672743 82.811872 \r\nL 155.707733 82.715356 \r\nL 155.756564 82.569839 \r\nL 156.071578 82.761769 \r\nL 156.273546 82.173753 \r\nL 156.499546 81.776449 \r\nL 156.663194 81.674241 \r\nL 156.691369 81.722551 \r\nL 156.721888 81.622908 \r\nL 156.782626 81.423173 \r\nL 156.794825 81.471439 \r\nL 157.365574 80.660852 \r\nL 157.513674 80.553707 \r\nL 157.537798 80.650166 \r\nL 157.541721 80.548116 \r\nL 157.621374 80.445911 \r\nL 157.875293 80.488419 \r\nL 157.945593 80.173526 \r\nL 158.083064 79.855721 \r\nL 158.103839 79.904119 \r\nL 158.209522 80.098459 \r\nL 158.335907 79.680312 \r\nL 158.496133 79.57537 \r\nL 158.534472 79.672558 \r\nL 158.574362 79.721265 \r\nL 158.582117 79.616028 \r\nL 158.606186 79.664767 \r\nL 158.708165 79.126785 \r\nL 158.759925 79.224261 \r\nL 159.019994 79.001403 \r\nL 159.11294 79.050272 \r\nL 159.135978 78.943008 \r\nL 159.151224 78.835575 \r\nL 159.192483 78.933361 \r\nL 159.436047 78.717798 \r\nL 159.554631 78.550371 \r\nL 159.751763 78.055154 \r\nL 159.808532 78.152708 \r\nL 159.907162 77.933648 \r\nL 159.924781 77.823854 \r\nL 160.013338 77.872603 \r\nL 160.622946 77.13299 \r\nL 160.709286 77.230394 \r\nL 160.746594 77.167462 \r\nL 160.991919 76.992158 \r\nL 161.117191 77.138872 \r\nL 161.117811 77.026366 \r\nL 161.322817 76.347451 \r\nL 161.66242 76.282291 \r\nL 161.733496 76.216916 \r\nL 161.77188 76.314495 \r\nL 161.775676 76.200101 \r\nL 161.837189 76.297895 \r\nL 162.14068 75.953248 \r\nL 162.178644 76.002102 \r\nL 162.353587 76.051038 \r\nL 162.394462 75.704339 \r\nL 162.459306 75.802121 \r\nL 162.616585 75.618702 \r\nL 162.675407 75.667685 \r\nL 162.682077 75.55112 \r\nL 163.009025 75.249089 \r\nL 163.066889 75.34701 \r\nL 163.080009 75.396095 \r\nL 163.506271 75.524894 \r\nL 163.52201 75.406825 \r\nL 163.556763 75.100707 \r\nL 163.810866 75.150084 \r\nL 164.568428 73.782395 \r\nL 164.718316 73.929665 \r\nL 164.732322 73.807553 \r\nL 165.07572 73.044389 \r\nL 165.087316 73.093306 \r\nL 165.286164 73.142308 \r\nL 165.561133 72.617307 \r\nL 165.739816 72.491995 \r\nL 165.760792 72.589803 \r\nL 165.846101 72.737165 \r\nL 165.851503 72.611293 \r\nL 165.879833 72.485198 \r\nL 165.892351 72.534357 \r\nL 166.365337 72.301901 \r\nL 166.498409 72.450071 \r\nL 166.887373 72.116299 \r\nL 167.045208 71.859591 \r\nL 167.124477 71.958286 \r\nL 167.287404 72.007768 \r\nL 167.397448 71.539987 \r\nL 167.513085 71.638819 \r\nL 167.831192 71.558226 \r\nL 167.855516 71.607794 \r\nL 167.907039 71.47734 \r\nL 168.293822 71.726167 \r\nL 168.322681 71.595105 \r\nL 168.406868 71.332259 \r\nL 168.667959 71.068447 \r\nL 168.672448 70.936177 \r\nL 169.084239 70.636708 \r\nL 169.28181 70.150835 \r\nL 169.391224 70.01636 \r\nL 169.435275 70.0656 \r\nL 169.478686 70.114931 \r\nL 169.480046 69.980047 \r\nL 169.657644 69.709522 \r\nL 169.807523 69.758705 \r\nL 169.856236 69.622982 \r\nL 170.130666 69.721463 \r\nL 170.554693 69.448928 \r\nL 170.803759 68.584297 \r\nL 170.84895 68.44566 \r\nL 170.905035 68.494391 \r\nL 171.20722 68.173863 \r\nL 171.854364 68.574365 \r\nL 171.854574 68.624066 \r\nL 171.903752 68.482789 \r\nL 172.085766 68.532498 \r\nL 172.243391 68.248944 \r\nL 172.266521 67.964289 \r\nL 172.392659 68.013736 \r\nL 172.638705 67.539181 \r\nL 172.639891 67.588551 \r\nL 172.708403 67.638017 \r\nL 172.983874 67.108215 \r\nL 173.200267 67.011851 \r\nL 173.232054 67.061138 \r\nL 173.590617 66.573368 \r\nL 173.726619 66.327296 \r\nL 173.74598 66.376393 \r\nL 174.041651 65.979879 \r\nL 174.042317 66.028874 \r\nL 174.091157 66.077966 \r\nL 174.47804 65.977691 \r\nL 174.508888 66.02688 \r\nL 174.824523 65.775926 \r\nL 174.867168 65.82511 \r\nL 174.931218 65.923777 \r\nL 175.020195 65.772893 \r\nL 175.077183 65.871755 \r\nL 175.095714 65.921336 \r\nL 175.171899 65.769836 \r\nL 175.213477 65.869094 \r\nL 175.268896 65.918875 \r\nL 175.371978 65.61432 \r\nL 175.556766 64.995179 \r\nL 175.705213 65.044623 \r\nL 175.855703 64.734796 \r\nL 175.902089 64.784124 \r\nL 175.964297 64.833555 \r\nL 176.17464 64.052239 \r\nL 176.215671 64.101216 \r\nL 176.30879 63.676904 \r\nL 176.483231 63.774687 \r\nL 176.661531 63.297619 \r\nL 177.11126 63.44402 \r\nL 177.308602 63.591352 \r\nL 177.535597 63.430954 \r\nL 177.964688 63.529461 \r\nL 177.986813 63.207168 \r\nL 178.069066 63.25634 \r\nL 178.238124 63.354999 \r\nL 178.446442 62.917014 \r\nL 178.54644 63.015581 \r\nL 178.904218 63.114572 \r\nL 179.033897 62.622469 \r\nL 179.05813 62.671754 \r\nL 179.521198 61.872307 \r\nL 179.560103 61.704757 \r\nL 179.873575 61.732667 \r\nL 180.180505 61.103869 \r\nL 180.190486 61.152672 \r\nL 180.66454 61.030867 \r\nL 180.777139 61.079752 \r\nL 181.052865 60.736906 \r\nL 181.078713 60.785626 \r\nL 181.173073 60.613491 \r\nL 181.186978 60.662182 \r\nL 181.254431 60.710982 \r\nL 181.261758 60.538185 \r\nL 181.347906 60.684826 \r\nL 181.445167 60.783137 \r\nL 181.670055 60.484012 \r\nL 181.913856 60.309197 \r\nL 181.955653 60.407325 \r\nL 182.011072 60.5059 \r\nL 182.016692 60.330114 \r\nL 182.159546 60.026359 \r\nL 182.172529 60.075492 \r\nL 182.227036 59.898206 \r\nL 182.420728 59.720514 \r\nL 182.42987 59.542413 \r\nL 182.486886 59.591216 \r\nL 182.782356 59.68916 \r\nL 183.027417 59.248534 \r\nL 183.425395 59.29745 \r\nL 183.467438 59.116789 \r\nL 183.482374 59.214654 \r\nL 183.59406 59.263758 \r\nL 183.748319 58.766607 \r\nL 183.800271 58.448774 \r\nL 183.997439 58.264777 \r\nL 184.024592 58.313282 \r\nL 184.397388 58.459486 \r\nL 184.47787 58.088902 \r\nL 184.691846 57.951429 \r\nL 184.720367 58.000023 \r\nL 185.101493 56.872985 \r\nL 185.240843 56.541066 \r\nL 185.303871 56.636144 \r\nL 185.673374 56.253994 \r\nL 185.695335 56.301358 \r\nL 185.738519 55.917007 \r\nL 185.795762 56.011378 \r\nL 186.181468 55.671449 \r\nL 186.211276 55.282268 \r\nL 186.245108 55.329023 \r\nL 186.336959 55.133536 \r\nL 186.352214 55.180223 \r\nL 186.421985 55.227025 \r\nL 186.654591 54.880422 \r\nL 186.67262 54.927085 \r\nL 186.860884 55.020757 \r\nL 186.875355 54.822708 \r\nL 187.616138 54.905722 \r\nL 187.620262 54.705546 \r\nL 188.014389 54.303687 \r\nL 188.017619 54.102001 \r\nL 188.203502 53.899807 \r\nL 188.212106 53.697104 \r\nL 188.238565 53.743274 \r\nL 188.37839 53.789562 \r\nL 188.658979 53.381824 \r\nL 188.794999 53.427829 \r\nL 189.040178 52.696047 \r\nL 189.239253 52.741588 \r\nL 189.548674 51.698443 \r\nL 189.760122 51.877563 \r\nL 189.974608 51.711318 \r\nL 190.039224 51.846582 \r\nL 190.418662 51.634067 \r\nL 190.432731 51.679172 \r\nL 191.095167 51.815196 \r\nL 191.392781 51.171653 \r\nL 191.782265 51.261595 \r\nL 191.997243 50.61159 \r\nL 192.050901 50.393772 \r\nL 192.083921 50.438009 \r\nL 192.227495 50.219381 \r\nL 192.339072 49.339024 \r\nL 192.381754 49.117461 \r\nL 192.452036 49.203458 \r\nL 192.71362 49.376844 \r\nL 192.758537 49.153237 \r\nL 192.795672 49.240147 \r\nL 193.001353 49.015444 \r\nL 193.161314 48.337661 \r\nL 193.189498 48.423055 \r\nL 193.340199 48.465927 \r\nL 193.552696 48.238009 \r\nL 193.574502 48.009465 \r\nL 194.144329 48.180091 \r\nL 194.88108 47.339379 \r\nL 194.883115 47.106083 \r\nL 194.988405 47.189944 \r\nL 195.021552 47.274276 \r\nL 195.602904 47.530129 \r\nL 195.890144 47.293472 \r\nL 196.400648 47.336263 \r\nL 196.430346 47.09866 \r\nL 196.517224 47.184051 \r\nL 196.558173 47.226929 \r\nL 196.779036 46.987815 \r\nL 196.816874 46.748017 \r\nL 197.385561 47.004481 \r\nL 197.414365 47.047657 \r\nL 197.490532 46.805174 \r\nL 197.929102 46.934585 \r\nL 198.061783 46.690528 \r\nL 198.063389 46.733624 \r\nL 198.390847 46.907269 \r\nL 198.539887 46.661025 \r\nL 198.669119 46.704457 \r\nL 198.821434 46.45719 \r\nL 198.863368 46.543837 \r\nL 199.126695 46.295237 \r\nL 199.328918 46.38179 \r\nL 199.379875 46.131843 \r\nL 199.459801 46.175005 \r\nL 199.87214 46.305265 \r\nL 199.948188 46.053335 \r\nL 200.573681 45.420652 \r\nL 200.574831 45.463457 \r\nL 200.578252 45.207205 \r\nL 200.63254 45.292557 \r\nL 200.825439 44.776412 \r\nL 201.103309 44.818626 \r\nL 201.318169 44.340341 \r\nL 201.942339 44.424079 \r\nL 202.107528 43.898896 \r\nL 202.733404 44.023425 \r\nL 202.75095 43.758583 \r\nL 203.264419 43.492919 \r\nL 203.853498 43.61652 \r\nL 203.917356 43.783131 \r\nL 203.937183 43.514164 \r\nL 204.623122 43.723598 \r\nL 204.695685 43.808304 \r\nL 205.023116 43.89355 \r\nL 205.029694 43.620463 \r\nL 205.0839 43.662921 \r\nL 205.487124 43.920556 \r\nL 205.605991 43.643977 \r\nL 205.947847 43.687025 \r\nL 206.06853 43.172831 \r\nL 206.745062 43.473695 \r\nL 206.81299 43.190902 \r\nL 206.847926 43.234055 \r\nL 207.325373 43.495995 \r\nL 207.40487 43.209457 \r\nL 207.459705 43.253218 \r\nL 207.805175 43.297127 \r\nL 207.915721 42.719521 \r\nL 208.144395 42.429257 \r\nL 208.203746 42.138012 \r\nL 208.236446 42.180405 \r\nL 209.023981 42.30845 \r\nL 209.114627 41.761697 \r\nL 209.315956 40.869509 \r\nL 209.75668 41.034117 \r\nL 210.001996 40.732883 \r\nL 210.363514 40.815141 \r\nL 210.400649 40.511951 \r\nL 210.466177 40.552838 \r\nL 211.147372 40.801189 \r\nL 211.204798 40.493689 \r\nL 211.644573 40.576704 \r\nL 211.740703 40.267167 \r\nL 211.747564 40.308418 \r\nL 212.455565 40.038217 \r\nL 213.126304 40.203357 \r\nL 213.16185 39.888686 \r\nL 213.526535 39.92985 \r\nL 213.549144 39.613546 \r\nL 214.038097 39.736436 \r\nL 214.05733 39.417475 \r\nL 214.075524 39.45823 \r\nL 214.251835 39.540188 \r\nL 214.521411 38.895681 \r\nL 214.565644 38.93585 \r\nL 214.741828 38.976168 \r\nL 215.219558 38.691025 \r\nL 215.346308 38.731113 \r\nL 215.658877 37.414365 \r\nL 215.868929 37.082095 \r\nL 216.021281 37.119616 \r\nL 216.158359 36.450213 \r\nL 216.161698 36.486753 \r\nL 216.678205 36.56025 \r\nL 216.9328 36.221946 \r\nL 217.324693 36.331517 \r\nL 217.524197 35.990189 \r\nL 217.747095 36.062767 \r\nL 217.949884 35.096891 \r\nL 218.486848 35.202231 \r\nL 218.52942 34.851364 \r\nL 218.567266 34.886135 \r\nL 218.64024 34.921042 \r\nL 219.192969 35.062049 \r\nL 219.289135 35.09765 \r\nL 219.347584 34.741006 \r\nL 219.428659 34.846739 \r\nL 219.684714 34.953745 \r\nL 219.781081 34.989701 \r\nL 219.865624 34.627087 \r\nL 220.046935 34.662529 \r\nL 220.15093 34.297784 \r\nL 220.215728 34.332702 \r\nL 220.335507 34.402962 \r\nL 220.374375 34.034711 \r\nL 221.268049 34.209464 \r\nL 221.550198 34.351883 \r\nL 221.629175 33.975853 \r\nL 221.756692 34.011119 \r\nL 221.883734 33.632803 \r\nL 222.214951 33.702337 \r\nL 222.940708 33.737324 \r\nL 223.039173 33.355238 \r\nL 223.55756 33.424185 \r\nL 223.649402 33.039 \r\nL 224.357987 33.244876 \r\nL 224.548915 32.853494 \r\nL 224.851647 32.921762 \r\nL 224.860206 32.527132 \r\nL 225.399231 32.594424 \r\nL 226.027251 31.395399 \r\nL 226.146684 31.426853 \r\nL 226.508394 30.617055 \r\nL 226.602006 30.67707 \r\nL 226.716803 30.267806 \r\nL 227.234862 30.297169 \r\nL 227.367013 29.885229 \r\nL 227.407888 29.942635 \r\nL 228.544369 30.029717 \r\nL 228.950622 29.219065 \r\nL 229.806358 29.41525 \r\nL 229.875846 28.986011 \r\nL 229.88643 29.013627 \r\nL 230.994023 29.239253 \r\nL 231.069223 28.799427 \r\nL 231.144879 28.855216 \r\nL 231.593486 28.939904 \r\nL 231.706094 28.493014 \r\nL 232.9704 28.802898 \r\nL 233.225853 27.960576 \r\nL 233.260269 27.987836 \r\nL 233.271874 27.518343 \r\nL 234.333921 27.6511 \r\nL 234.338666 27.678058 \r\nL 235.195661 27.814937 \r\nL 235.531988 26.843346 \r\nL 236.03366 26.868762 \r\nL 236.04397 26.377959 \r\nL 236.14229 26.426748 \r\nL 236.301722 25.930908 \r\nL 237.801445 26.00131 \r\nL 238.081641 25.499063 \r\nL 238.712454 25.544066 \r\nL 239.044894 25.036553 \r\nL 240.2538 25.188658 \r\nL 240.395532 25.233191 \r\nL 240.918207 25.255642 \r\nL 241.020048 24.731911 \r\nL 242.250451 24.860815 \r\nL 242.39835 24.325645 \r\nL 242.449316 24.346102 \r\nL 244.263907 24.600926 \r\nL 244.510729 24.043022 \r\nL 250.592266 24.522979 \r\nL 250.757848 23.924479 \r\nL 251.686557 24.033757 \r\nL 251.721137 23.422179 \r\nL 254.628979 23.764803 \r\nL 255.183332 23.11707 \r\nL 259.020595 23.328769 \r\nL 259.387688 22.654412 \r\nL 262.075241 22.96505 \r\nL 262.539194 22.268422 \r\nL 262.710816 21.544445 \r\nL 266.790119 21.773657 \r\nL 269.985639 22.048386 \r\nL 270.343682 20.421401 \r\nL 282.167364 20.815005 \r\nL 284.23467 21.078839 \r\nL 294.581457 21.527738 \r\nL 297.932285 21.907113 \r\nL 297.975697 21.936705 \r\nL 299.48427 22.058795 \r\nL 304.923207 22.220326 \r\nL 305.024137 22.253899 \r\nL 309.20665 22.501788 \r\nL 309.240719 22.539155 \r\nL 309.386775 21.203689 \r\nL 309.574528 21.232501 \r\nL 310.103663 19.869024 \r\nL 310.309463 19.908815 \r\nL 311.906411 19.94976 \r\nL 312.438775 18.527159 \r\nL 316.442094 18.628656 \r\nL 316.585158 17.083636 \r\nL 363.363068 17.083636 \r\nL 363.363068 17.083636 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p6e270b4024)\" d=\"M 58.999432 17.083636 \r\nL 59.110336 17.173447 \r\nL 59.649439 17.263257 \r\nL 59.76009 17.353068 \r\nL 60.529602 17.442878 \r\nL 60.640324 17.532688 \r\nL 60.740802 17.622499 \r\nL 60.851534 17.80212 \r\nL 61.376163 17.89193 \r\nL 61.487245 17.981741 \r\nL 61.776718 18.071551 \r\nL 61.887147 18.251172 \r\nL 62.26566 18.340982 \r\nL 62.37641 18.520603 \r\nL 62.93971 18.610413 \r\nL 63.052285 18.790034 \r\nL 63.174188 18.879845 \r\nL 63.283936 19.059466 \r\nL 63.305886 19.149276 \r\nL 63.41499 19.328897 \r\nL 63.47085 19.418707 \r\nL 63.581926 19.598328 \r\nL 64.049915 19.688138 \r\nL 64.159691 19.777949 \r\nL 64.388456 19.867759 \r\nL 64.499363 19.95757 \r\nL 64.861574 20.04738 \r\nL 64.97352 20.227001 \r\nL 65.100379 20.316811 \r\nL 65.211623 20.496432 \r\nL 65.406036 20.586243 \r\nL 65.532874 20.855674 \r\nL 65.79311 20.945484 \r\nL 65.899993 21.035295 \r\nL 65.94362 21.125105 \r\nL 66.054821 21.394536 \r\nL 66.696712 21.484347 \r\nL 66.804955 21.574157 \r\nL 66.986543 21.663968 \r\nL 67.102756 22.023209 \r\nL 67.230538 22.11302 \r\nL 67.343936 22.382451 \r\nL 67.471618 22.472261 \r\nL 67.583061 22.562072 \r\nL 67.688045 22.651882 \r\nL 67.801068 22.921313 \r\nL 68.268723 23.011124 \r\nL 68.377648 23.190745 \r\nL 68.488303 23.280555 \r\nL 68.598826 23.549986 \r\nL 68.748626 23.639797 \r\nL 68.860559 23.819418 \r\nL 68.907383 23.909228 \r\nL 69.016815 23.999038 \r\nL 69.321054 24.088849 \r\nL 69.437759 24.26847 \r\nL 69.562271 24.35828 \r\nL 69.672516 24.448091 \r\nL 69.778209 24.537901 \r\nL 69.895877 24.807332 \r\nL 70.003165 24.897143 \r\nL 70.115856 25.076763 \r\nL 70.258008 25.166574 \r\nL 70.365499 25.346195 \r\nL 70.518119 25.436005 \r\nL 70.634064 25.795247 \r\nL 70.800762 25.885057 \r\nL 70.909583 26.064678 \r\nL 71.01023 26.154488 \r\nL 71.126115 26.334109 \r\nL 71.239056 26.42392 \r\nL 71.3547 26.51373 \r\nL 71.416754 26.603541 \r\nL 71.53116 26.693351 \r\nL 71.56723 26.783161 \r\nL 71.68989 27.052593 \r\nL 71.768614 27.142403 \r\nL 71.888709 27.501645 \r\nL 72.135337 27.591455 \r\nL 72.246441 27.771076 \r\nL 72.754194 27.860886 \r\nL 72.873696 28.399749 \r\nL 73.138641 28.489559 \r\nL 73.249922 28.57937 \r\nL 73.375888 28.66918 \r\nL 73.497043 28.848801 \r\nL 73.659076 28.938611 \r\nL 73.771043 29.028422 \r\nL 74.025649 29.118232 \r\nL 74.137427 29.387663 \r\nL 74.150381 29.477474 \r\nL 74.266272 29.746905 \r\nL 74.359353 29.836716 \r\nL 74.469746 30.016336 \r\nL 75.082519 30.106147 \r\nL 75.205678 30.285768 \r\nL 75.379064 30.375578 \r\nL 75.491778 30.645009 \r\nL 75.616258 30.73482 \r\nL 75.753659 31.094061 \r\nL 75.886631 31.183872 \r\nL 75.996141 31.363493 \r\nL 76.12637 31.722734 \r\nL 76.462899 31.812545 \r\nL 76.578605 31.992166 \r\nL 76.739093 32.081976 \r\nL 76.865619 32.351407 \r\nL 77.070826 32.441218 \r\nL 77.177702 32.620838 \r\nL 77.31848 33.159701 \r\nL 77.332394 33.249511 \r\nL 77.461202 33.429132 \r\nL 77.500476 33.518943 \r\nL 77.632388 33.878184 \r\nL 77.781126 33.967995 \r\nL 77.890619 34.057805 \r\nL 77.973336 34.147616 \r\nL 78.109384 34.327236 \r\nL 78.167225 34.417047 \r\nL 78.286051 34.686478 \r\nL 78.355904 34.776288 \r\nL 78.473122 35.04572 \r\nL 78.487282 35.13553 \r\nL 78.604187 35.315151 \r\nL 78.789664 35.404961 \r\nL 78.9038 35.674393 \r\nL 78.994576 35.764203 \r\nL 79.107793 36.123445 \r\nL 79.138313 36.213255 \r\nL 79.247609 36.392876 \r\nL 79.288995 36.482686 \r\nL 79.397789 36.841928 \r\nL 79.547151 36.931738 \r\nL 79.652874 37.29098 \r\nL 79.833378 37.380791 \r\nL 79.963823 37.560411 \r\nL 80.098037 37.650222 \r\nL 80.235654 38.009463 \r\nL 80.322828 38.099274 \r\nL 80.444905 38.458516 \r\nL 80.521706 38.548326 \r\nL 80.633262 38.907568 \r\nL 80.918527 38.997378 \r\nL 81.035793 39.087188 \r\nL 81.211162 39.176999 \r\nL 81.322791 39.35662 \r\nL 81.559638 39.44643 \r\nL 81.668085 39.715861 \r\nL 81.740972 39.805672 \r\nL 81.892621 40.075103 \r\nL 82.060123 40.164913 \r\nL 82.170729 40.344534 \r\nL 82.200308 40.434345 \r\nL 82.307536 40.703776 \r\nL 82.442586 40.793586 \r\nL 82.567879 41.063018 \r\nL 82.724097 41.152828 \r\nL 82.830742 41.332449 \r\nL 82.944025 41.422259 \r\nL 82.994387 41.51207 \r\nL 83.111356 41.691691 \r\nL 83.284695 41.781501 \r\nL 83.444926 42.230553 \r\nL 83.480902 42.320363 \r\nL 83.598017 42.410174 \r\nL 83.780886 42.499984 \r\nL 83.882748 42.589795 \r\nL 84.007605 42.679605 \r\nL 84.122003 42.859226 \r\nL 84.300128 42.949036 \r\nL 84.407189 43.218468 \r\nL 84.543327 43.308278 \r\nL 84.657787 43.487899 \r\nL 84.899057 43.577709 \r\nL 85.042738 43.75733 \r\nL 85.259113 43.847141 \r\nL 85.372186 44.026761 \r\nL 85.430148 44.116572 \r\nL 85.541712 44.386003 \r\nL 85.745844 44.475813 \r\nL 85.857437 44.565624 \r\nL 85.883294 44.655434 \r\nL 86.1314 45.104486 \r\nL 86.231369 45.194297 \r\nL 86.358602 45.284107 \r\nL 86.46772 45.373918 \r\nL 86.604778 45.91278 \r\nL 86.634061 46.002591 \r\nL 86.742956 46.182211 \r\nL 86.785385 46.272022 \r\nL 86.906957 46.451643 \r\nL 86.958459 46.541453 \r\nL 87.084231 46.810884 \r\nL 87.309273 46.900695 \r\nL 87.392883 47.080316 \r\nL 87.672972 47.170126 \r\nL 87.793637 47.349747 \r\nL 87.919226 47.619178 \r\nL 88.050538 47.708988 \r\nL 88.197614 47.888609 \r\nL 88.320183 47.97842 \r\nL 88.475808 48.247851 \r\nL 88.575414 48.337661 \r\nL 88.686954 48.517282 \r\nL 88.847741 48.607093 \r\nL 88.973736 48.696903 \r\nL 89.000656 48.786713 \r\nL 89.133752 48.966334 \r\nL 89.195966 49.056145 \r\nL 89.320499 49.325576 \r\nL 89.646443 49.415386 \r\nL 89.775823 49.774628 \r\nL 89.847802 49.864438 \r\nL 89.944927 49.954249 \r\nL 90.275451 50.582922 \r\nL 90.338687 50.672732 \r\nL 90.58427 51.570836 \r\nL 90.781963 52.019888 \r\nL 90.894056 52.28932 \r\nL 90.981609 52.37913 \r\nL 91.092048 52.648561 \r\nL 91.120562 52.738372 \r\nL 91.237205 53.097613 \r\nL 91.339124 53.187424 \r\nL 91.479985 53.277234 \r\nL 91.639716 53.367045 \r\nL 91.743252 53.456855 \r\nL 91.961791 53.546666 \r\nL 92.110592 53.636476 \r\nL 92.356628 53.726286 \r\nL 92.571418 54.265149 \r\nL 92.618917 54.354959 \r\nL 92.731372 54.53458 \r\nL 92.800513 54.624391 \r\nL 92.974046 54.983632 \r\nL 93.117256 55.073443 \r\nL 93.194369 55.163253 \r\nL 93.321555 55.342874 \r\nL 93.401659 55.432684 \r\nL 93.504189 55.612305 \r\nL 93.676213 56.151168 \r\nL 93.750602 56.240978 \r\nL 93.873669 56.779841 \r\nL 93.940435 56.869651 \r\nL 94.047129 56.959461 \r\nL 94.290686 57.049272 \r\nL 94.401102 57.318703 \r\nL 94.54321 57.408513 \r\nL 94.655526 57.498324 \r\nL 94.793991 57.588134 \r\nL 94.886982 57.947376 \r\nL 95.073421 58.216807 \r\nL 95.091037 58.306618 \r\nL 95.201373 58.486238 \r\nL 95.265396 58.576049 \r\nL 95.365862 58.665859 \r\nL 95.401108 58.75567 \r\nL 95.535661 58.935291 \r\nL 95.674537 59.025101 \r\nL 95.758218 59.114911 \r\nL 95.882928 59.204722 \r\nL 95.986088 59.384343 \r\nL 96.091338 59.474153 \r\nL 96.24514 60.013016 \r\nL 96.479631 60.102826 \r\nL 96.628994 60.372257 \r\nL 96.786145 60.462068 \r\nL 96.909866 60.821309 \r\nL 96.948657 60.91112 \r\nL 97.05106 61.00093 \r\nL 97.068979 61.090741 \r\nL 97.168042 61.180551 \r\nL 97.325855 61.449982 \r\nL 97.367323 61.539793 \r\nL 97.479876 61.719413 \r\nL 97.529429 61.809224 \r\nL 97.649322 62.168466 \r\nL 97.698892 62.258276 \r\nL 97.815729 62.437897 \r\nL 97.983154 62.527707 \r\nL 98.099539 62.886949 \r\nL 98.391287 62.976759 \r\nL 98.536887 63.06657 \r\nL 98.610677 63.15638 \r\nL 98.840295 63.605432 \r\nL 98.914121 63.695243 \r\nL 99.164059 64.503536 \r\nL 99.202808 64.593347 \r\nL 99.31589 64.862778 \r\nL 99.457608 65.22202 \r\nL 99.570878 65.31183 \r\nL 99.754374 65.760882 \r\nL 99.776814 65.850693 \r\nL 99.887593 66.030313 \r\nL 99.969814 66.120124 \r\nL 100.084607 66.389555 \r\nL 100.142265 66.479366 \r\nL 100.244568 66.748797 \r\nL 100.293276 66.838607 \r\nL 100.395118 67.018228 \r\nL 100.476239 67.108038 \r\nL 100.611 67.287659 \r\nL 100.678143 67.37747 \r\nL 100.776303 67.46728 \r\nL 100.927323 67.557091 \r\nL 101.141175 67.646901 \r\nL 101.282277 68.455195 \r\nL 101.359114 68.545005 \r\nL 101.484646 68.814436 \r\nL 101.647705 68.904247 \r\nL 101.842661 69.353299 \r\nL 101.903121 69.443109 \r\nL 102.160371 69.892161 \r\nL 102.199745 69.981972 \r\nL 102.30499 70.071782 \r\nL 102.562623 70.161593 \r\nL 102.683707 70.251403 \r\nL 102.709629 70.341213 \r\nL 102.840808 71.059697 \r\nL 103.000186 71.239318 \r\nL 103.310528 71.329128 \r\nL 103.44179 71.508749 \r\nL 103.516214 71.598559 \r\nL 103.623721 71.77818 \r\nL 103.928795 71.867991 \r\nL 104.070672 72.227232 \r\nL 104.17984 72.406853 \r\nL 104.326668 72.496664 \r\nL 104.436926 72.676284 \r\nL 104.602695 72.766095 \r\nL 104.732738 72.945716 \r\nL 104.923958 73.035526 \r\nL 105.052761 73.394768 \r\nL 105.337835 73.484578 \r\nL 105.448641 73.754009 \r\nL 105.508101 73.84382 \r\nL 105.593333 73.93363 \r\nL 105.710859 74.023441 \r\nL 105.780192 74.113251 \r\nL 105.893347 74.203061 \r\nL 106.002962 74.292872 \r\nL 106.118864 74.652114 \r\nL 106.334212 74.741924 \r\nL 106.461993 75.011355 \r\nL 106.846171 75.101166 \r\nL 107.533301 76.268701 \r\nL 107.875043 76.358511 \r\nL 108.027892 76.627943 \r\nL 108.116659 76.717753 \r\nL 108.300959 77.076995 \r\nL 108.880503 78.513961 \r\nL 109.081071 78.603772 \r\nL 109.27209 79.232445 \r\nL 109.341861 79.322255 \r\nL 109.519527 79.861118 \r\nL 109.66312 79.950928 \r\nL 109.812949 80.220359 \r\nL 109.849454 80.31017 \r\nL 109.943859 80.489791 \r\nL 110.089884 80.579601 \r\nL 110.345209 81.118464 \r\nL 110.491767 81.208274 \r\nL 110.595908 81.477705 \r\nL 110.858883 81.747136 \r\nL 111.128459 81.836947 \r\nL 111.238754 82.016568 \r\nL 111.38487 82.106378 \r\nL 111.497765 82.196189 \r\nL 111.561418 82.285999 \r\nL 111.670901 82.375809 \r\nL 111.897477 82.46562 \r\nL 112.108596 82.824861 \r\nL 112.173764 82.914672 \r\nL 112.265254 83.184103 \r\nL 112.459179 83.273914 \r\nL 112.540123 83.543345 \r\nL 112.681713 83.633155 \r\nL 112.799206 83.812776 \r\nL 112.887157 83.902586 \r\nL 113.018925 84.172018 \r\nL 113.099266 84.261828 \r\nL 113.294104 84.71088 \r\nL 113.615992 85.339553 \r\nL 113.676019 85.429364 \r\nL 113.787139 85.698795 \r\nL 114.024855 85.788605 \r\nL 114.143786 85.968226 \r\nL 114.215299 86.058036 \r\nL 114.770816 87.135761 \r\nL 114.828908 87.225572 \r\nL 114.941877 87.584814 \r\nL 115.100278 87.674624 \r\nL 115.295343 88.213486 \r\nL 115.375415 88.303297 \r\nL 115.478251 88.482918 \r\nL 115.528506 88.572728 \r\nL 115.64959 88.842159 \r\nL 115.677619 88.93197 \r\nL 116.83653 90.548557 \r\nL 116.92298 90.638368 \r\nL 117.014726 90.817989 \r\nL 117.201689 90.907799 \r\nL 117.303339 91.17723 \r\nL 117.400669 91.267041 \r\nL 117.49382 91.356851 \r\nL 117.688384 91.446661 \r\nL 117.793848 91.626282 \r\nL 118.004306 91.716093 \r\nL 118.041162 91.895714 \r\nL 118.201351 91.985524 \r\nL 118.317755 92.254955 \r\nL 118.422858 92.344766 \r\nL 118.534198 92.434576 \r\nL 118.767497 92.524386 \r\nL 118.880584 92.704007 \r\nL 119.420549 92.793818 \r\nL 120.185762 94.410405 \r\nL 120.5176 94.500216 \r\nL 120.587987 94.679836 \r\nL 121.513932 95.847372 \r\nL 121.61128 96.026993 \r\nL 121.864702 96.116803 \r\nL 121.982515 96.296424 \r\nL 122.100716 96.386234 \r\nL 122.211992 96.565855 \r\nL 122.40688 96.745476 \r\nL 122.473635 96.835286 \r\nL 122.591658 97.014907 \r\nL 122.792559 97.104718 \r\nL 122.899897 97.284339 \r\nL 122.980184 97.374149 \r\nL 123.193447 97.913011 \r\nL 123.316794 98.002822 \r\nL 123.560116 98.541684 \r\nL 123.749037 99.170357 \r\nL 123.807832 99.260168 \r\nL 123.922342 99.619409 \r\nL 124.186768 99.70922 \r\nL 124.466312 100.248082 \r\nL 124.592401 100.337893 \r\nL 124.694019 100.607324 \r\nL 125.058352 100.697134 \r\nL 125.186754 100.876755 \r\nL 125.333782 100.966566 \r\nL 125.391472 101.056376 \r\nL 125.531936 101.325807 \r\nL 125.566547 101.415618 \r\nL 125.965857 102.134101 \r\nL 126.004625 102.223911 \r\nL 126.09591 102.403532 \r\nL 126.171064 102.493343 \r\nL 126.287897 102.762774 \r\nL 126.372626 102.852584 \r\nL 126.405518 103.122016 \r\nL 126.578102 103.211826 \r\nL 126.719095 103.391447 \r\nL 126.849463 103.481257 \r\nL 127.118546 104.10993 \r\nL 127.262974 104.199741 \r\nL 127.3843 104.558982 \r\nL 127.63998 104.828414 \r\nL 127.689391 104.918224 \r\nL 127.87266 105.187655 \r\nL 127.995651 105.457086 \r\nL 128.241309 105.636707 \r\nL 128.54577 105.726518 \r\nL 128.651951 105.995949 \r\nL 128.696507 106.085759 \r\nL 128.783463 106.17557 \r\nL 128.87437 106.26538 \r\nL 128.967398 106.445001 \r\nL 129.14058 106.534811 \r\nL 129.250145 106.714432 \r\nL 129.474489 106.804243 \r\nL 129.623661 106.983864 \r\nL 129.823982 107.253295 \r\nL 129.919715 107.343105 \r\nL 129.99033 107.522726 \r\nL 130.402792 107.612536 \r\nL 130.523849 107.792157 \r\nL 130.842038 108.510641 \r\nL 130.932625 108.690261 \r\nL 131.372496 108.780072 \r\nL 131.519049 108.959693 \r\nL 131.88685 109.049503 \r\nL 132.015767 109.318934 \r\nL 132.092732 109.408745 \r\nL 132.224322 109.498555 \r\nL 132.315767 109.678176 \r\nL 132.796084 109.767986 \r\nL 133.073895 110.127228 \r\nL 133.116887 110.217039 \r\nL 133.348544 110.666091 \r\nL 133.447275 110.755901 \r\nL 133.619453 111.025332 \r\nL 133.686277 111.115143 \r\nL 133.7316 111.204953 \r\nL 133.834742 111.294764 \r\nL 133.955251 111.654005 \r\nL 134.634712 112.462299 \r\nL 134.742886 112.552109 \r\nL 134.988767 112.911351 \r\nL 135.066476 113.001161 \r\nL 135.256081 113.270593 \r\nL 135.338954 113.360403 \r\nL 135.43241 113.450214 \r\nL 135.55458 113.719645 \r\nL 135.650865 113.809455 \r\nL 136.021371 113.899266 \r\nL 136.302772 114.258507 \r\nL 136.586518 114.348318 \r\nL 136.721799 114.527939 \r\nL 136.89343 114.617749 \r\nL 137.273032 115.156611 \r\nL 137.601677 115.246422 \r\nL 137.753034 115.336232 \r\nL 137.819493 115.426043 \r\nL 137.96055 115.515853 \r\nL 138.032091 115.605664 \r\nL 138.183421 115.695474 \r\nL 138.718623 115.785284 \r\nL 139.497499 117.04263 \r\nL 139.538401 117.132441 \r\nL 139.718426 117.401872 \r\nL 140.15967 117.491682 \r\nL 140.31256 117.671303 \r\nL 140.348025 117.761114 \r\nL 140.463543 118.030545 \r\nL 140.762471 118.479597 \r\nL 140.957268 118.569407 \r\nL 141.034584 118.838839 \r\nL 141.219591 118.928649 \r\nL 142.37545 120.814668 \r\nL 142.645182 120.904478 \r\nL 142.718192 121.26372 \r\nL 143.012631 121.35353 \r\nL 143.155448 121.443341 \r\nL 143.156571 121.533151 \r\nL 143.849918 121.892393 \r\nL 143.972097 121.982203 \r\nL 144.14527 122.072014 \r\nL 144.385431 122.161824 \r\nL 144.411726 122.251634 \r\nL 144.683474 122.341445 \r\nL 144.778071 122.521066 \r\nL 144.860132 122.610876 \r\nL 144.96951 122.880307 \r\nL 145.237964 122.970118 \r\nL 145.639391 123.329359 \r\nL 145.857061 123.41917 \r\nL 145.990171 123.778411 \r\nL 146.095151 123.868222 \r\nL 146.121948 123.958032 \r\nL 146.281562 124.047843 \r\nL 146.662661 124.317274 \r\nL 146.737058 124.407084 \r\nL 146.823297 124.496895 \r\nL 147.201312 124.586705 \r\nL 147.452796 124.676516 \r\nL 147.624326 125.305189 \r\nL 147.797818 125.394999 \r\nL 147.920709 125.484809 \r\nL 148.113352 125.57462 \r\nL 148.53393 125.933861 \r\nL 148.64891 126.023672 \r\nL 148.906197 126.382914 \r\nL 149.209687 126.921776 \r\nL 149.721714 127.370828 \r\nL 149.869194 127.460639 \r\nL 149.95141 127.73007 \r\nL 150.312719 128.089311 \r\nL 150.528756 128.179122 \r\nL 150.659164 128.358743 \r\nL 150.782502 128.448553 \r\nL 150.89511 128.538364 \r\nL 151.087507 128.628174 \r\nL 151.492309 129.077226 \r\nL 151.586806 129.346657 \r\nL 152.151397 129.436468 \r\nL 152.474768 129.88552 \r\nL 152.879251 129.97533 \r\nL 153.098345 130.154951 \r\nL 153.14464 130.244761 \r\nL 153.248215 130.424382 \r\nL 153.445548 130.873434 \r\nL 153.61706 130.963245 \r\nL 153.714988 131.232676 \r\nL 154.281102 131.322486 \r\nL 154.509639 131.412297 \r\nL 154.558862 131.502107 \r\nL 154.645166 131.681728 \r\nL 155.034777 132.13078 \r\nL 155.311954 132.220591 \r\nL 155.61476 132.669643 \r\nL 156.273546 133.567747 \r\nL 156.467412 133.657557 \r\nL 156.587584 133.837178 \r\nL 156.794825 134.106609 \r\nL 157.297947 134.19642 \r\nL 157.437982 134.465851 \r\nL 157.528108 134.555661 \r\nL 157.621374 134.645472 \r\nL 157.933212 134.914903 \r\nL 158.075062 135.274145 \r\nL 158.103839 135.363955 \r\nL 158.22485 135.723197 \r\nL 158.50637 135.813007 \r\nL 158.759925 136.44168 \r\nL 158.895215 136.531491 \r\nL 159.177264 136.800922 \r\nL 159.192483 136.890732 \r\nL 159.59661 136.980543 \r\nL 159.717329 137.070353 \r\nL 159.764035 137.160164 \r\nL 159.924781 137.249974 \r\nL 160.206638 137.429595 \r\nL 160.56375 137.519405 \r\nL 160.673556 137.699026 \r\nL 160.889794 137.968457 \r\nL 161.047848 138.058268 \r\nL 161.146916 138.237889 \r\nL 161.66242 138.327699 \r\nL 161.837189 138.776751 \r\nL 162.178644 138.866561 \r\nL 162.395493 139.046182 \r\nL 162.682077 139.315614 \r\nL 163.009025 139.405424 \r\nL 163.080009 139.674855 \r\nL 163.38527 139.854476 \r\nL 163.556763 140.213718 \r\nL 164.568428 140.66277 \r\nL 165.561133 141.471064 \r\nL 165.759332 141.560874 \r\nL 165.851503 141.920116 \r\nL 166.058735 142.009926 \r\nL 166.498409 142.548789 \r\nL 167.045208 142.638599 \r\nL 167.124477 142.81822 \r\nL 167.471032 143.087651 \r\nL 167.513085 143.177461 \r\nL 167.831192 143.267272 \r\nL 167.993005 143.446893 \r\nL 168.293822 143.806134 \r\nL 169.003721 143.895945 \r\nL 169.19047 144.075566 \r\nL 169.435275 144.165376 \r\nL 169.54541 144.255186 \r\nL 169.856236 144.344997 \r\nL 170.554693 144.524618 \r\nL 170.649044 144.614428 \r\nL 170.768048 144.704239 \r\nL 171.050936 144.794049 \r\nL 171.107167 144.97367 \r\nL 171.307118 145.06348 \r\nL 171.73766 145.692153 \r\nL 171.854574 146.051395 \r\nL 172.266521 146.141205 \r\nL 172.541298 146.231016 \r\nL 172.634025 146.320826 \r\nL 172.708403 146.590257 \r\nL 173.153771 146.680068 \r\nL 173.370812 146.949499 \r\nL 173.618062 147.039309 \r\nL 173.726619 147.12912 \r\nL 173.8195 147.21893 \r\nL 174.091157 147.488361 \r\nL 174.47804 147.578172 \r\nL 174.508888 147.667982 \r\nL 174.824523 147.757793 \r\nL 175.281925 148.566086 \r\nL 175.463847 148.655897 \r\nL 175.556766 148.835518 \r\nL 176.661531 149.464191 \r\nL 177.008971 149.643811 \r\nL 177.535597 150.003053 \r\nL 178.069066 150.272484 \r\nL 178.54644 150.721536 \r\nL 179.330334 151.170589 \r\nL 179.560103 151.350209 \r\nL 179.825373 151.52983 \r\nL 179.953628 151.709451 \r\nL 180.03714 151.799261 \r\nL 180.644723 151.889072 \r\nL 180.777139 152.068693 \r\nL 181.173073 152.158503 \r\nL 181.347906 152.607555 \r\nL 181.670055 152.876986 \r\nL 181.922323 152.966797 \r\nL 182.172529 153.415849 \r\nL 182.486886 153.505659 \r\nL 183.425395 153.954711 \r\nL 183.482091 154.044522 \r\nL 183.482374 154.134332 \r\nL 183.800271 154.403764 \r\nL 184.024592 154.493574 \r\nL 184.975838 154.942626 \r\nL 185.240843 155.032436 \r\nL 185.303871 155.212057 \r\nL 185.738519 155.301868 \r\nL 185.795762 155.481489 \r\nL 186.211276 155.571299 \r\nL 186.421985 155.84073 \r\nL 186.654591 155.930541 \r\nL 186.67262 156.020351 \r\nL 187.163161 156.379593 \r\nL 187.225286 156.469403 \r\nL 188.017619 156.738834 \r\nL 188.238565 156.828645 \r\nL 188.460323 156.918455 \r\nL 188.844368 157.008266 \r\nL 189.010069 157.187886 \r\nL 189.548674 157.277697 \r\nL 189.760122 157.636939 \r\nL 189.974608 157.726749 \r\nL 190.039224 157.99618 \r\nL 190.432731 158.085991 \r\nL 192.050901 158.535043 \r\nL 192.083921 158.624853 \r\nL 192.442356 158.714664 \r\nL 192.527144 158.894284 \r\nL 192.791694 159.253526 \r\nL 192.795672 159.343336 \r\nL 193.179453 159.433147 \r\nL 193.189498 159.522957 \r\nL 193.552696 159.612768 \r\nL 193.764098 159.702578 \r\nL 194.40959 159.972009 \r\nL 194.797842 160.06182 \r\nL 195.007127 160.421062 \r\nL 195.021552 160.510872 \r\nL 195.51452 160.870114 \r\nL 195.602904 161.049734 \r\nL 196.430346 161.139545 \r\nL 196.779036 161.408976 \r\nL 196.971497 161.498787 \r\nL 197.33329 161.768218 \r\nL 197.414365 162.037649 \r\nL 197.611452 162.127459 \r\nL 197.929102 162.30708 \r\nL 198.063389 162.396891 \r\nL 198.184308 162.576512 \r\nL 198.188688 162.666322 \r\nL 199.788364 163.474616 \r\nL 199.948188 163.564426 \r\nL 200.326951 163.654237 \r\nL 200.578252 163.923668 \r\nL 200.63254 164.103289 \r\nL 201.208362 164.193099 \r\nL 201.489936 164.37272 \r\nL 203.830067 164.911582 \r\nL 203.937183 165.360634 \r\nL 204.598131 165.719876 \r\nL 204.695685 165.989307 \r\nL 205.0839 166.258739 \r\nL 205.487124 166.797601 \r\nL 205.970739 166.887412 \r\nL 206.06853 166.977222 \r\nL 206.925479 167.785516 \r\nL 207.023599 168.054947 \r\nL 207.459705 168.324378 \r\nL 209.035322 168.77343 \r\nL 209.114627 168.863241 \r\nL 209.545095 168.953051 \r\nL 209.733651 169.132672 \r\nL 209.75668 169.222482 \r\nL 210.343132 169.312293 \r\nL 210.466177 169.491914 \r\nL 211.147372 170.030776 \r\nL 211.432934 170.120587 \r\nL 211.747564 170.300207 \r\nL 212.455565 170.390018 \r\nL 213.16185 170.749259 \r\nL 213.830107 171.018691 \r\nL 214.192438 171.288122 \r\nL 214.357582 171.377932 \r\nL 214.565644 171.467743 \r\nL 214.741828 171.557553 \r\nL 215.219558 171.647364 \r\nL 215.530375 171.737174 \r\nL 216.158359 171.826984 \r\nL 216.161698 171.916795 \r\nL 217.045901 172.186226 \r\nL 218.093642 172.814899 \r\nL 218.141433 172.904709 \r\nL 218.567266 173.08433 \r\nL 218.64024 173.174141 \r\nL 219.15567 173.353762 \r\nL 219.192969 173.533382 \r\nL 219.428659 173.892624 \r\nL 219.469826 173.982434 \r\nL 220.215728 174.431487 \r\nL 220.374375 174.611107 \r\nL 220.881923 174.790728 \r\nL 220.889131 174.880539 \r\nL 221.268049 175.060159 \r\nL 221.629175 175.419401 \r\nL 222.03937 175.599022 \r\nL 222.214951 175.688832 \r\nL 223.649402 175.958264 \r\nL 223.765769 176.048074 \r\nL 224.166584 176.227695 \r\nL 224.277696 176.407316 \r\nL 224.357987 176.497126 \r\nL 224.706412 176.586937 \r\nL 224.996098 176.766557 \r\nL 225.753167 176.856368 \r\nL 226.508394 176.946178 \r\nL 226.716803 177.125799 \r\nL 227.402049 177.30542 \r\nL 227.407888 177.39523 \r\nL 228.784639 177.664662 \r\nL 228.950622 177.754472 \r\nL 229.429995 178.023903 \r\nL 230.357007 178.742387 \r\nL 230.881926 178.922007 \r\nL 231.069223 179.191439 \r\nL 231.391408 179.55068 \r\nL 232.865876 180.269164 \r\nL 232.9704 180.628405 \r\nL 233.155799 180.718216 \r\nL 233.271874 180.987647 \r\nL 234.193868 181.167268 \r\nL 234.239397 181.346889 \r\nL 234.338666 181.526509 \r\nL 234.798841 181.70613 \r\nL 235.195661 181.975562 \r\nL 236.04397 182.065372 \r\nL 236.301722 182.244993 \r\nL 237.801445 182.514424 \r\nL 238.401911 182.604234 \r\nL 239.044894 182.694045 \r\nL 239.754501 182.873666 \r\nL 239.851908 183.053287 \r\nL 240.241757 183.232907 \r\nL 240.265643 183.412528 \r\nL 240.395532 183.502339 \r\nL 242.069778 183.86158 \r\nL 242.242349 184.041201 \r\nL 242.250451 184.131012 \r\nL 242.449316 184.220822 \r\nL 243.012794 184.669874 \r\nL 243.063614 184.759684 \r\nL 243.475689 184.939305 \r\nL 243.98944 185.208737 \r\nL 245.339202 185.747599 \r\nL 246.748233 186.286462 \r\nL 247.750573 186.466082 \r\nL 248.049474 186.735514 \r\nL 250.14563 187.004945 \r\nL 250.554402 187.184566 \r\nL 250.592266 187.274376 \r\nL 251.032552 187.364187 \r\nL 253.099548 188.621532 \r\nL 253.106044 188.711343 \r\nL 254.44077 189.070584 \r\nL 254.628979 189.160395 \r\nL 255.678307 189.250205 \r\nL 256.844166 189.519637 \r\nL 256.895552 189.609447 \r\nL 257.176077 189.699257 \r\nL 258.292594 189.878878 \r\nL 258.464453 189.968689 \r\nL 260.172458 190.32793 \r\nL 260.791865 190.597362 \r\nL 261.603149 191.136224 \r\nL 261.635558 191.315845 \r\nL 262.936853 191.675087 \r\nL 262.96375 191.764897 \r\nL 264.597304 191.944518 \r\nL 264.690824 192.034328 \r\nL 266.172884 192.48338 \r\nL 266.179453 192.573191 \r\nL 267.402191 192.842622 \r\nL 267.464398 192.932432 \r\nL 267.919154 193.112053 \r\nL 267.974701 193.201864 \r\nL 270.343682 193.920347 \r\nL 272.638576 194.369399 \r\nL 274.087242 194.818451 \r\nL 277.346356 195.177693 \r\nL 277.973646 195.267503 \r\nL 280.915903 195.447124 \r\nL 280.924863 195.536934 \r\nL 281.346135 195.716555 \r\nL 281.707206 196.075797 \r\nL 282.284461 196.255418 \r\nL 282.654693 196.435039 \r\nL 283.018866 196.70447 \r\nL 283.637142 196.884091 \r\nL 283.653273 196.973901 \r\nL 284.015202 197.153522 \r\nL 284.23467 197.422953 \r\nL 287.458401 197.961816 \r\nL 287.547305 198.051626 \r\nL 290.475402 198.590489 \r\nL 290.67071 198.680299 \r\nL 294.581457 199.219162 \r\nL 297.590319 200.207076 \r\nL 297.762287 200.386697 \r\nL 298.341914 200.656128 \r\nL 299.48427 200.925559 \r\nL 302.288135 201.10518 \r\nL 304.923207 201.374612 \r\nL 305.024137 201.464422 \r\nL 306.194065 201.644043 \r\nL 306.413004 201.733853 \r\nL 310.243442 202.362526 \r\nL 310.309463 202.452337 \r\nL 312.681582 202.721768 \r\nL 312.719647 202.811578 \r\nL 313.13705 202.991199 \r\nL 314.310738 203.17082 \r\nL 314.404769 203.26063 \r\nL 316.583078 203.440251 \r\nL 316.585158 203.530062 \r\nL 317.44635 203.799493 \r\nL 317.677496 203.889303 \r\nL 320.46857 204.158734 \r\nL 320.790408 204.428166 \r\nL 323.75106 204.787407 \r\nL 323.808632 204.877218 \r\nL 327.695036 205.685512 \r\nL 327.864085 205.775322 \r\nL 329.073338 205.954943 \r\nL 329.189796 206.134564 \r\nL 331.810562 206.493805 \r\nL 332.213402 206.673426 \r\nL 333.94513 206.853047 \r\nL 334.153813 207.032668 \r\nL 334.334029 207.122478 \r\nL 340.473449 208.200203 \r\nL 342.259537 208.379824 \r\nL 343.16779 208.739066 \r\nL 343.260709 208.918687 \r\nL 344.847639 209.188118 \r\nL 345.377941 209.367739 \r\nL 345.421718 209.547359 \r\nL 348.672037 210.086222 \r\nL 348.913292 210.176032 \r\nL 351.488347 210.535274 \r\nL 351.649841 210.625084 \r\nL 353.38071 210.894516 \r\nL 353.549358 211.074137 \r\nL 353.593591 211.163947 \r\nL 354.435568 211.343568 \r\nL 355.038716 211.612999 \r\nL 355.167747 211.702809 \r\nL 355.891058 211.88243 \r\nL 357.781852 212.331482 \r\nL 358.45197 212.511103 \r\nL 358.618391 212.690724 \r\nL 359.800509 212.870345 \r\nL 360.222383 213.319397 \r\nL 361.568422 213.588828 \r\nL 361.867341 213.768449 \r\nL 362.332006 213.94807 \r\nL 362.644155 214.217501 \r\nL 363.33024 214.576743 \r\nL 363.363068 214.756364 \r\nL 363.363068 214.756364 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 43.78125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 224.64 \r\nL 378.58125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 378.58125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 7.2 \r\nL 378.58125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 294.571875 132.098125 \r\nL 371.58125 132.098125 \r\nQ 373.58125 132.098125 373.58125 130.098125 \r\nL 373.58125 101.741875 \r\nQ 373.58125 99.741875 371.58125 99.741875 \r\nL 294.571875 99.741875 \r\nQ 292.571875 99.741875 292.571875 101.741875 \r\nL 292.571875 130.098125 \r\nQ 292.571875 132.098125 294.571875 132.098125 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\">\r\n     <path d=\"M 296.571875 107.840313 \r\nL 316.571875 107.840313 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\"/>\r\n    <g id=\"text_15\">\r\n     <!-- Precision -->\r\n     <defs>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n     </defs>\r\n     <g transform=\"translate(324.571875 111.340313)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"60.287109\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"101.369141\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"162.892578\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"217.873047\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"245.65625\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"297.755859\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"325.539062\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"386.720703\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 296.571875 122.518438 \r\nL 316.571875 122.518438 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_16\">\r\n     <!-- Recall -->\r\n     <defs>\r\n      <path d=\"M 44.390625 34.1875 \r\nQ 47.5625 33.109375 50.5625 29.59375 \r\nQ 53.5625 26.078125 56.59375 19.921875 \r\nL 66.609375 0 \r\nL 56 0 \r\nL 46.6875 18.703125 \r\nQ 43.0625 26.03125 39.671875 28.421875 \r\nQ 36.28125 30.8125 30.421875 30.8125 \r\nL 19.671875 30.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nL 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.578125 72.90625 50.734375 67.671875 \r\nQ 56.890625 62.453125 56.890625 51.90625 \r\nQ 56.890625 45.015625 53.6875 40.46875 \r\nQ 50.484375 35.9375 44.390625 34.1875 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 38.921875 \r\nL 32.078125 38.921875 \r\nQ 39.203125 38.921875 42.84375 42.21875 \r\nQ 46.484375 45.515625 46.484375 51.90625 \r\nQ 46.484375 58.296875 42.84375 61.546875 \r\nQ 39.203125 64.796875 32.078125 64.796875 \r\nz\r\n\" id=\"DejaVuSans-82\"/>\r\n     </defs>\r\n     <g transform=\"translate(324.571875 126.018438)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"69.419922\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"130.943359\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"185.923828\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"247.203125\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"274.986328\" xlink:href=\"#DejaVuSans-108\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p6e270b4024\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Threshold-PR curve\n",
    "train_loss, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader)\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_true.ravel(), y_prob.ravel())\n",
    "plt.plot(thresholds, precisions[:-1], \"r--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"b-\", label=\"Recall\")\n",
    "plt.ylabel(\"Performance\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.21466015"
      ]
     },
     "metadata": {},
     "execution_count": 369
    }
   ],
   "source": [
    "# Best threshold for f1\n",
    "threshold = find_best_threshold(y_true.ravel(), y_prob.ravel())\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine predictions using threshold\n",
    "test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
    "y_pred = np.array([np.where(prob >= threshold, 1, 0) for prob in y_prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"precision\": 0.34976369495166487,\n  \"recall\": 0.2389525693244905,\n  \"f1\": 0.25651611949847763,\n  \"num_samples\": 480.0\n}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "performance = get_performance(\n",
    "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "print (json.dumps(performance['overall'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='tag', index=3, options=('interpretability', 'segmentation', 'produ…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d70d9cc9e5724f8ba9fcb443891310e4"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "@widgets.interact(tag=list(sorted_tags_by_f1.keys()))\n",
    "def display_tag_analysis(tag='transformers'):\n",
    "    # Performance\n",
    "    print (json.dumps(performance[\"class\"][tag], indent=2))\n",
    "    \n",
    "    # TP, FP, FN samples\n",
    "    index = label_encoder.class_to_index[tag]\n",
    "    tp, fp, fn = [], [], []\n",
    "    for i in range(len(y_test)):\n",
    "        true = y_test[i][index]\n",
    "        pred = y_pred[i][index]\n",
    "        if true and pred:\n",
    "            tp.append(i)\n",
    "        elif not true and pred:\n",
    "            fp.append(i)\n",
    "        elif true and not pred:\n",
    "            fn.append(i)\n",
    "            \n",
    "    # Samples\n",
    "    num_samples = 3\n",
    "    if len(tp): \n",
    "        print (\"\\n=== True positives ===\")\n",
    "        for i in tp[:num_samples]:        \n",
    "            print (f\"  {X_test_raw[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")\n",
    "    if len(fp): \n",
    "        print (\"=== False positives === \")\n",
    "        for i in fp[:num_samples]:        \n",
    "            print (f\"  {X_test_raw[i]}\")\n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")\n",
    "    if len(fn): \n",
    "        print (\"=== False negatives ===\")\n",
    "        for i in fn[:num_samples]:        \n",
    "            print (f\"  {X_test_raw[i]}\") \n",
    "            print (f\"    true: {label_encoder.decode([y_test[i]])[0]}\")\n",
    "            print (f\"    pred: {label_encoder.decode([y_pred[i]])[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts\n",
    "dir = Path(\"rnn\")\n",
    "dir.mkdir(parents=True, exist_ok=True)\n",
    "tokenizer.save(fp=Path(dir, 'tokenzier.json'))\n",
    "label_encoder.save(fp=Path(dir, 'label_encoder.json'))\n",
    "torch.save(best_model.state_dict(), Path(dir, 'model.pt'))\n",
    "with open(Path(dir, 'performance.json'), \"w\") as fp:\n",
    "    json.dump(performance, indent=2, sort_keys=False, fp=fp)"
   ]
  },
  {
   "source": [
    "Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model:\n<bound method Module.named_parameters of RNN(\n  (embeddings): Embedding(136, 128, padding_idx=0)\n  (rnn): GRU(128, 128, batch_first=True, bidirectional=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=256, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=35, bias=True)\n)>\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = RNN(\n",
    "    embedding_dim=embedding_dim, vocab_size=vocab_size,\n",
    "    rnn_hidden_dim=rnn_hidden_dim, hidden_dim=hidden_dim, \n",
    "    dropout_p=dropout_p, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "print (f\"Model:\\n{model.named_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embeddings): Embedding(136, 128, padding_idx=0)\n",
       "  (rnn): GRU(128, 128, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=35, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 375
    }
   ],
   "source": [
    "# Load artifacts\n",
    "device = torch.device(\"cpu\")\n",
    "tokenizer = Tokenizer.load(fp=Path(dir, 'tokenzier.json'))\n",
    "label_encoder = LabelEncoder.load(fp=Path(dir, 'label_encoder.json'))\n",
    "model = RNN(\n",
    "    embedding_dim=embedding_dim, vocab_size=vocab_size,\n",
    "    rnn_hidden_dim=rnn_hidden_dim, hidden_dim=hidden_dim, \n",
    "    dropout_p=dropout_p, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(Path(dir, 'model.pt'), map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "text = \"Transfer learning with BERT for self-supervised learning\"\n",
    "X = np.array(tokenizer.texts_to_sequences([preprocess(text)]))\n",
    "y_filler = label_encoder.encode([np.array([label_encoder.classes[0]]*len(X))])\n",
    "dataset = RNNTextDataset(X=X, y=y_filler)\n",
    "dataloader = dataset.create_dataloader(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "metadata": {},
     "execution_count": 378
    }
   ],
   "source": [
    "# Inference\n",
    "y_prob = trainer.predict_step(dataloader)\n",
    "y_pred = np.array([np.where(prob >= threshold, 1, 0) for prob in y_prob])\n",
    "label_encoder.decode(y_pred)"
   ]
  },
  {
   "source": [
    "limitation: since we're using character embeddings our encoded sequences are quite long (>100), the RNNs may potentially be suffering from memory issues. We also can't process our tokens in parallel because we're restricted by sequential processin"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Transformers w/ Contextual Embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data splits\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)\n",
    "X_test_raw = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "cuda = True\n",
    "device = torch.device('cuda' if (\n",
    "    torch.cuda.is_available() and cuda) else 'cpu')\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "if device.type == 'cuda':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "31090\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "vocab_size = len(tokenizer)\n",
    "print (vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1000, 60]) torch.Size([1000, 60])\n",
      "torch.Size([227, 61]) torch.Size([227, 61])\n",
      "torch.Size([217, 58]) torch.Size([217, 58])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize inputs\n",
    "encoded_input = tokenizer(X_train.tolist(), return_tensors='pt', padding=True)\n",
    "X_train_ids = encoded_input['input_ids']\n",
    "X_train_masks = encoded_input['attention_mask']\n",
    "print (X_train_ids.shape, X_train_masks.shape)\n",
    "encoded_input = tokenizer(X_val.tolist(), return_tensors='pt', padding=True)\n",
    "X_val_ids = encoded_input['input_ids']\n",
    "X_val_masks = encoded_input['attention_mask']\n",
    "print (X_val_ids.shape, X_val_masks.shape)\n",
    "encoded_input = tokenizer(X_test.tolist(), return_tensors='pt', padding=True)\n",
    "X_test_ids = encoded_input['input_ids']\n",
    "X_test_masks = encoded_input['attention_mask']\n",
    "print (X_test_ids.shape, X_test_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([  102,  6160,  1923,   288,  3254,  1572, 18205,  5560,   137,  4578,\n          147,   626, 23474,   291,  2715,   494, 10558,   205,   103,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n[CLS] albumentations fast image augmentation library and easy to use wrapper around other libraries. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# Decode\n",
    "print (f\"{X_train_ids[0]}\\n{tokenizer.decode(X_train_ids[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['[CLS]', 'alb', '##ument', '##ations', 'fast', 'image', 'augmentation', 'library', 'and', 'easy', 'to', 'use', 'wrap', '##per', 'around', 'other', 'libraries', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# Sub-word tokens\n",
    "print (tokenizer.convert_ids_to_tokens(ids=X_train_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "class counts:\n  [120  41 388 106  41  75  34  73  51  78  64  51  55  93  51 429  33  69\n  30  51 258  32  49  59  57  60  48  40 213  40  34  46 196  39  39]\n\nclass weights:\n  {0: 0.008333333333333333, 1: 0.024390243902439025, 2: 0.002577319587628866, 3: 0.009433962264150943, 4: 0.024390243902439025, 5: 0.013333333333333334, 6: 0.029411764705882353, 7: 0.0136986301369863, 8: 0.0196078431372549, 9: 0.01282051282051282, 10: 0.015625, 11: 0.0196078431372549, 12: 0.01818181818181818, 13: 0.010752688172043012, 14: 0.0196078431372549, 15: 0.002331002331002331, 16: 0.030303030303030304, 17: 0.014492753623188406, 18: 0.03333333333333333, 19: 0.0196078431372549, 20: 0.003875968992248062, 21: 0.03125, 22: 0.02040816326530612, 23: 0.01694915254237288, 24: 0.017543859649122806, 25: 0.016666666666666666, 26: 0.020833333333333332, 27: 0.025, 28: 0.004694835680751174, 29: 0.025, 30: 0.029411764705882353, 31: 0.021739130434782608, 32: 0.00510204081632653, 33: 0.02564102564102564, 34: 0.02564102564102564}\n"
     ]
    }
   ],
   "source": [
    "# Class weights\n",
    "counts = np.bincount([label_encoder.class_to_index[class_] for class_ in all_tags])\n",
    "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
    "print (\"class counts:\\n\"\n",
    "    f\"  {counts}\\n\\n\"\n",
    "    \"class weights:\\n\"\n",
    "    f\"  {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids, masks, targets):\n",
    "        self.ids = ids\n",
    "        self.masks = masks\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Dataset(N={len(self)})>\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ids = torch.tensor(self.ids[index], dtype=torch.long)\n",
    "        masks = torch.tensor(self.masks[index], dtype=torch.long)\n",
    "        targets = torch.FloatTensor(self.targets[index])\n",
    "        return ids, masks, targets\n",
    "\n",
    "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data splits:\n  Train dataset:<Dataset(N=1000)>\n  Val dataset: <Dataset(N=227)>\n  Test dataset: <Dataset(N=217)>\nSample point:\n  ids: tensor([  102,  6160,  1923,   288,  3254,  1572, 18205,  5560,   137,  4578,\n          147,   626, 23474,   291,  2715,   494, 10558,   205,   103,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n  masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n  targets: tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = TransformerTextDataset(ids=X_train_ids, masks=X_train_masks, targets=y_train)\n",
    "val_dataset = TransformerTextDataset(ids=X_val_ids, masks=X_val_masks, targets=y_val)\n",
    "test_dataset = TransformerTextDataset(ids=X_test_ids, masks=X_test_masks, targets=y_test)\n",
    "print (\"Data splits:\\n\"\n",
    "    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n",
    "    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n",
    "    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  ids: {train_dataset[0][0]}\\n\"\n",
    "    f\"  masks: {train_dataset[0][1]}\\n\"\n",
    "    f\"  targets: {train_dataset[0][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample batch:\n  ids: torch.Size([128, 60])\n  masks: torch.Size([128, 60])\n  targets: torch.Size([128, 35])\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 128\n",
    "train_dataloader = train_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "val_dataloader = val_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "test_dataloader = test_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "batch = next(iter(train_dataloader))\n",
    "print (\"Sample batch:\\n\"\n",
    "    f\"  ids: {batch[0].size()}\\n\"\n",
    "    f\"  masks: {batch[1].size()}\\n\"\n",
    "    f\"  targets: {batch[2].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "embedding_dim = transformer.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, transformer, dropout_p, embedding_dim, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "        self.fc1 = torch.nn.Linear(embedding_dim, num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ids, masks = inputs\n",
    "        seq, pool = self.transformer(input_ids=ids, attention_mask=masks)\n",
    "        z = self.dropout(pool)\n",
    "        z = self.fc1(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method Module.named_parameters of Transformer(\n  (transformer): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=768, out_features=35, bias=True)\n)>\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "dropout_p = 0.5\n",
    "model = Transformer(\n",
    "    transformer=transformer, dropout_p=dropout_p,\n",
    "    embedding_dim=embedding_dim, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "print (model.named_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "lr = 1e-4\n",
    "num_epochs = 200\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "class_weights_tensor = torch.Tensor(np.array(list(class_weights.values())))\n",
    "loss_fn = nn.BCEWithLogitsLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer & scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer module\n",
    "trainer = Trainer(\n",
    "    model=model, device=device, loss_fn=loss_fn, \n",
    "    optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 4.00 GiB total capacity; 2.22 GiB already allocated; 2.45 MiB free; 2.28 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-399-8d0f0dee99db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m best_model = trainer.train(\n\u001b[1;32m----> 3\u001b[1;33m     num_epochs, patience, train_dataloader, val_dataloader)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-271-92b71a82bb31>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, num_epochs, patience, train_dataloader, val_dataloader)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;31m# Steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-271-92b71a82bb31>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Reset gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Define loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mJ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-393-84a08ca4bd6c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    851\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m         )\n\u001b[0;32m    855\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    486\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m                 )\n\u001b[0;32m    490\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         )\n\u001b[0;32m    409\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m         )\n\u001b[0;32m    346\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[0mmixed_key_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[0mmixed_value_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1692\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 4.00 GiB total capacity; 2.22 GiB already allocated; 2.45 MiB free; 2.28 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "best_model = trainer.train(\n",
    "    num_epochs, patience, train_dataloader, val_dataloader)"
   ]
  },
  {
   "source": [
    "# Tracking Experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import mlflow\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify arguments\n",
    "args = Namespace(\n",
    "    char_level=True,\n",
    "    filter_sizes=list(range(1, 11)),\n",
    "    batch_size=64,\n",
    "    embedding_dim=128,\n",
    "    num_filters=128,\n",
    "    hidden_dim=128,\n",
    "    dropout_p=0.5,\n",
    "    lr=2e-4,\n",
    "    num_epochs=200,\n",
    "    patience=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tracking URI\n",
    "EXPERIMENTS_DIR = Path(\"experiments\")\n",
    "Path(EXPERIMENTS_DIR).mkdir(exist_ok=True) # create experiments dir\n",
    "mlflow.set_tracking_uri(\"file:\\\\\" + str(EXPERIMENTS_DIR.absolute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer (modified for experiment tracking)\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, device, loss_fn=None,\n",
    "                optimizer=None, scheduler=None):\n",
    "\n",
    "        # Set params\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train_step(self, dataloader):\n",
    "        \"\"\"Train step.\"\"\"\n",
    "        # Set model to train mode\n",
    "        self.model.train()\n",
    "        loss = 0.0\n",
    "\n",
    "        # Iterate over train batches\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Step\n",
    "            batch = [item.to(self.device) for item in batch]\n",
    "            inputs, targets = batch[:-1], batch[-1]\n",
    "            self.optimizer.zero_grad()  # Reset gradients\n",
    "            z = self.model(inputs)  # Forward pass\n",
    "            J = self.loss_fn(z, targets)  # Define loss\n",
    "            J.backward()  # Backward pass\n",
    "            self.optimizer.step()  # Update weights\n",
    "\n",
    "            # Cumulative Metrics\n",
    "            loss += (J.detach().item() - loss) / (i + 1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def eval_step(self, dataloader):\n",
    "        \"\"\"Validation or test step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        loss = 0.0\n",
    "        y_trues, y_probs = [], []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Step\n",
    "                batch = [item.to(self.device) for item in batch]  # Set device\n",
    "                inputs, y_true = batch[:-1], batch[-1]\n",
    "                z = self.model(inputs)  # Forward pass\n",
    "                J = self.loss_fn(z, y_true).item()\n",
    "\n",
    "                # Cumulative Metrics\n",
    "                loss += (J - loss) / (i + 1)\n",
    "\n",
    "                # Store outputs\n",
    "                y_prob = torch.sigmoid(z).cpu().numpy()\n",
    "                y_probs.extend(y_prob)\n",
    "                y_trues.extend(y_true.cpu().numpy())\n",
    "\n",
    "        return loss, np.vstack(y_trues), np.vstack(y_probs)\n",
    "\n",
    "    def predict_step(self, dataloader):\n",
    "        \"\"\"Prediction step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        y_probs = []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Forward pass w/ inputs\n",
    "                inputs, targets = batch[:-1], batch[-1]\n",
    "                y_prob = self.model(inputs)\n",
    "\n",
    "                # Store outputs\n",
    "                y_probs.extend(y_prob)\n",
    "\n",
    "        return np.vstack(y_probs)\n",
    "\n",
    "    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n",
    "        best_val_loss = np.inf\n",
    "        for epoch in range(num_epochs):\n",
    "            # Steps\n",
    "            train_loss = self.train_step(dataloader=train_dataloader)\n",
    "            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = self.model\n",
    "                _patience = patience  # reset _patience\n",
    "            else:\n",
    "                _patience -= 1\n",
    "            if not _patience:  # 0\n",
    "                print(\"Stopping early!\")\n",
    "                break\n",
    "\n",
    "            # Tracking\n",
    "            mlflow.log_metrics(\n",
    "                {\"train_loss\": train_loss, \"val_loss\": val_loss}, step=epoch\n",
    "            )\n",
    "\n",
    "            # Logging\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1} | \"\n",
    "                f\"train_loss: {train_loss:.5f}, \"\n",
    "                f\"val_loss: {val_loss:.5f}, \"\n",
    "                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n",
    "                f\"_patience: {_patience}\"\n",
    "            )\n",
    "\n",
    "        return best_model, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the cnn function\n",
    "def train_cnn(args, df):\n",
    "    \"\"\"Train a CNN using specific arguments.\"\"\"\n",
    "\n",
    "    # Set seeds\n",
    "    set_seed()\n",
    "\n",
    "    # Get data splits\n",
    "    preprocessed_df = df.copy()\n",
    "    preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)\n",
    "    X_test_raw = X_test\n",
    "    num_classes = len(label_encoder)\n",
    "\n",
    "    # Set device\n",
    "    cuda = True\n",
    "    device = torch.device('cuda' if (\n",
    "        torch.cuda.is_available() and cuda) else 'cpu')\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    if device.type == 'cuda':\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "    # Tokenize\n",
    "    tokenizer = Tokenizer(char_level=args.char_level)\n",
    "    tokenizer.fit_on_texts(texts=X_train)\n",
    "    vocab_size = len(tokenizer)\n",
    "\n",
    "    # Convert texts to sequences of indices\n",
    "    X_train = np.array(tokenizer.texts_to_sequences(X_train))\n",
    "    X_val = np.array(tokenizer.texts_to_sequences(X_val))\n",
    "    X_test = np.array(tokenizer.texts_to_sequences(X_test))\n",
    "\n",
    "    # Class weights\n",
    "    counts = np.bincount([label_encoder.class_to_index[class_] for class_ in all_tags])\n",
    "    class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CNNTextDataset(\n",
    "        X=X_train, y=y_train, max_filter_size=max(args.filter_sizes))\n",
    "    val_dataset = CNNTextDataset(\n",
    "        X=X_val, y=y_val, max_filter_size=max(args.filter_sizes))\n",
    "    test_dataset = CNNTextDataset(\n",
    "        X=X_test, y=y_test, max_filter_size=max(args.filter_sizes))\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = train_dataset.create_dataloader(\n",
    "        batch_size=args.batch_size)\n",
    "    val_dataloader = val_dataset.create_dataloader(\n",
    "        batch_size=args.batch_size)\n",
    "    test_dataloader = test_dataset.create_dataloader(\n",
    "        batch_size=args.batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    model = CNN(\n",
    "        embedding_dim=args.embedding_dim, vocab_size=vocab_size,\n",
    "        num_filters=args.num_filters, filter_sizes=args.filter_sizes,\n",
    "        hidden_dim=args.hidden_dim, dropout_p=args.dropout_p,\n",
    "        num_classes=num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define loss\n",
    "    class_weights_tensor = torch.Tensor(np.array(list(class_weights.values())))\n",
    "    loss_fn = nn.BCEWithLogitsLoss(weight=class_weights_tensor)\n",
    "\n",
    "    # Define optimizer & scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "    # Trainer module\n",
    "    trainer = Trainer(\n",
    "        model=model, device=device, loss_fn=loss_fn,\n",
    "        optimizer=optimizer, scheduler=scheduler)\n",
    "\n",
    "    # Train\n",
    "    best_model, best_val_loss = trainer.train(\n",
    "        args.num_epochs, args.patience, train_dataloader, val_dataloader)\n",
    "\n",
    "    # Best threshold for f1\n",
    "    train_loss, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader)\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true.ravel(), y_prob.ravel())\n",
    "    threshold = find_best_threshold(y_true.ravel(), y_prob.ravel())\n",
    "\n",
    "    # Determine predictions using threshold\n",
    "    test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
    "    y_pred = np.array([np.where(prob >= threshold, 1, 0) for prob in y_prob])\n",
    "\n",
    "    # Evaluate\n",
    "    performance = get_performance(\n",
    "        y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "\n",
    "    return {\n",
    "        \"args\": args,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"model\": best_model,\n",
    "        \"performance\": performance,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }"
   ]
  },
  {
   "source": [
    "### Tracking"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Experiment \n",
    "mlflow.set_experiment(experiment_name=\"baselines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(d,filepath):\n",
    "    \"\"\"Save dict to a json file. \"\"\"\n",
    "    with open(filepath,\"w\") as fp:\n",
    "        json.dump(d,indent=2,sort_keys=False,fp=fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.24 GiB already allocated; 2.45 MiB free; 2.28 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-408-cd61798805fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cnn\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#Train and Evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0martifacts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#Log Key Metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-404-ed21bd50aae8>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[1;34m(args, df)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     best_model, best_val_loss = trainer.train(\n\u001b[1;32m---> 77\u001b[1;33m         args.num_epochs, args.patience, train_dataloader, val_dataloader)\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# Best threshold for f1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-403-190dbd1cfe47>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, num_epochs, patience, train_dataloader, val_dataloader)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;31m# Steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-403-190dbd1cfe47>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Reset gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Define loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mJ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-330-56fd90a422f3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, channel_first)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;31m# Conv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0m_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpadding_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_right\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;31m# Pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    257\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m    258\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 259\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.24 GiB already allocated; 2.45 MiB free; 2.28 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"cnn\") as run:\n",
    "    #Train and Evaluate \n",
    "    artifacts=train_cnn(args=args,df=df)\n",
    "\n",
    "    #Log Key Metrics\n",
    "    mlflow.log_metrics({\"precision\": artifacts[\"performance\"][\"overall\"][\"precision\"]})\n",
    "    mlflow.log_metrics({\"recall\": artifacts[\"performance\"][\"overall\"][\"recall\"]})\n",
    "    mlflow.log_metrics({\"f1\": artifacts[\"performance\"][\"overall\"][\"f1\"]})\n",
    "\n",
    "    #Log artifacts\n",
    "    with tempfile.TemporaryDirectory() as fp:\n",
    "        artifacts[\"tokenizer\"].save(Path(fp, \"tokenizer.json\"))\n",
    "        artifacts[\"label_encoder\"].save(Path(fp, \"label_encoder.json\"))\n",
    "        torch.save(artifacts[\"model\"].state_dict(), Path(fp, \"model.pt\"))\n",
    "        save_dict(artifacts[\"performance\"], Path(fp, \"performance.json\"))\n",
    "        mlflow.log_artifacts(fp)\n",
    "\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params(vars(artifacts[\"args\"]))\n",
    "    \n"
   ]
  },
  {
   "source": [
    "### Viewing "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(filepath):\n",
    "\n",
    "    with open(filepath,\"r\") as fp:\n",
    "        d=json.load(fp)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load components\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment_id = mlflow.get_experiment_by_name(\"baselines\").experiment_id\n",
    "all_runs = mlflow.search_runs(experiment_ids=experiment_id, order_by=[\"metrics.f1 DESC\"])\n",
    "print (all_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best run\n",
    "device = torch.device(\"cpu\")\n",
    "best_run_id = all_runs.iloc[0].run_id\n",
    "best_run = mlflow.get_run(run_id=best_run_id)\n",
    "with tempfile.TemporaryDirectory() as fp:\n",
    "    client.download_artifacts(run_id=best_run_id, path=\"\", dst_path=fp)\n",
    "    tokenizer = Tokenizer.load(fp=Path(fp, \"tokenizer.json\"))\n",
    "    label_encoder = LabelEncoder.load(fp=Path(fp, \"label_encoder.json\"))\n",
    "    model_state = torch.load(Path(fp, \"model.pt\"), map_location=device)\n",
    "    performance = load_dict(filepath=Path(fp, \"performance.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (json.dumps(performance[\"overall\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Wrong shape for input_ids (shape torch.Size([1, 47])) or attention_mask (shape torch.Size([1]))",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-420-f7f8e375199d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_prob\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-403-190dbd1cfe47>\u001b[0m in \u001b[0;36mpredict_step\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;31m# Forward pass w/ inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                 \u001b[0my_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[1;31m# Store outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-393-84a08ca4bd6c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[1;31m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m         \u001b[1;31m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m         \u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_extended_attention_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m         \u001b[1;31m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mget_extended_attention_mask\u001b[1;34m(self, attention_mask, input_shape, device)\u001b[0m\n\u001b[0;32m    260\u001b[0m             raise ValueError(\n\u001b[0;32m    261\u001b[0m                 \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n\u001b[1;32m--> 262\u001b[1;33m                     \u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m                 )\n\u001b[0;32m    264\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong shape for input_ids (shape torch.Size([1, 47])) or attention_mask (shape torch.Size([1]))"
     ]
    }
   ],
   "source": [
    "# Load artifacts\n",
    "device = torch.device(\"cpu\")\n",
    "model = CNN(\n",
    "    embedding_dim=args.embedding_dim, vocab_size=len(tokenizer),\n",
    "    num_filters=args.num_filters, filter_sizes=args.filter_sizes,\n",
    "    hidden_dim=args.hidden_dim, dropout_p=args.dropout_p,\n",
    "    num_classes=len(label_encoder))\n",
    "model.load_state_dict(model_state)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sun Feb 21 18:11:31 2021       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.20       Driver Version: 460.20       CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 1650   WDDM  | 00000000:01:00.0 Off |                  N/A |\n| N/A   81C    P0    18W /  N/A |   3499MiB /  4096MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      5352      C   ...Data\\Anaconda3\\python.exe    N/A      |\n+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "y_prob = trainer.predict_step(dataloader)\n",
    "y_pred = np.array([np.where(prob >= threshold, 1, 0) for prob in y_prob])\n",
    "label_encoder.decode(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Optimization using Optuna"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from numpyencoder import NumpyEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify arguments\n",
    "args = Namespace(\n",
    "    char_level=True,\n",
    "    filter_sizes=list(range(1, 11)),\n",
    "    batch_size=64,\n",
    "    embedding_dim=128,\n",
    "    num_filters=128,\n",
    "    hidden_dim=128,\n",
    "    dropout_p=0.5,\n",
    "    lr=2e-4,\n",
    "    num_epochs=200,\n",
    "    patience=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer (modified for experiment tracking)\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, device, loss_fn=None,\n",
    "                optimizer=None, scheduler=None, trial=None):\n",
    "\n",
    "        # Set params\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.trial = trial\n",
    "\n",
    "    def train_step(self, dataloader):\n",
    "        \"\"\"Train step.\"\"\"\n",
    "        # Set model to train mode\n",
    "        self.model.train()\n",
    "        loss = 0.0\n",
    "\n",
    "        # Iterate over train batches\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Step\n",
    "            batch = [item.to(self.device) for item in batch]\n",
    "            inputs, targets = batch[:-1], batch[-1]\n",
    "            self.optimizer.zero_grad()  # Reset gradients\n",
    "            z = self.model(inputs)  # Forward pass\n",
    "            J = self.loss_fn(z, targets)  # Define loss\n",
    "            J.backward()  # Backward pass\n",
    "            self.optimizer.step()  # Update weights\n",
    "\n",
    "            # Cumulative Metrics\n",
    "            loss += (J.detach().item() - loss) / (i + 1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def eval_step(self, dataloader):\n",
    "        \"\"\"Validation or test step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        loss = 0.0\n",
    "        y_trues, y_probs = [], []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Step\n",
    "                batch = [item.to(self.device) for item in batch]  # Set device\n",
    "                inputs, y_true = batch[:-1], batch[-1]\n",
    "                z = self.model(inputs)  # Forward pass\n",
    "                J = self.loss_fn(z, y_true).item()\n",
    "\n",
    "                # Cumulative Metrics\n",
    "                loss += (J - loss) / (i + 1)\n",
    "\n",
    "                # Store outputs\n",
    "                y_prob = torch.sigmoid(z).cpu().numpy()\n",
    "                y_probs.extend(y_prob)\n",
    "                y_trues.extend(y_true.cpu().numpy())\n",
    "\n",
    "        return loss, np.vstack(y_trues), np.vstack(y_probs)\n",
    "\n",
    "    def predict_step(self, dataloader):\n",
    "        \"\"\"Prediction step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        y_probs = []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Forward pass w/ inputs\n",
    "                inputs, targets = batch[:-1], batch[-1]\n",
    "                y_prob = self.model(inputs)\n",
    "\n",
    "                # Store outputs\n",
    "                y_probs.extend(y_prob)\n",
    "\n",
    "        return np.vstack(y_probs)\n",
    "\n",
    "    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n",
    "        best_val_loss = np.inf\n",
    "        for epoch in range(num_epochs):\n",
    "            # Steps\n",
    "            train_loss = self.train_step(dataloader=train_dataloader)\n",
    "            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = self.model\n",
    "                _patience = patience  # reset _patience\n",
    "            else:\n",
    "                _patience -= 1\n",
    "            if not _patience:  # 0\n",
    "                print(\"Stopping early!\")\n",
    "                break\n",
    "\n",
    "            # Logging\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1} | \"\n",
    "                f\"train_loss: {train_loss:.5f}, \"\n",
    "                f\"val_loss: {val_loss:.5f}, \"\n",
    "                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n",
    "                f\"_patience: {_patience}\"\n",
    "            )\n",
    "\n",
    "            # Pruning based on the intermediate value\n",
    "            self.trial.report(val_loss, epoch)\n",
    "            if self.trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        return best_model, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(args, df, trial=None):\n",
    "    \"\"\"Train a CNN using specific arguments.\"\"\"\n",
    "\n",
    "    # Set seeds\n",
    "    set_seed()\n",
    "\n",
    "    # Get data splits\n",
    "    preprocessed_df = df.copy()\n",
    "    preprocessed_df.text = preprocessed_df.text.apply(preprocess, lower=True)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = get_data_splits(preprocessed_df)\n",
    "    X_test_raw = X_test\n",
    "    num_classes = len(label_encoder)\n",
    "\n",
    "    # Set device\n",
    "    cuda = True\n",
    "    device = torch.device('cuda' if (\n",
    "        torch.cuda.is_available() and cuda) else 'cpu')\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    if device.type == 'cuda':\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "    # Tokenize\n",
    "    tokenizer = Tokenizer(char_level=args.char_level)\n",
    "    tokenizer.fit_on_texts(texts=X_train)\n",
    "    vocab_size = len(tokenizer)\n",
    "\n",
    "    # Convert texts to sequences of indices\n",
    "    X_train = np.array(tokenizer.texts_to_sequences(X_train))\n",
    "    X_val = np.array(tokenizer.texts_to_sequences(X_val))\n",
    "    X_test = np.array(tokenizer.texts_to_sequences(X_test))\n",
    "\n",
    "    # Class weights\n",
    "    counts = np.bincount([label_encoder.class_to_index[class_] for class_ in all_tags])\n",
    "    class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CNNTextDataset(\n",
    "        X=X_train, y=y_train, max_filter_size=max(args.filter_sizes))\n",
    "    val_dataset = CNNTextDataset(\n",
    "        X=X_val, y=y_val, max_filter_size=max(args.filter_sizes))\n",
    "    test_dataset = CNNTextDataset(\n",
    "        X=X_test, y=y_test, max_filter_size=max(args.filter_sizes))\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = train_dataset.create_dataloader(\n",
    "        batch_size=args.batch_size)\n",
    "    val_dataloader = val_dataset.create_dataloader(\n",
    "        batch_size=args.batch_size)\n",
    "    test_dataloader = test_dataset.create_dataloader(\n",
    "        batch_size=args.batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    model = CNN(\n",
    "        embedding_dim=args.embedding_dim, vocab_size=vocab_size,\n",
    "        num_filters=args.num_filters, filter_sizes=args.filter_sizes,\n",
    "        hidden_dim=args.hidden_dim, dropout_p=args.dropout_p,\n",
    "        num_classes=num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define loss\n",
    "    class_weights_tensor = torch.Tensor(np.array(list(class_weights.values())))\n",
    "    loss_fn = nn.BCEWithLogitsLoss(weight=class_weights_tensor)\n",
    "\n",
    "    # Define optimizer & scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "    # Trainer module\n",
    "    trainer = Trainer(\n",
    "        model=model, device=device, loss_fn=loss_fn,\n",
    "        optimizer=optimizer, scheduler=scheduler, trial=trial)\n",
    "\n",
    "    # Train\n",
    "    best_model, best_val_loss = trainer.train(\n",
    "        args.num_epochs, args.patience, train_dataloader, val_dataloader)\n",
    "\n",
    "    # Best threshold for f1\n",
    "    train_loss, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader)\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true.ravel(), y_prob.ravel())\n",
    "    threshold = find_best_threshold(y_true.ravel(), y_prob.ravel())\n",
    "\n",
    "    # Determine predictions using threshold\n",
    "    test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
    "    y_pred = np.array([np.where(prob >= threshold, 1, 0) for prob in y_prob])\n",
    "\n",
    "    # Evaluate\n",
    "    performance = get_performance(\n",
    "        y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "\n",
    "    return {\n",
    "        \"args\": args,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"model\": best_model,\n",
    "        \"performance\": performance,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"threshold\": threshold,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, args):\n",
    "    \"\"\"Objective function for optimization trials.\"\"\"\n",
    "\n",
    "    # Paramters (to tune)\n",
    "    args.embedding_dim = trial.suggest_int(\"embedding_dim\", 128, 512)\n",
    "    args.num_filters = trial.suggest_int(\"num_filters\", 128, 512)\n",
    "    args.hidden_dim = trial.suggest_int(\"hidden_dim\", 128, 512)\n",
    "    args.dropout_p = trial.suggest_uniform(\"dropout_p\", 0.3, 0.8)\n",
    "    args.lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-4)\n",
    "\n",
    "    # Train & evaluate\n",
    "    artifacts = train_cnn(args=args, df=df, trial=trial)\n",
    "\n",
    "    # Set additional attributes\n",
    "    trial.set_user_attr(\"precision\", artifacts[\"performance\"][\"overall\"][\"precision\"])\n",
    "    trial.set_user_attr(\"recall\", artifacts[\"performance\"][\"overall\"][\"recall\"])\n",
    "    trial.set_user_attr(\"f1\", artifacts[\"performance\"][\"overall\"][\"f1\"])\n",
    "    trial.set_user_attr(\"threshold\", artifacts[\"threshold\"])\n",
    "\n",
    "    return artifacts[\"performance\"][\"overall\"][\"f1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-ce809eb1093a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintegration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLflowCallback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "from optuna.integration.mlflow import MLflowCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 50 # small sample for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    "study = optuna.create_study(study_name=\"optimization\", direction=\"maximize\", pruner=pruner)\n",
    "mlflow_callback = MLflowCallback(\n",
    "    tracking_uri=mlflow.get_tracking_uri(), metric_name='f1')\n",
    "study.optimize(lambda trial: objective(trial, args),\n",
    "               n_trials=NUM_TRIALS,\n",
    "               callbacks=[mlflow_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLFlow dashboard\n",
    "get_ipython().system_raw(\"mlflow server -h 0.0.0.0 -p 5000 --backend-store-uri $PWD/experiments/ &\")\n",
    "ngrok.kill()\n",
    "ngrok.set_auth_token(\"\")\n",
    "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
    "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All trials\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df = trials_df.sort_values([\"value\"], ascending=False)  # sort by metric\n",
    "trials_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best trial\n",
    "print (f\"Best value (val loss): {study.best_trial.value}\")\n",
    "print (f\"Best hyperparameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best parameters\n",
    "params = {**args.__dict__, **study.best_trial.params}\n",
    "params[\"threshold\"] = study.best_trial.user_attrs[\"threshold\"]\n",
    "print (json.dumps(params, indent=2, cls=NumpyEncoder))"
   ]
  }
 ]
}